WEBVTT

1
00:00:00.260 --> 00:00:03.350
In this video,
we discuss data reduction and

2
00:00:03.350 --> 00:00:08.234
unsupervised learning, which are two
essential concepts in cluster analysis.

3
00:00:08.234 --> 00:00:13.070
A dataset is essentially a table where
the variables, which are also called

4
00:00:13.070 --> 00:00:18.290
features or attributes, are in the columns
and the observations are in the rows.

5
00:00:18.290 --> 00:00:22.050
This means that all the data values
are in the body of the table.

6
00:00:22.050 --> 00:00:26.080
The process of reducing the number of
variables is known, as dimensionality

7
00:00:26.080 --> 00:00:31.270
reduction, while grouping observations
is a form of data reduction.

8
00:00:31.270 --> 00:00:36.548
Isolating the key variables in a dataset
is important in order to build robust,

9
00:00:36.548 --> 00:00:37.981
predictive models.

10
00:00:37.981 --> 00:00:39.421
It turns out that often,

11
00:00:39.421 --> 00:00:44.270
there is some degree of redundancy
among the variables in a dataset.

12
00:00:44.270 --> 00:00:49.022
And this is why it is possible to reduce
the number of dimensions without losing

13
00:00:49.022 --> 00:00:50.538
critical information.

14
00:00:50.538 --> 00:00:55.366
Redundancy occurs when different
attributes respond in similar ways to

15
00:00:55.366 --> 00:00:58.280
some common underlying factor.

16
00:00:58.280 --> 00:01:03.050
For instance, let's assume the human
resource department of a company

17
00:01:03.050 --> 00:01:06.060
creates an instrument to
measure job satisfaction.

18
00:01:06.060 --> 00:01:10.690
Employees are asked to rate seven
statements using a scale from

19
00:01:10.690 --> 00:01:15.540
one to seven, where one means that they
strongly disagree with the statement and

20
00:01:15.540 --> 00:01:18.020
seven means that they strongly agree.

21
00:01:18.020 --> 00:01:21.460
Let's also assume that
the statements in this survey are.

22
00:01:21.460 --> 00:01:24.440
My supervisor treats
me with consideration.

23
00:01:24.440 --> 00:01:29.749
My supervisor consults me concerning
important decisions that affect my work.

24
00:01:29.749 --> 00:01:33.700
My supervisor gives me recognition
when I do a good a job.

25
00:01:33.700 --> 00:01:38.824
My supervisor gives me the support
I need to do my job well.

26
00:01:38.824 --> 00:01:40.469
My pay is fair.

27
00:01:40.469 --> 00:01:45.140
My pay is appropriate, given the amount
of responsibility that comes with my job.

28
00:01:45.140 --> 00:01:49.020
My pay is comparable to the pay
earned by other employees

29
00:01:49.020 --> 00:01:51.128
whose job are similar to mine.

30
00:01:51.128 --> 00:01:55.488
Let's suppose that the HR department wants
to use the responses as seven separate

31
00:01:55.488 --> 00:01:57.712
variables to predict intention to quit.

32
00:01:57.712 --> 00:02:01.962
The problem with conducting this
study the way it is set up is

33
00:02:01.962 --> 00:02:05.112
a redundancy in the predictive variables.

34
00:02:05.112 --> 00:02:09.761
The seven items in the questionnaire
are not really measuring seven different

35
00:02:09.761 --> 00:02:11.210
constructs.

36
00:02:11.210 --> 00:02:16.143
More likely, items one to four are
measuring a single construct that could

37
00:02:16.143 --> 00:02:20.073
be reasonably be labeled
satisfaction with supervision.

38
00:02:20.073 --> 00:02:24.808
While items five to seven are measuring
a different construct that could be

39
00:02:24.808 --> 00:02:26.770
label satisfaction with pay.

40
00:02:27.950 --> 00:02:32.395
These constructs could be
identified with a technique called

41
00:02:32.395 --> 00:02:35.910
principal component analysis or
PCA for short.

42
00:02:35.910 --> 00:02:38.882
This technique creates
new variables as linear

43
00:02:38.882 --> 00:02:41.820
combinations of the original variables.

44
00:02:41.820 --> 00:02:46.034
These new variables are called
principal components.

45
00:02:46.034 --> 00:02:48.212
In our job satisfaction example,

46
00:02:48.212 --> 00:02:52.425
a principal component analysis
would identify two components.

47
00:02:52.425 --> 00:02:56.916
PCA would transform the original
seven values into two scores,

48
00:02:56.916 --> 00:02:58.670
one for each component.

49
00:02:58.670 --> 00:03:01.440
We don't show how these
scores were calculated.

50
00:03:01.440 --> 00:03:06.690
But for example, in this case,
the employee with ID 102274

51
00:03:06.690 --> 00:03:11.393
seems to be more satisfied with
supervision than with pay.

52
00:03:11.393 --> 00:03:16.033
Cluster analysis, on the other hand,
is a data reduction technique

53
00:03:16.033 --> 00:03:20.194
in the sense that it can take
a large number of observations and

54
00:03:20.194 --> 00:03:24.460
reduce them into a small
number of identifiable groups.

55
00:03:24.460 --> 00:03:27.899
Each of these groups can be
interpreted more easily and

56
00:03:27.899 --> 00:03:29.848
is represented by a centroid.

57
00:03:29.848 --> 00:03:35.596
The scatter plot shows four clusters for
the scores in the job satisfaction survey.

58
00:03:35.596 --> 00:03:38.879
The stars represent the centroid
of each cluster and

59
00:03:38.879 --> 00:03:43.490
can be used to characterize all
the observations in the group.

60
00:03:43.490 --> 00:03:49.260
For instance, the gray cluster consists of
employees with low job satisfaction and

61
00:03:49.260 --> 00:03:53.115
is represented by average
scores close to two.

62
00:03:53.115 --> 00:03:57.420
Cluster analysis can achieve
very significant data reductions

63
00:03:57.420 --> 00:03:59.690
by transforming thousands or

64
00:03:59.690 --> 00:04:04.260
even hundreds of thousands of
observations into interpretable groups.

65
00:04:05.270 --> 00:04:08.558
Now, let's talk about the concept
of unsupervised learning.

66
00:04:08.558 --> 00:04:12.380
You may recall that classification
techniques were discussed in the second

67
00:04:12.380 --> 00:04:14.059
course of this special session.

68
00:04:15.090 --> 00:04:20.062
In classification, the objective is to
find a set of rules that can be applied

69
00:04:20.062 --> 00:04:24.663
to a new observation in order to assign
this new observation to a group.

70
00:04:24.663 --> 00:04:25.518
The methods for

71
00:04:25.518 --> 00:04:29.800
classification develop rules by
discovering patterns in historical data.

72
00:04:30.930 --> 00:04:34.194
The critical feature of
this historical data is

73
00:04:34.194 --> 00:04:37.867
that classification of
the observations is known and

74
00:04:37.867 --> 00:04:41.797
it is used to learn how to
classify future observations.

75
00:04:41.797 --> 00:04:44.449
Because this piece of
information is available,

76
00:04:44.449 --> 00:04:47.540
the process is known as
supervised learning.

77
00:04:47.540 --> 00:04:53.470
For instance, this table shows ten answers
to the job satisfaction survey and

78
00:04:53.470 --> 00:04:57.450
it also indicates whether or
not the employee quit.

79
00:04:57.450 --> 00:05:02.410
The two employees that quit had low
ratings for the salary questions five,

80
00:05:02.410 --> 00:05:08.240
six and seven and some mixed ratings for
the supervisor questions one through four.

81
00:05:08.240 --> 00:05:12.576
A prediction model built on this
data will fall in the category of

82
00:05:12.576 --> 00:05:15.869
supervised learning,
because the outcome that

83
00:05:15.869 --> 00:05:19.898
the model is trying to predict
is known in historical data.

84
00:05:19.898 --> 00:05:24.940
In unsupervised learning, the observations
in the historical data are not labeled.

85
00:05:24.940 --> 00:05:29.880
That is, we don't know if an observation
belongs to one group or another.

86
00:05:29.880 --> 00:05:34.440
This means that we don't know how
many different groups there are in

87
00:05:34.440 --> 00:05:37.820
a population from which
the dataset originated.

88
00:05:37.820 --> 00:05:40.744
Discovering the number
of groups is therefore,

89
00:05:40.744 --> 00:05:43.250
one of the main outcomes of the analysis.

90
00:05:43.250 --> 00:05:45.401
For example, in a previous video,

91
00:05:45.401 --> 00:05:50.784
we described how the market intelligence
firm, Information Resources Incorporated,

92
00:05:50.784 --> 00:05:55.304
conducted a cluster analysis of survey
data to establish that the market of

93
00:05:55.304 --> 00:05:59.538
natural and organic products
consisted of seven distinct segments,

94
00:05:59.538 --> 00:06:04.060
a number that was not known prior
to the completion of the analysis.

95
00:06:04.060 --> 00:06:08.940
Cluster analysis can also be applied
to historical data that is labeled

96
00:06:08.940 --> 00:06:11.210
with the purpose of finding new labels.

97
00:06:11.210 --> 00:06:12.890
For example, in one study,

98
00:06:12.890 --> 00:06:17.330
cluster analysis was used to
categorize mutual funds based on

99
00:06:17.330 --> 00:06:22.410
their financial characteristics instead
of their investment objectives.

100
00:06:22.410 --> 00:06:24.113
The historical data for

101
00:06:24.113 --> 00:06:29.054
the study consisted of 904 different
funds that fund managers had

102
00:06:29.054 --> 00:06:34.604
classified into seven categories
according to the investment objectives.

103
00:06:34.604 --> 00:06:38.238
That is the fund managers
assigned a label to each fund and

104
00:06:38.238 --> 00:06:41.738
decided there were seven possible labels.

105
00:06:41.738 --> 00:06:46.918
However, a cluster analysis on financial
variables related to the funds

106
00:06:46.918 --> 00:06:51.524
concluded that there were only
three distinct fund categories.

107
00:06:51.524 --> 00:06:56.126
The reduction in the number of
categories has significant benefits to

108
00:06:56.126 --> 00:06:59.570
investors seeking to
diversify their portfolios.

109
00:06:59.570 --> 00:07:03.027
The study determined that
the consolidated categories

110
00:07:03.027 --> 00:07:05.821
were more informative
about performance and

111
00:07:05.821 --> 00:07:10.245
risk than the original seven categories
created by the fund managers.

112
00:07:10.245 --> 00:07:14.851
In terms of data to use, the analyst
initially considered 28 financial

113
00:07:14.851 --> 00:07:18.510
variables that were related to risk and
return.

114
00:07:18.510 --> 00:07:24.675
However, after applying principal
component analysis, they found that 16 out

115
00:07:24.675 --> 00:07:30.768
of the 28 variables were able to explain
98% of the variation in the dataset.

116
00:07:30.768 --> 00:07:36.300
Therefore, they only use 16 variables per
cluster which as we all ready mentioned,

117
00:07:36.300 --> 00:07:39.030
resulted in three fund categories.

118
00:07:39.030 --> 00:07:42.112
This example shows that
dimensionality reduction and

119
00:07:42.112 --> 00:07:44.390
data reduction compliment each other.

120
00:07:44.390 --> 00:07:49.015
As a matter fact, it is a common
practice to apply dimensionality

121
00:07:49.015 --> 00:07:52.891
reduction techniques such
as PCA before clustering.