WEBVTT

1
00:00:00.960 --> 00:00:04.280
If you've spent any time around computers
you've almost certainly heard the term

2
00:00:04.280 --> 00:00:06.192
garbage in, garbage out.

3
00:00:06.192 --> 00:00:09.210
Meaning that the quality of your outputs
is only as good as the quality of your

4
00:00:09.210 --> 00:00:10.410
inputs.

5
00:00:10.410 --> 00:00:14.170
This principle absolutely applies
when we talk about data analytics.

6
00:00:14.170 --> 00:00:18.100
Your ability to execute an analysis and
have confidence in the results has a huge

7
00:00:18.100 --> 00:00:21.440
dependence on the overall
quality of the data you use.

8
00:00:21.440 --> 00:00:24.870
However, data quality can be
a pretty tricky thing to manage.

9
00:00:24.870 --> 00:00:28.760
And usually requires a fairly relentless
focus on preventing, detecting and

10
00:00:28.760 --> 00:00:29.830
remediating issues.

11
00:00:30.890 --> 00:00:33.710
In this video we'll take
a closer look at data quality.

12
00:00:33.710 --> 00:00:37.820
First, we'll try to define what data
quality is, then we'll discuss how data

13
00:00:37.820 --> 00:00:40.350
quality can be managed at each
point in the data life cycle.

14
00:00:41.870 --> 00:00:44.310
So what exactly is data quality?

15
00:00:44.310 --> 00:00:47.670
There are two over arching
definitions that we might apply.

16
00:00:47.670 --> 00:00:50.270
The first, and the one you see
in most technical articles or

17
00:00:50.270 --> 00:00:55.330
standards documents, is the fitness for
use or meets requirements definition.

18
00:00:55.330 --> 00:00:59.480
This definition basically says the data
quality is the degree to which data can be

19
00:00:59.480 --> 00:01:00.629
used for its intended purpose.

20
00:01:01.690 --> 00:01:05.080
The second definition is a bit
more philosophical and suggests

21
00:01:05.080 --> 00:01:09.810
that data quality is the degree to which
data accurately represents the real world.

22
00:01:09.810 --> 00:01:11.530
If you've been paying
attention in this course,

23
00:01:11.530 --> 00:01:14.930
you know how fond we are of understanding
what is happening in the real world, so

24
00:01:14.930 --> 00:01:16.940
we really like that definition.

25
00:01:16.940 --> 00:01:20.040
However, in the real world you
also have to get things done.

26
00:01:20.040 --> 00:01:23.480
So we think there's a lot of value
to the first definition as well.

27
00:01:23.480 --> 00:01:25.990
The good news is,
we really don't have to choose.

28
00:01:25.990 --> 00:01:29.779
We can do our best to make our data as
representative of the real world as

29
00:01:29.779 --> 00:01:30.494
practical.

30
00:01:30.494 --> 00:01:33.992
And we can decide when we've gotten close
enough that the decisions we make and

31
00:01:33.992 --> 00:01:35.589
the actions we take are sound ones.

32
00:01:36.950 --> 00:01:39.800
We can also add a bit more
detail around what it means for

33
00:01:39.800 --> 00:01:41.710
data to be of high quality.

34
00:01:41.710 --> 00:01:45.930
There are few characteristics that
generally help to define good data.

35
00:01:45.930 --> 00:01:48.720
The first is completeness or
a measure of whether or

36
00:01:48.720 --> 00:01:51.490
not we have all the data
we expect to have.

37
00:01:51.490 --> 00:01:54.110
Are we capturing all events
we should be capturing?

38
00:01:54.110 --> 00:01:55.120
When we capture an event,

39
00:01:55.120 --> 00:01:58.720
do we have all the attributes of
that event that we expect to have?

40
00:01:58.720 --> 00:02:00.190
If we use reference data,

41
00:02:00.190 --> 00:02:02.550
are all the values in that
reference data accounted for?

42
00:02:03.930 --> 00:02:08.480
A related idea is uniqueness, which is
basically the opposite of completeness.

43
00:02:08.480 --> 00:02:12.750
For example, if I record one event,
am I sure I've recorded it only once and

44
00:02:12.750 --> 00:02:13.739
not multiple times?

45
00:02:15.420 --> 00:02:20.020
A second idea is accuracy, a measure of
whether the data we have is an accurate

46
00:02:20.020 --> 00:02:22.980
representative of the idea
it's trying to capture.

47
00:02:22.980 --> 00:02:25.700
If the data point is a number,
is it the right number?

48
00:02:25.700 --> 00:02:29.180
If it's a string, is it the right
string and is it spelled correctly?

49
00:02:29.180 --> 00:02:31.739
Are timestamps and
other attributes correctly captured?

50
00:02:33.090 --> 00:02:36.770
The concept of consistency
is an extension of accuracy.

51
00:02:36.770 --> 00:02:39.860
Do I capture the same data
the same way every time?

52
00:02:39.860 --> 00:02:43.520
Or if I capture it in two different
places, do I have the same values?

53
00:02:43.520 --> 00:02:47.110
A third measure is what we might
call conformance or validity.

54
00:02:47.110 --> 00:02:50.390
Whether stored data conforms to syntax and
coding and

55
00:02:50.390 --> 00:02:52.920
other specifications of a data model.

56
00:02:52.920 --> 00:02:55.020
Is data stored in the correct format?

57
00:02:55.020 --> 00:02:58.520
If codes are used for attributes,
are they the expected codes?

58
00:02:58.520 --> 00:03:01.970
Are pieces of data named using the
conventions that have been established for

59
00:03:01.970 --> 00:03:03.070
a system or database?

60
00:03:04.680 --> 00:03:08.620
A fourth measure is timeliness, which
speaks to whether data is captured or

61
00:03:08.620 --> 00:03:12.940
made available soon enough after a real
world event for it to be useful.

62
00:03:12.940 --> 00:03:16.580
You might hear the term data latency
to describe how long it takes for

63
00:03:16.580 --> 00:03:20.220
data to be available for
something like reporting or analytics.

64
00:03:20.220 --> 00:03:23.480
For example, if you need to
make a same day decision and

65
00:03:23.480 --> 00:03:27.390
the data isn't available until the next
day, the data is of low usefulness for

66
00:03:27.390 --> 00:03:30.919
that purpose, and of low quality
per our fitness for use definition.

67
00:03:32.430 --> 00:03:35.270
The fifth and
final measure we'll include is prominence,

68
00:03:35.270 --> 00:03:39.380
which is the degree to which we have
visibility into the origins of the data.

69
00:03:39.380 --> 00:03:43.400
This is kind of a second-order measure,
but speaks to how much confidence we have

70
00:03:43.400 --> 00:03:45.840
that the data we're looking at is real and
is accurate.

71
00:03:47.400 --> 00:03:50.210
It makes sense that if I'm somehow
able to measure each of these

72
00:03:50.210 --> 00:03:54.770
characteristics of my data, I'll get a
pretty good idea of how good that data is.

73
00:03:54.770 --> 00:03:58.450
So let's talk a little bit more about how
and where we might take these measures and

74
00:03:58.450 --> 00:04:00.410
what things we might do
to manage data equality.

75
00:04:01.910 --> 00:04:04.750
It's useful to bring back
the first part of the information

76
00:04:04.750 --> 00:04:08.870
action value chain we discussed in module
one, which you may recall looks like this.

77
00:04:09.940 --> 00:04:13.880
We start with events and characteristics
in the real world, capture data in source

78
00:04:13.880 --> 00:04:18.950
systems, store data, extract data,
and execute our analysis.

79
00:04:18.950 --> 00:04:22.650
We can also look at this framework from
more of a systems point of view like this.

80
00:04:23.960 --> 00:04:27.980
Here you can see a series of steps for
how data moves through our systems.

81
00:04:27.980 --> 00:04:31.310
We capture data in our source systems
using some set of automated or

82
00:04:31.310 --> 00:04:32.860
manual processes.

83
00:04:32.860 --> 00:04:36.670
We then take that data and drive it to
the storage location using extract,

84
00:04:36.670 --> 00:04:40.430
transform and load operations,
or other similar mechanisms.

85
00:04:40.430 --> 00:04:44.900
We then extract the data from storage for
use in reports and analytics using SQL or

86
00:04:44.900 --> 00:04:46.710
some set of business intelligence tools.

87
00:04:47.980 --> 00:04:52.630
We started this video by talking about
the idea of garbage in, garbage out.

88
00:04:52.630 --> 00:04:55.690
The most effective way to address
data quality issues is to

89
00:04:55.690 --> 00:04:58.260
prevent them from ever
happening in the first place

90
00:04:58.260 --> 00:05:01.780
by controlling by how data
is captured at the source.

91
00:05:01.780 --> 00:05:05.510
How we do this depends on exactly
how the data is captured.

92
00:05:05.510 --> 00:05:10.220
One of the biggest drivers of bad data are
errors introduced via manual data entry by

93
00:05:10.220 --> 00:05:15.540
people, whether their customers, other
outside partners or our own employees.

94
00:05:15.540 --> 00:05:18.820
To help minimize these types of
errors organizations might build

95
00:05:18.820 --> 00:05:23.180
in validation mechanisms or auto
populate certain pieces of information.

96
00:05:23.180 --> 00:05:27.260
For example, an online form might force
you to enter a valid phone number in

97
00:05:27.260 --> 00:05:32.130
a specific format, make you use a dropdown
box to choose the state where you live, or

98
00:05:32.130 --> 00:05:35.760
even prepopulate your city based
on the ZIP code you enter.

99
00:05:35.760 --> 00:05:37.793
It also might not let you submit or

100
00:05:37.793 --> 00:05:41.065
proceed unless all the require
fields are filled in.

101
00:05:41.065 --> 00:05:45.125
Generally speaking, the less information
that is typed into free-form fields,

102
00:05:45.125 --> 00:05:47.220
the higher the quality
of the data will be.

103
00:05:49.020 --> 00:05:53.390
Other capture mechanisms might be driven
by the design of the source system itself.

104
00:05:53.390 --> 00:05:57.380
Sometimes there are bugs in source
applications that result in bad data.

105
00:05:57.380 --> 00:06:01.360
In these cases the best solution
is to find the bugs and fix them.

106
00:06:01.360 --> 00:06:05.240
In both cases we seek to improve data
quality from preventing it from the start.

107
00:06:06.270 --> 00:06:09.890
However, even the best data capture
mechanisms aren't perfect, and

108
00:06:09.890 --> 00:06:13.340
it's inevitable that some bad data
will make it into the source system.

109
00:06:13.340 --> 00:06:15.045
If we're smart, we can still try and

110
00:06:15.045 --> 00:06:17.908
catch it before it makes its way
downstream to other systems.

111
00:06:17.908 --> 00:06:21.042
The way we do this is with some
set of automated checks that run

112
00:06:21.042 --> 00:06:23.198
against the data within a source system.

113
00:06:23.198 --> 00:06:27.212
And look for one or more of the quality
characteristics we discussed ealier.

114
00:06:27.212 --> 00:06:30.585
In some cases it maybe possible
to automatically correct data or

115
00:06:30.585 --> 00:06:31.900
fill in missing values.

116
00:06:31.900 --> 00:06:35.627
In other cases, errors can be flagged and
picked up by some remediation or

117
00:06:35.627 --> 00:06:36.839
maintenance process.

118
00:06:38.120 --> 00:06:41.320
Again, catching issues early
is almost always better, but

119
00:06:41.320 --> 00:06:43.270
it's not always possible.

120
00:06:43.270 --> 00:06:46.070
Our next opportunity to enforce
quality is in the process

121
00:06:46.070 --> 00:06:51.220
used to bring data into a common location,
usually a database of some sort.

122
00:06:51.220 --> 00:06:54.250
There are a couple things we
typically do in ETL processes or

123
00:06:54.250 --> 00:06:57.140
other data loading operations
that help with quality.

124
00:06:57.140 --> 00:06:59.730
First, we applied what's
called audit balance and

125
00:06:59.730 --> 00:07:01.665
control operations to our jobs.

126
00:07:01.665 --> 00:07:05.250
These operations is generally
make sure that transfer processes

127
00:07:05.250 --> 00:07:06.570
itself happens as intended.

128
00:07:06.570 --> 00:07:11.440
And that we don't actually introduced
data quality problems as those jobs run.

129
00:07:11.440 --> 00:07:14.630
There's a lot of different ways
these operations can be set up but

130
00:07:14.630 --> 00:07:18.820
usually they involve constructing summary
metrics on both sides of the transfer and

131
00:07:18.820 --> 00:07:19.900
ensuring that they balance.

132
00:07:21.210 --> 00:07:25.880
Secondly, we can actually write ETL code
in such a way that data is standardized,

133
00:07:25.880 --> 00:07:27.700
forced into a common format, or

134
00:07:27.700 --> 00:07:31.210
even filled in using reference
information as it's loaded.

135
00:07:31.210 --> 00:07:33.049
We can even take this to step further and

136
00:07:33.049 --> 00:07:36.137
enforce what's called referential
integrity in our database.

137
00:07:36.137 --> 00:07:40.626
Which basically means that all reference
data contained in data records must have

138
00:07:40.626 --> 00:07:43.124
known values and related reference tables.

139
00:07:43.124 --> 00:07:47.323
This help's ensure that no unknown
attribute values can enter the database.

140
00:07:47.323 --> 00:07:50.610
Of course it's also a good idea
to set some sort of flag or

141
00:07:50.610 --> 00:07:54.488
alert when an unknown values
is observed and filtered out.

142
00:07:54.488 --> 00:07:55.822
Despite our best efforts,

143
00:07:55.822 --> 00:07:59.191
it's still possible that data has
made it's way into our database.

144
00:07:59.191 --> 00:07:59.865
Or worse yet,

145
00:07:59.865 --> 00:08:04.460
that we somehow introduce some errors
in moving it from one place to another.

146
00:08:04.460 --> 00:08:08.210
Just like we did in our source systems,
we can set up ordinate checks for data

147
00:08:08.210 --> 00:08:11.577
quality characteristics for our database,
but there are couple of differences.

148
00:08:11.577 --> 00:08:16.080
First, because I'm potentially
getting data from multiple places and

149
00:08:16.080 --> 00:08:20.650
because I may stored for a lot longer on
my database, I may be able to use some of

150
00:08:20.650 --> 00:08:25.460
that other data to make sense of what I'm
seeing from one particular source system.

151
00:08:25.460 --> 00:08:29.060
This allows my data quality checks
to be a bit more sophisticated.

152
00:08:29.060 --> 00:08:33.210
And it's not uncommon to see statistical
techniques like those used in statistical

153
00:08:33.210 --> 00:08:37.930
process control to detect and alert when a
metric has exceeded some normal threshold.

154
00:08:39.670 --> 00:08:43.300
On the other hand, because more steps have
happened between the event itself and

155
00:08:43.300 --> 00:08:44.850
the data in my database,

156
00:08:44.850 --> 00:08:49.400
my checks may not be as precise at
telling me exactly where the problem is.

157
00:08:49.400 --> 00:08:52.470
I may need to backtrack through
the source system to isolate the issue.

158
00:08:53.830 --> 00:08:57.780
Now, if we've done a good job we should
catch the vast majority of issues

159
00:08:57.780 --> 00:09:01.090
long before they end up in
a downstream report or analysis.

160
00:09:01.090 --> 00:09:03.790
However, something will
inevitably slip through, and

161
00:09:03.790 --> 00:09:07.360
every time we manipulate data we
have the potential for new errors.

162
00:09:07.360 --> 00:09:11.040
It turns out that we can implement checks
similar to those we've discussed at each

163
00:09:11.040 --> 00:09:14.560
downstream step in the process,
including our reporting and analytics.

164
00:09:15.710 --> 00:09:19.180
At the end of the line is what
we might call the eyeball check.

165
00:09:19.180 --> 00:09:22.910
The person reviewing a report or
interpreting analysis needs to know enough

166
00:09:22.910 --> 00:09:26.185
about the business to recognize
when data looks fishy.

167
00:09:26.185 --> 00:09:29.260
While we certainly don't want to rely
on that to catch quality issues,

168
00:09:29.260 --> 00:09:31.470
it is the method of last resort.

169
00:09:31.470 --> 00:09:33.140
So given all the options we have for

170
00:09:33.140 --> 00:09:37.340
where we execute data quality, the
question becomes, which ones do we use?

171
00:09:37.340 --> 00:09:39.448
The answer is simple, all of them.

172
00:09:39.448 --> 00:09:42.874
The best data quality programs use a multi
faceted approach that puts quality

173
00:09:42.874 --> 00:09:45.400
controls at every step of the process.

174
00:09:45.400 --> 00:09:49.650
They also integrate these checks into
a larger coordinated process that ensures

175
00:09:49.650 --> 00:09:54.100
data quality issues are fully investigated
and remediated by the correct team.

176
00:09:54.100 --> 00:09:58.060
This is one reason why data quality is
often integrated into an organization's

177
00:09:58.060 --> 00:10:00.250
larger data governance process.

178
00:10:00.250 --> 00:10:03.151
This way,
the same accountability structures and

179
00:10:03.151 --> 00:10:07.540
communication mechanisms can be leveraged
to identify and resolve issues.

180
00:10:07.540 --> 00:10:11.110
As an analyst you have an important
role to play in data quality.

181
00:10:11.110 --> 00:10:13.650
In addition to being one of those
performing the eye ball check,

182
00:10:13.650 --> 00:10:16.013
you will be spending a lot of
time pouring through the data.

183
00:10:16.013 --> 00:10:19.074
And you'll almost certainly come across
something that doesn't look like

184
00:10:19.074 --> 00:10:20.360
from time to time.

185
00:10:20.360 --> 00:10:22.210
When you do, take the initiative and

186
00:10:22.210 --> 00:10:25.459
get it resolved using whatever structures
are present in your organization.

187
00:10:27.210 --> 00:10:28.790
Let's quickly recap what
we've learned today.

188
00:10:29.900 --> 00:10:32.060
We defined data quality
from both a fitness for

189
00:10:32.060 --> 00:10:35.360
use and
real world representation perspective.

190
00:10:35.360 --> 00:10:37.580
We then introduced a number
of data quality measures,

191
00:10:37.580 --> 00:10:42.050
including completeness and uniqueness,
accuracy and consistency, conformance and

192
00:10:42.050 --> 00:10:44.195
validity, timeliness, and prominence.

193
00:10:44.195 --> 00:10:48.960
Finally, we stepped through the path that
data takes on its way from the real world

194
00:10:48.960 --> 00:10:50.175
to analysis and

195
00:10:50.175 --> 00:10:53.770
described the types of data quality
measures that can be taken at each step.

196
00:10:53.770 --> 00:10:56.050
Including how those
collective measures can and

197
00:10:56.050 --> 00:10:59.930
should be integrated into
a larger data quality process.

198
00:10:59.930 --> 00:11:03.365
So long for now but may the balance
of quality always be in your favor.