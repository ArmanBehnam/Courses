WEBVTT

1
00:00:01.220 --> 00:00:04.730
We've covered a lot of ground in our
module on analytical technologies.

2
00:00:04.730 --> 00:00:06.010
We've talked about data storage,

3
00:00:06.010 --> 00:00:08.850
including a detailed look
at the relational database.

4
00:00:08.850 --> 00:00:11.780
We've also outlined a number of
different types of tools that exist

5
00:00:11.780 --> 00:00:14.850
in the world of data and
data analytics, both traditional and

6
00:00:14.850 --> 00:00:19.680
emerging, including exposure to ideas
like big data and cloud computing.

7
00:00:19.680 --> 00:00:22.790
In this video we're going to
take a look at how data analysts

8
00:00:22.790 --> 00:00:24.960
actually use some of these tools.

9
00:00:24.960 --> 00:00:26.945
For now, we're going to do
this at a really high level.

10
00:00:26.945 --> 00:00:30.840
You'll have plenty of opportunity in later
modules and courses to go deeper and

11
00:00:30.840 --> 00:00:33.360
do exactly how certain operations and
analyses are done.

12
00:00:34.370 --> 00:00:36.670
There are a lot of different tools and
techniques out there and

13
00:00:36.670 --> 00:00:38.700
we couldn't possibly cover them all.

14
00:00:38.700 --> 00:00:42.780
However, we will cover just a few
common ways analysts work with data and

15
00:00:42.780 --> 00:00:45.439
how one might think about using
one type of tool versus another.

16
00:00:46.440 --> 00:00:50.230
We'll start by looking at three broad
methodologies a data analyst might employ

17
00:00:50.230 --> 00:00:51.620
to access and analyze data.

18
00:00:52.710 --> 00:00:55.950
Let's call the first method
the intermediate file approach.

19
00:00:55.950 --> 00:00:58.520
In this approach we extract
data from a database or

20
00:00:58.520 --> 00:01:02.060
other location where data is stored,
and we export the data we need into

21
00:01:02.060 --> 00:01:05.720
a standalone file like a text file or
Excel file.

22
00:01:05.720 --> 00:01:09.030
This often involves writing SQL code
against the database to extract

23
00:01:09.030 --> 00:01:10.940
just the data we need.

24
00:01:10.940 --> 00:01:15.340
We then import the data into an analytical
tool like Excel, a business intelligence

25
00:01:15.340 --> 00:01:19.410
tool, or a statistical software package or
programming environment.

26
00:01:19.410 --> 00:01:22.070
Once the data is in
the analytical environment I can

27
00:01:22.070 --> 00:01:25.230
execute whatever type
of analysis is desired.

28
00:01:25.230 --> 00:01:29.030
Note that this approach assumes that all
the data I need is already integrated in

29
00:01:29.030 --> 00:01:30.110
one database environment.

30
00:01:31.400 --> 00:01:33.460
So why might I use this approach?

31
00:01:33.460 --> 00:01:37.310
For starters, you might not be able to
connect your analytical tools directly to

32
00:01:37.310 --> 00:01:40.730
your database due to security or
stability concerns.

33
00:01:40.730 --> 00:01:43.050
Or you may not have time to
set up all the permissions and

34
00:01:43.050 --> 00:01:46.200
network connections required
to make a direct connection.

35
00:01:46.200 --> 00:01:49.720
It might be the case that you need to
extract the data at one point in time and

36
00:01:49.720 --> 00:01:52.540
analyze it later or analyze it offline.

37
00:01:52.540 --> 00:01:56.860
You might also want to drive the same
data set into multiple analytical tools.

38
00:01:56.860 --> 00:02:01.180
In these cases, it's more convenient to
store the data in an intermediate file and

39
00:02:01.180 --> 00:02:02.319
import it for analysis.

40
00:02:03.500 --> 00:02:08.282
Finally, by breaking access into two
steps I have a little mores control and

41
00:02:08.282 --> 00:02:12.992
visibility into each steps, which might
be useful for data validation and

42
00:02:12.992 --> 00:02:14.203
quality control.

43
00:02:14.203 --> 00:02:17.559
A second method might be called
the direct connection approach.

44
00:02:17.559 --> 00:02:21.727
Wth this approach we connect our
analytics tool directly to a database or

45
00:02:21.727 --> 00:02:26.032
other data source using what's called
an open database connectivity, or

46
00:02:26.032 --> 00:02:31.720
ODBC connection, or some other application
program interface or API connection.

47
00:02:31.720 --> 00:02:36.760
Broadly, APIs are standard mechanisms for
exchanging information between programs,

48
00:02:36.760 --> 00:02:40.849
and ODBC is one special case of
an API used to connect to databases.

49
00:02:42.060 --> 00:02:45.290
Most analytical tools have the ability
to connect directly to the most

50
00:02:45.290 --> 00:02:48.620
common database systems and
a number of other common data sources.

51
00:02:49.830 --> 00:02:53.870
In this approach, I use the analytical
tool interface to set up a connection and

52
00:02:53.870 --> 00:02:56.360
identify the data I want to access.

53
00:02:56.360 --> 00:02:59.519
Usually this involves the same
ideas as SQL queries on a database.

54
00:03:00.530 --> 00:03:03.510
The tool executes the extraction
operation in the background,

55
00:03:03.510 --> 00:03:06.480
and that can proceed with
whatever analysis is desired.

56
00:03:07.510 --> 00:03:10.220
There are a couple nice things about
the direct connection approach.

57
00:03:10.220 --> 00:03:12.910
First, it cuts out a couple
of steps in the process

58
00:03:12.910 --> 00:03:17.220
since I don't need to export and
import data using an intermediate file.

59
00:03:17.220 --> 00:03:19.760
Secondly, a direct
connection can be set up to

60
00:03:19.760 --> 00:03:22.950
automatically refresh as
the underlying data changes.

61
00:03:22.950 --> 00:03:25.790
Meaning that my analytical
work can be refreshed as well

62
00:03:25.790 --> 00:03:28.320
to reflect the most recent information.

63
00:03:28.320 --> 00:03:32.238
On the downside, it can require a bit of
set up to establish the actual connection

64
00:03:32.238 --> 00:03:33.400
to the database.

65
00:03:33.400 --> 00:03:36.940
And it's a little bit harder to be sure
that the extraction process is happening

66
00:03:36.940 --> 00:03:37.525
as intended.

67
00:03:37.525 --> 00:03:41.050
Offline analysis can be
a bit trickier as well

68
00:03:41.050 --> 00:03:44.660
as we need to make sure we have
a copy of our data stored locally.

69
00:03:44.660 --> 00:03:48.348
The last method we'll cover is what we'll
call the downstream integration approach.

70
00:03:48.348 --> 00:03:52.250
It's quite often the case in data
analytics that the information we need is

71
00:03:52.250 --> 00:03:55.180
located in a bunch of different
locations and formats.

72
00:03:55.180 --> 00:03:59.118
While it would be nice to have everything
on one warehouse, it doesn't always makes

73
00:03:59.118 --> 00:04:02.733
sense to spend the time and energy to
put everything there prior to analysis.

74
00:04:02.733 --> 00:04:05.765
Especially if we're not entirely sure
those sources will turn out to be

75
00:04:05.765 --> 00:04:06.330
important.

76
00:04:07.440 --> 00:04:11.020
The newer and more complex the analysis,
the more likely it is that we'll need to

77
00:04:11.020 --> 00:04:13.490
do at least some integration
in the analytical environment.

78
00:04:14.900 --> 00:04:19.920
In cases like this, we generally use a
broader set of API and ODBC connections as

79
00:04:19.920 --> 00:04:24.040
our analytical tool of choice to connect
to several sources concurrently.

80
00:04:24.040 --> 00:04:26.960
And use additional functionality
in the tool to integrate the data

81
00:04:26.960 --> 00:04:29.250
construct analytical data sets.

82
00:04:29.250 --> 00:04:32.340
We then proceed, as we would,
using the other methods.

83
00:04:32.340 --> 00:04:35.180
Of course, there are a number of other
approaches that we might take when

84
00:04:35.180 --> 00:04:37.800
working with data to get
the results we're looking for,

85
00:04:37.800 --> 00:04:40.590
including hybrids of the ones
we've discussed here.

86
00:04:40.590 --> 00:04:43.640
For example, I might perform
certain manipulations in Excel,

87
00:04:43.640 --> 00:04:47.740
and then import the results into
a more sophisticated analysis tool.

88
00:04:47.740 --> 00:04:50.360
I can also do the opposite,
using an advanced tool to

89
00:04:50.360 --> 00:04:53.780
isolate some set of data that I
want to incorporate into Excel.

90
00:04:53.780 --> 00:04:56.520
Perhaps into a business or
financial model.

91
00:04:56.520 --> 00:04:59.240
The approach you take in any
situation will depend on what it is

92
00:04:59.240 --> 00:05:00.580
you're trying to do.

93
00:05:00.580 --> 00:05:03.200
But one thing we haven't really
discussed is when you'd want to use

94
00:05:03.200 --> 00:05:04.850
one tool versus another.

95
00:05:04.850 --> 00:05:08.470
This is a really complicated question and
the answer depends not only on

96
00:05:08.470 --> 00:05:12.530
the capabilities of the tool itself but on
the skills of the analysts, the nature of

97
00:05:12.530 --> 00:05:16.650
the data environment and even the
organization in which an analyst works.

98
00:05:16.650 --> 00:05:20.640
In our video on data and analysis tools
we broadly discussed what functions

99
00:05:20.640 --> 00:05:24.070
each type of tool is designed to perform,
but we also saw that there's

100
00:05:24.070 --> 00:05:27.170
quite a bit of overlap in
the capabilities of different tools.

101
00:05:27.170 --> 00:05:30.430
What one analyst finds really
easy to do in one application

102
00:05:30.430 --> 00:05:34.110
another analyst might find more
intuitive in a different application.

103
00:05:34.110 --> 00:05:37.160
That having been said, here are a few
ideas that you can start with

104
00:05:37.160 --> 00:05:40.350
that I've drawn from my own experience
in leading analytical teams.

105
00:05:40.350 --> 00:05:43.660
But you'll have to discover what
works best in your environment.

106
00:05:43.660 --> 00:05:45.170
Let's start with Excel.

107
00:05:45.170 --> 00:05:47.792
Excel is really great for
quick and dirty analyses.

108
00:05:47.792 --> 00:05:51.517
Basic charts and graphs are for when you
want to share your analysis with business

109
00:05:51.517 --> 00:05:55.150
partners who don't have access
to more sophisticated tools.

110
00:05:55.150 --> 00:05:58.450
Excel also makes certain types
of manipulations really easy,

111
00:05:58.450 --> 00:06:01.570
like calculations that depend
on multiple rows of data.

112
00:06:01.570 --> 00:06:04.250
And it's a great environment for
trial and error around really

113
00:06:04.250 --> 00:06:08.009
complicated calculations since you
can see every formula in every cell.

114
00:06:09.020 --> 00:06:11.780
It also turns out that an awful
lot of financial modeling and

115
00:06:11.780 --> 00:06:14.230
business case development
is done in excel.

116
00:06:14.230 --> 00:06:17.810
When analysis outputs are used as
the inputs to these Excel models,

117
00:06:17.810 --> 00:06:20.580
it can be convenient just to
start in that environment.

118
00:06:20.580 --> 00:06:25.460
On the flipside, Excel is not necessarily
a great tool for sharing data broadly or

119
00:06:25.460 --> 00:06:28.150
for developing standard reports or
dashboards.

120
00:06:28.150 --> 00:06:30.210
There's also a size limitation.

121
00:06:30.210 --> 00:06:34.250
The baseline Excel product can only
handle about 1 million rows of data,

122
00:06:34.250 --> 00:06:37.160
which may not be large enough for
some analysis.

123
00:06:37.160 --> 00:06:41.790
Although with Microsoft's power pivot
plug-in, larger data sets can be handled.

124
00:06:41.790 --> 00:06:43.350
Even with less than 1 million rows,

125
00:06:43.350 --> 00:06:45.929
performance can be sluggish on
anything but high-end machine.

126
00:06:47.760 --> 00:06:50.940
Let's move on to business intelligence
tools, which includes standard reporting,

127
00:06:50.940 --> 00:06:54.060
data visualization, and
data exploration tools.

128
00:06:54.060 --> 00:06:57.860
These tools are a good choice for a wide
variety of analytical needs intended

129
00:06:57.860 --> 00:07:01.458
to make complex manipulation of data
easier and faster than other tools.

130
00:07:01.458 --> 00:07:05.593
It goes without saying that the analysis
requires extensive exploration or

131
00:07:05.593 --> 00:07:07.669
advance visualization techniques.

132
00:07:07.669 --> 00:07:11.656
Tools suited to those operations
will produce better results.

133
00:07:11.656 --> 00:07:15.680
Additionally, some organizations will
have tools that permanently sit on top of

134
00:07:15.680 --> 00:07:19.355
a data environment and provide pre-built
data structures like cubes, or

135
00:07:19.355 --> 00:07:23.730
predefined calculations that facilitate
repeated analytic operations.

136
00:07:23.730 --> 00:07:27.200
Business intelligence tools are also
preferable in cases when the output of

137
00:07:27.200 --> 00:07:31.240
analysis will be shared broadly or
turned into a standard report

138
00:07:31.240 --> 00:07:35.720
since they typically include more advanced
scheduling distribution functionality.

139
00:07:35.720 --> 00:07:36.810
Statistical modeling and

140
00:07:36.810 --> 00:07:39.820
advanced programming tools
are the obvious choice when we need to do

141
00:07:39.820 --> 00:07:44.890
highly sophisticated analysis, especially
using advanced analytic techniques.

142
00:07:44.890 --> 00:07:48.630
There are ways of incorporating more
advanced techniques into both Excel and

143
00:07:48.630 --> 00:07:52.520
other applications using plugins or
complimentary applications.

144
00:07:52.520 --> 00:07:55.550
In fact, we'll be using some
of these in later courses.

145
00:07:55.550 --> 00:07:58.540
However, few of these work
were really well at scale,

146
00:07:58.540 --> 00:08:01.036
where we need to analyze
very large datasets, or

147
00:08:01.036 --> 00:08:05.290
where we need to drive the results of our
analysis back into business operations.

148
00:08:05.290 --> 00:08:09.330
There are also cases where we want to
have a really high degree of flexibility,

149
00:08:09.330 --> 00:08:11.530
where we need very fast performance or

150
00:08:11.530 --> 00:08:16.010
where we want to build analytics directly
into other software or data processes.

151
00:08:16.010 --> 00:08:19.570
In these cases, we might want to
code our analytics from scratch

152
00:08:19.570 --> 00:08:23.430
using a programming language adept
at performing data manipulation.

153
00:08:23.430 --> 00:08:27.120
The downside of this approach is that
we have to build things from scratch.

154
00:08:27.120 --> 00:08:29.970
We give up a lot of the simplification
that's provided by more user

155
00:08:29.970 --> 00:08:30.529
friendly tools.

156
00:08:31.590 --> 00:08:33.910
So what should you take
away from this discussion?

157
00:08:33.910 --> 00:08:36.740
First, you should recognize that
there are a lot of different ways of

158
00:08:36.740 --> 00:08:39.300
navigating through the analytical process.

159
00:08:39.300 --> 00:08:42.890
But there are a few common ways that
we'll apply more often than not.

160
00:08:42.890 --> 00:08:44.742
Secondly, when it comes to tools,

161
00:08:44.742 --> 00:08:47.845
there's always more than one
way to accomplish anything.

162
00:08:47.845 --> 00:08:51.161
But there are some broad considerations
that can help you pick which tool is right

163
00:08:51.161 --> 00:08:51.710
for the job.

164
00:08:52.850 --> 00:08:56.520
But like we said at the beginning of this
module, the most powerful tool you have as

165
00:08:56.520 --> 00:08:59.510
a data analyst is the one
right here between your ears.

166
00:08:59.510 --> 00:09:01.102
If you have a solid thought process and

167
00:09:01.102 --> 00:09:03.689
a good foundation of knowledge
about your data environment,

168
00:09:03.689 --> 00:09:06.893
you'll be well down the road to success
regardless of the tools you choose.