WEBVTT

1
00:00:00.410 --> 00:00:03.680
The human mind is an exceptional
problem solving machine.

2
00:00:03.680 --> 00:00:07.010
Our ability to think broadly and make
abstract connections in the world around

3
00:00:07.010 --> 00:00:10.230
us allows us to interpret nuances
in information that are so

4
00:00:10.230 --> 00:00:13.050
far beyond the capabilities of machines.

5
00:00:13.050 --> 00:00:14.745
However, we are fallible.

6
00:00:14.745 --> 00:00:18.835
And it turns out there's been all sorts of
research that's identified very specific

7
00:00:18.835 --> 00:00:22.415
ways in which we make mistakes,
and there are a lot of them.

8
00:00:22.415 --> 00:00:27.415
Take a look at this diagram categorized
by Buster Benson and arranged by JM3.

9
00:00:27.415 --> 00:00:31.465
Each one of these items is a specific
way we alter information to deviate from

10
00:00:31.465 --> 00:00:32.385
reality.

11
00:00:32.385 --> 00:00:37.185
We call these phenomena cognitive biases,
and most of us are susceptible to most,

12
00:00:37.185 --> 00:00:38.740
if not all, of them.

13
00:00:38.740 --> 00:00:41.560
Cognitive biases can impact
the way we look at data and

14
00:00:41.560 --> 00:00:44.370
influence our interpretations of analysis.

15
00:00:44.370 --> 00:00:47.650
We'll be hard pressed to cover all
this biases in one course let alone

16
00:00:47.650 --> 00:00:49.160
in one video.

17
00:00:49.160 --> 00:00:51.590
But what we will do is look at
some of the most common and

18
00:00:51.590 --> 00:00:55.680
potentially damaging biases that
play on to our analytical process.

19
00:00:55.680 --> 00:01:00.590
Specifically we'll look at
confirmation bias, the framing effect,

20
00:01:00.590 --> 00:01:06.720
availability, or vividness bias, anchoring
and fundamental attribution error.

21
00:01:06.720 --> 00:01:08.490
Let's start with confirmation bias.

22
00:01:08.490 --> 00:01:12.660
Confirmation bias is the tendency to favor
information that confirms one's beliefs or

23
00:01:12.660 --> 00:01:13.920
hypotheses.

24
00:01:13.920 --> 00:01:16.870
There are really two ways that
we can exhibit this bias.

25
00:01:16.870 --> 00:01:19.640
The first is by selectively
gathering information,

26
00:01:19.640 --> 00:01:23.600
that is we only seek out data that
would serve to support a hypothesis and

27
00:01:23.600 --> 00:01:27.140
fail to seek out data that
might disprove the hypothesis.

28
00:01:27.140 --> 00:01:30.340
The second is by selectively
interpreting information.

29
00:01:30.340 --> 00:01:33.960
This happens when we only focus on
data that supports our hypothesis

30
00:01:33.960 --> 00:01:36.410
even when we have data that refutes it.

31
00:01:36.410 --> 00:01:38.000
Here are a couple of examples.

32
00:01:38.000 --> 00:01:41.860
Suppose we feel like our customer care
centers are not doing a good job.

33
00:01:41.860 --> 00:01:45.560
We decide to send a survey to a sample
of customers that have called in, but

34
00:01:45.560 --> 00:01:48.700
exclude customers who have
received credits on their bills

35
00:01:48.700 --> 00:01:52.930
because these might be customers
whose satisfaction has been bought.

36
00:01:52.930 --> 00:01:56.290
This would be an example of
selectively seeking out information.

37
00:01:56.290 --> 00:01:58.350
Let's say that we were
analyzing sales results for

38
00:01:58.350 --> 00:02:00.210
different locations in a region.

39
00:02:00.210 --> 00:02:04.400
We believe their performance is strong but
when we look at the data we see that there

40
00:02:04.400 --> 00:02:08.470
are a couple of locations that are
dragging down total sales below target.

41
00:02:08.470 --> 00:02:10.560
We decide there must be something
wrong with the data and

42
00:02:10.560 --> 00:02:13.430
we exclude those locations
from the analysis.

43
00:02:13.430 --> 00:02:16.810
This would be an example of
selectively interpreting information.

44
00:02:16.810 --> 00:02:19.740
In business there is often
pressure to show good results.

45
00:02:19.740 --> 00:02:23.760
And this can subconsciously play into
how we gather and interpret information.

46
00:02:23.760 --> 00:02:27.150
People can also have strong beliefs
about how things should be done, so

47
00:02:27.150 --> 00:02:30.250
you tend to go after information
that supports their agenda.

48
00:02:30.250 --> 00:02:33.330
As data analysts, we should try
our best to remain objective and

49
00:02:33.330 --> 00:02:35.170
avoid both of these traps.

50
00:02:35.170 --> 00:02:36.980
So that's confirmation bias.

51
00:02:36.980 --> 00:02:39.100
Let's move on to the framing effect.

52
00:02:39.100 --> 00:02:42.930
The framing effect is the tendency to draw
different conclusions from information

53
00:02:42.930 --> 00:02:44.690
based on how it's presented.

54
00:02:44.690 --> 00:02:47.830
To illustrate this we'll take
a classic example from research and

55
00:02:47.830 --> 00:02:50.700
recast it in the hypothetical
business context.

56
00:02:50.700 --> 00:02:54.550
Let's say we've completed an analysis
of how the restructure 600 bad business

57
00:02:54.550 --> 00:02:58.830
investments and we're presenting
options to a set of decision makers.

58
00:02:58.830 --> 00:03:03.500
If we do nothing,
all 600 investments will lose $100 each.

59
00:03:03.500 --> 00:03:07.510
If we do something to an investment,
we have some chance it will make $100.

60
00:03:07.510 --> 00:03:10.305
Otherwise, it still loses $100.

61
00:03:10.305 --> 00:03:14.140
Let's say we outline the above scenario
and present the options as follows.

62
00:03:14.140 --> 00:03:17.725
Option A,
200 investments will make $100 each.

63
00:03:17.725 --> 00:03:22.815
Option B, there's a one-third probability
that all 600 investments will make

64
00:03:22.815 --> 00:03:28.060
$100 each and a two-thirds probability
that all 600 will lose $100 each.

65
00:03:29.450 --> 00:03:32.220
Research suggests that most
people would prefer option A,

66
00:03:32.220 --> 00:03:35.220
when the options
are presented in this way.

67
00:03:35.220 --> 00:03:38.420
However, let's say that we
presented the options like this.

68
00:03:38.420 --> 00:03:42.790
Option A,
400 investments will loose $100 each.

69
00:03:42.790 --> 00:03:46.822
Option B, there's a one-third probability
that none of the investments will loose

70
00:03:46.822 --> 00:03:50.970
$100 and a two-thirds probability that
all the investments will loose $100.

71
00:03:52.170 --> 00:03:56.390
In this case the resource suggest
that most people now choose option B.

72
00:03:56.390 --> 00:03:57.545
Now, if you read closely,

73
00:03:57.545 --> 00:04:01.130
you'll see that both cases
describe exactly the same choice.

74
00:04:01.130 --> 00:04:04.830
Option A is a certainty
that loses $20,000 overall.

75
00:04:04.830 --> 00:04:09.517
And option B has a one-third chance of
making $60,000 and a two-third chance of

76
00:04:09.517 --> 00:04:15.030
losing $60,000 with the expected
outcome of the same net $20,000 loss.

77
00:04:15.030 --> 00:04:18.420
Rationally, decision makers should
make the same choice in both scenarios

78
00:04:18.420 --> 00:04:19.470
but they don't.

79
00:04:19.470 --> 00:04:22.060
It turns out that framing
things in a positive way

80
00:04:22.060 --> 00:04:25.260
can elicit different results than in
framing them in a more negative way.

81
00:04:25.260 --> 00:04:28.960
As data analysts, we need to keep this
in mind both as we look at options and

82
00:04:28.960 --> 00:04:30.520
make our own conclusions.

83
00:04:30.520 --> 00:04:34.520
As well as how we present options and
recommend results to our decision maker.

84
00:04:34.520 --> 00:04:37.800
It turns out there are other ways in
which how we are exposed to information

85
00:04:37.800 --> 00:04:39.630
influences our thinking.

86
00:04:39.630 --> 00:04:43.470
Availability or vividness bias is
the tendency to believe recent or

87
00:04:43.470 --> 00:04:45.750
vivid events are more likely to occur.

88
00:04:45.750 --> 00:04:49.240
There are all sorts of common examples
of this, like how people perceive

89
00:04:49.240 --> 00:04:52.390
the risk of a plane crash or
getting attacked by a shark.

90
00:04:52.390 --> 00:04:54.530
Both events are exceedingly rare, but

91
00:04:54.530 --> 00:04:57.800
are often perceived to happen more
frequently than they actually do.

92
00:04:57.800 --> 00:05:00.440
In the business context,
when will we see this bias

93
00:05:00.440 --> 00:05:04.970
impact analysis is when small samples
of input highlight touchy topics.

94
00:05:04.970 --> 00:05:08.160
For example, let's say we have
a focus group of seven people, and

95
00:05:08.160 --> 00:05:10.800
two of them say they have
a problem with our product.

96
00:05:10.800 --> 00:05:12.230
This input is vivid.

97
00:05:12.230 --> 00:05:15.370
Not only is there a problem but
we're hearing about the problem through

98
00:05:15.370 --> 00:05:18.700
a personal and possibly passionate
interaction with a customer.

99
00:05:18.700 --> 00:05:20.420
It's also two out of seven people.

100
00:05:20.420 --> 00:05:24.360
So we might jump to the conclusion that
nearly 30% of our products have issues.

101
00:05:24.360 --> 00:05:25.770
This may or may not be that case.

102
00:05:25.770 --> 00:05:29.039
But cognitively we tend to over
value these vivid examples.

103
00:05:30.070 --> 00:05:32.320
Here's a real example
of something similar.

104
00:05:32.320 --> 00:05:34.760
An executive at a wireless
carrier had spent an hour or so

105
00:05:34.760 --> 00:05:37.850
in a call center listening to
interactions with customers.

106
00:05:37.850 --> 00:05:41.250
Of the ten calls he heard, two of them
included complaints about dropped or

107
00:05:41.250 --> 00:05:42.530
blocked calls.

108
00:05:42.530 --> 00:05:45.240
He was already concerned
about network quality, so

109
00:05:45.240 --> 00:05:48.210
he asked the analytics team to prove
that high levels of dropped and

110
00:05:48.210 --> 00:05:51.240
blocked calls were leaving
the customers cancelling their service.

111
00:05:51.240 --> 00:05:53.910
It turns out that not only was
the actual rate of dropped and

112
00:05:53.910 --> 00:05:58.750
blocked calls extremely low, about 1%,
even when viewed at the customer level.

113
00:05:58.750 --> 00:06:01.700
But, there was no discernible
relationship to cancellation.

114
00:06:01.700 --> 00:06:05.250
In this example, not only was there
availability bias involved, but

115
00:06:05.250 --> 00:06:07.150
also confirmation bias.

116
00:06:07.150 --> 00:06:10.830
The executive was quick to pick up on data
that supported his belief that network

117
00:06:10.830 --> 00:06:12.560
quality was an issue.

118
00:06:12.560 --> 00:06:15.775
What's really interesting about this
example is not that it happened,

119
00:06:15.775 --> 00:06:17.682
these type of analyses are pretty common.

120
00:06:17.682 --> 00:06:21.887
But the exact same request was made two
more times by two different leaders, and

121
00:06:21.887 --> 00:06:25.906
despite the results of the initial
analysis two more analyses were done only

122
00:06:25.906 --> 00:06:28.360
to come to the same conclusion.

123
00:06:28.360 --> 00:06:31.060
It turns out that some
biases are hard to overcome.

124
00:06:31.060 --> 00:06:34.780
The good news is, we're actually able
to use data to show the truth and

125
00:06:34.780 --> 00:06:38.530
prevent unnecessary actions that
wouldn't have had any impact.

126
00:06:38.530 --> 00:06:40.900
Okay, let's talk about anchoring.

127
00:06:40.900 --> 00:06:42.710
Anchoring is our tendency to focus or

128
00:06:42.710 --> 00:06:46.870
rely too heavily on the first piece of
information that's available to us.

129
00:06:46.870 --> 00:06:51.010
This bias is regularly exploited in
areas like pricing and negotiation.

130
00:06:51.010 --> 00:06:55.580
For example, shoppers may respond more
favorably to a product that was priced at

131
00:06:55.580 --> 00:07:00.239
$1,000 and marked down to $600 than
one that was simply listed for $600.

132
00:07:00.239 --> 00:07:04.342
This is because the shopper
initially anchors at $1,000 and

133
00:07:04.342 --> 00:07:08.240
relative to $1,000,
$600 seems like a good deal.

134
00:07:08.240 --> 00:07:11.770
The way we look at data can
also be subject to anchoring.

135
00:07:11.770 --> 00:07:15.690
Let's say that we're responsible for
doing the monthly 12-month sales forecast.

136
00:07:15.690 --> 00:07:17.930
We make some assumptions and
run our first forecast,

137
00:07:17.930 --> 00:07:20.030
which predicts average sales growth of 8%.

138
00:07:20.030 --> 00:07:22.680
The next month we run
the same forecast and

139
00:07:22.680 --> 00:07:26.590
come up with an average
sales growth of 14%.

140
00:07:26.590 --> 00:07:30.200
Even though we think our methodology
is sound, because the estimate is so

141
00:07:30.200 --> 00:07:34.050
much higher than the first forecast,
we back off some of our assumptions and

142
00:07:34.050 --> 00:07:36.800
take the forecast down to
a more reasonable 12%.

143
00:07:36.800 --> 00:07:37.900
In this case,

144
00:07:37.900 --> 00:07:42.380
how we interpreted the second forecast
was anchored by our initial forecast.

145
00:07:42.380 --> 00:07:45.160
As analysts,
we need to question our analyses.

146
00:07:45.160 --> 00:07:48.520
But it's also important to question
why we're questioning our analysis.

147
00:07:48.520 --> 00:07:51.120
And make sure we're asking
ourselves the right questions.

148
00:07:51.120 --> 00:07:55.210
The last cognitive bias we'll discuss is
called the fundamental attribution error.

149
00:07:55.210 --> 00:07:58.650
Fundamental attribution error impacts
how we interpret things that we observe

150
00:07:58.650 --> 00:07:59.800
people doing.

151
00:07:59.800 --> 00:08:02.619
Specifically we have a tendency
to focus on attributes or

152
00:08:02.619 --> 00:08:05.616
intentions of the person
themselves versus the situation or

153
00:08:05.616 --> 00:08:09.020
the environment when explaining
a person's behavior.

154
00:08:09.020 --> 00:08:11.960
Where this comes into play in
analytics is often in the process of

155
00:08:11.960 --> 00:08:16.930
translating analytical observations
into strategies and tactics for action.

156
00:08:16.930 --> 00:08:20.090
If we misinterpret why and
how customers behave,

157
00:08:20.090 --> 00:08:23.785
we may end up taking action that is
ineffective or counterproductive.

158
00:08:23.785 --> 00:08:27.395
One of the more interesting places
I've seen this come up is in fraud.

159
00:08:27.395 --> 00:08:31.075
I'm using the air quotes here because
the fraud itself is a negative term.

160
00:08:31.075 --> 00:08:34.185
It implies dishonesty and
an active attempt to deceive.

161
00:08:34.185 --> 00:08:38.395
Let's suppose that we sent a 50% discount
code out to a set of new customers to use

162
00:08:38.395 --> 00:08:39.610
on our website.

163
00:08:39.610 --> 00:08:41.820
Our intent with that it'd be for
new customers only,

164
00:08:41.820 --> 00:08:44.490
and that the discount
can only be used once.

165
00:08:44.490 --> 00:08:48.430
In fact, it clearly says just that in
the terms and conditions of the offer.

166
00:08:48.430 --> 00:08:52.300
However, when we see the discount
being used far more than expected,

167
00:08:52.300 --> 00:08:56.140
we look at the data and find that it's
being used by some existing customers and

168
00:08:56.140 --> 00:08:58.680
in many cases more than once per customer.

169
00:08:58.680 --> 00:09:02.060
Very quickly, ideas are generated
about how to stop the fraud and

170
00:09:02.060 --> 00:09:06.020
identify abusers, cancel any pending
purchases and block them from the site.

171
00:09:07.090 --> 00:09:08.540
What just happened here?

172
00:09:08.540 --> 00:09:12.650
We've implicitly assumed anyone using the
code we sent out in a way we don't intend

173
00:09:12.650 --> 00:09:16.710
are bad actors, and
we moved right to punitive solutions.

174
00:09:16.710 --> 00:09:18.270
Are these really bad people or

175
00:09:18.270 --> 00:09:21.890
are they just acting rationally in
the environment we created for them?

176
00:09:21.890 --> 00:09:25.500
In my experience it's surely wise to give
people the benefit of the doubt and to

177
00:09:25.500 --> 00:09:29.735
focus on the structures we create for them
in identifying problems and solutions.

178
00:09:29.735 --> 00:09:31.835
This helps avoid attribution error.

179
00:09:31.835 --> 00:09:35.275
Again, these are only a few of the many
ways we as humans are prone to

180
00:09:35.275 --> 00:09:37.625
misinterpret information in situations.

181
00:09:37.625 --> 00:09:40.315
If you find this interesting I
encourage you to explore some of

182
00:09:40.315 --> 00:09:42.755
the other biases we saw
in the larger framework.

183
00:09:42.755 --> 00:09:46.635
But for now the key takeaway from this
video is that even though we deal in hard

184
00:09:46.635 --> 00:09:49.435
data we can still make mistakes
in how we interpret and

185
00:09:49.435 --> 00:09:53.820
communicate that data, as well as in
how we frame options for taking action.

186
00:09:53.820 --> 00:09:56.443
By being aware of these biases and
how they effect us,

187
00:09:56.443 --> 00:10:00.313
we can train ourselves to avoid them and
minimize their influence on our work.