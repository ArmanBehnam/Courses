WEBVTT

1
00:00:00.320 --> 00:00:01.700
Welcome back.

2
00:00:01.700 --> 00:00:04.610
In our last video, we started
walking through a case study of how

3
00:00:04.610 --> 00:00:06.520
one company applied a multidimensional and

4
00:00:06.520 --> 00:00:10.410
analytical approach to decide on
a strategy for acquiring new customers.

5
00:00:10.410 --> 00:00:13.365
When we left of, the team had
determined the analytic outputs,

6
00:00:13.365 --> 00:00:16.705
constructed an analytic design and
verified that all the data needed for

7
00:00:16.705 --> 00:00:18.820
the analysis was available.

8
00:00:18.820 --> 00:00:22.930
In this video, we'll dig in deeper and
see how the analysis was actually done.

9
00:00:22.930 --> 00:00:25.580
As a reminder,
here's the high level process.

10
00:00:25.580 --> 00:00:29.160
A Customer Segmentation effort and
Geographic Segmentation effort would

11
00:00:29.160 --> 00:00:32.620
jointly guide internal and
external analysis streams.

12
00:00:32.620 --> 00:00:36.130
Those strings would come together to
characterize a set of strategic options

13
00:00:36.130 --> 00:00:38.800
that would be compared and acted upon.

14
00:00:38.800 --> 00:00:42.570
Let's look at each step, starting with
the Customer Segmentation effort.

15
00:00:42.570 --> 00:00:44.030
As we mentioned in the last video,

16
00:00:44.030 --> 00:00:46.970
a key consideration in the analysis
was the ability to link

17
00:00:46.970 --> 00:00:50.633
segments in the current customer base
to identifiable segments in the market.

18
00:00:50.633 --> 00:00:54.386
It would certainly be possible to
construct customer segments with internal

19
00:00:54.386 --> 00:00:56.928
data using clustering analysis or
other methods and

20
00:00:56.928 --> 00:00:59.780
the company had actually
done that already.

21
00:00:59.780 --> 00:01:03.720
However, it turns out that it's really
difficult to use this kind of segmentation

22
00:01:03.720 --> 00:01:06.424
externally with any degree of precision.

23
00:01:06.424 --> 00:01:10.060
The reason is, that the internal models
built using all sorts of information we

24
00:01:10.060 --> 00:01:13.960
have on current customers that we
don't have on potential customers.

25
00:01:13.960 --> 00:01:17.290
So, we often can't segment
the customer until after we have them.

26
00:01:17.290 --> 00:01:21.750
This is useful for a lot of things, but
not necessarily for customer acquisition.

27
00:01:21.750 --> 00:01:24.630
A number of third party companies had
developed products that help with

28
00:01:24.630 --> 00:01:25.850
this problem.

29
00:01:25.850 --> 00:01:29.990
Companies like Claritas, Acxiom,
Experian and others have developed off

30
00:01:29.990 --> 00:01:35.112
the shelf segmentation schemas that try to
capture key drivers of consumer behavior.

31
00:01:35.112 --> 00:01:38.580
Some are driven more by demographics,
others more by psychographics or

32
00:01:38.580 --> 00:01:40.120
technographics.

33
00:01:40.120 --> 00:01:42.740
Typically, these schemas break
all possible customers into 50 or

34
00:01:42.740 --> 00:01:47.200
60 different segments, and most of them
can be matched directly to customers in

35
00:01:47.200 --> 00:01:50.300
the market, making them highly actionable.

36
00:01:50.300 --> 00:01:53.490
The way we make the linkage between our
current customers and those third party

37
00:01:53.490 --> 00:01:58.140
schemas is usually by having the third
party company score our customer base.

38
00:01:58.140 --> 00:02:00.400
We won't get the details
of how this is done.

39
00:02:00.400 --> 00:02:01.100
But ultimately,

40
00:02:01.100 --> 00:02:04.980
we send the company a list of customers
with some basic information and they send

41
00:02:04.980 --> 00:02:08.500
the list back with the appropriate
segment attached each customer.

42
00:02:08.500 --> 00:02:11.490
We then load this information
back into our database.

43
00:02:11.490 --> 00:02:14.870
In this case, the company didn't
know which schema would work best.

44
00:02:14.870 --> 00:02:16.626
So, the first step was to assess four or

45
00:02:16.626 --> 00:02:20.690
five the available schemas to see
which produced the best results.

46
00:02:20.690 --> 00:02:24.140
Of course, that begs the question
as to what best means?.

47
00:02:24.140 --> 00:02:28.970
Remember that in segmentation, we seek to
maximize homogeneity within segments and

48
00:02:28.970 --> 00:02:31.050
heterogeneity between segments.

49
00:02:31.050 --> 00:02:34.485
In other words, we want members of
a segment to look like each other but

50
00:02:34.485 --> 00:02:36.620
look different than other segments.

51
00:02:36.620 --> 00:02:40.220
It turns out that the mechanics of the
assessment were fairly straightforward.

52
00:02:40.220 --> 00:02:42.868
A sample Customer List was
sent to each provider and

53
00:02:42.868 --> 00:02:44.725
returned with segments attached.

54
00:02:44.725 --> 00:02:48.270
The team then looked at a variety
of key metrics by segment.

55
00:02:48.270 --> 00:02:53.560
Average revenue, historical cancel rate,
usage behaviors, and a few other metric.

56
00:02:53.560 --> 00:02:54.640
Calculating the mean and

57
00:02:54.640 --> 00:02:59.510
variance around each one, as well as
the variance across the segment means.

58
00:02:59.510 --> 00:03:03.280
Using this information, it was fairly
clear that one schema in particular did

59
00:03:03.280 --> 00:03:07.740
a much better job of establishing segments
that had lower internal variance and

60
00:03:07.740 --> 00:03:12.000
larger variance across the means for
just about all relevant metrics.

61
00:03:12.000 --> 00:03:16.440
This segmentation schema was chosen as
the move forward option in the analysis.

62
00:03:16.440 --> 00:03:19.130
The second step in the analysis
was to find the right way to

63
00:03:19.130 --> 00:03:21.170
group geographic markets.

64
00:03:21.170 --> 00:03:25.090
In general, large scale acquisition
efforts incorporate mass advertising to

65
00:03:25.090 --> 00:03:28.235
support more tactical programs and
product offerings.

66
00:03:28.235 --> 00:03:31.800
There are limits to how finely mass
media efforts can be targeted.

67
00:03:31.800 --> 00:03:36.490
For example, we could create confusion if
we try to push one message to one town and

68
00:03:36.490 --> 00:03:39.320
a completely different message
to the next town over.

69
00:03:39.320 --> 00:03:41.520
These efforts can also be expensive and

70
00:03:41.520 --> 00:03:45.340
there are clear economies of scale
to having uniformity across markets.

71
00:03:45.340 --> 00:03:48.840
So the idea here was, to find the simplest
way that markets could be arranged to

72
00:03:48.840 --> 00:03:52.080
reflect differences in the types
of customers in those markets.

73
00:03:52.080 --> 00:03:54.410
The team thought about using
a clustering algorithm,

74
00:03:54.410 --> 00:03:57.280
like k-means clustering to group markets.

75
00:03:57.280 --> 00:04:00.610
However, they were concerned about
how those clusters would map

76
00:04:00.610 --> 00:04:04.580
back to the company's organization and
market communication structures.

77
00:04:04.580 --> 00:04:08.180
This is where the art and
science of data analytics come together.

78
00:04:08.180 --> 00:04:12.480
The team decided to try to take more of a
heuristic approach that would account for

79
00:04:12.480 --> 00:04:16.750
the company's regional structure and the
way it was currently looking at markets.

80
00:04:16.750 --> 00:04:21.067
Here's the actual artifact that
the team used to describe the approach.

81
00:04:21.067 --> 00:04:25.228
We won't walk through this in detail, but
the team made a qualitative determination

82
00:04:25.228 --> 00:04:28.448
as to how many geographic
segments would be actionable.

83
00:04:28.448 --> 00:04:31.530
Then, examined the few different ways
of grouping markets using a more

84
00:04:31.530 --> 00:04:33.310
quantitative assessment.

85
00:04:33.310 --> 00:04:36.620
Similar to the way the customer
segmentation schemas were examined,

86
00:04:36.620 --> 00:04:40.310
the team looked to combine geographies
that were similar to each other and

87
00:04:40.310 --> 00:04:43.910
land on a manageable number of groups
that were different from each other.

88
00:04:43.910 --> 00:04:46.680
In this case, what they looked
at was the concentration of

89
00:04:46.680 --> 00:04:51.450
customer segments present at each level of
the geographic hierarchy and the number of

90
00:04:51.450 --> 00:04:56.430
segments required to account for 50%
of the population in a geographic area.

91
00:04:56.430 --> 00:04:59.580
The smaller the number,
the more uniform the area.

92
00:04:59.580 --> 00:05:03.190
Ultimately, the team determined that only
three segments were required to capture

93
00:05:03.190 --> 00:05:05.960
significant differences
in market composition and

94
00:05:05.960 --> 00:05:08.980
that these segments could be
based on population density.

95
00:05:08.980 --> 00:05:13.910
Namely, major markets with more than one
million people, urban markets with more

96
00:05:13.910 --> 00:05:17.730
than 100,000 people, and rural
markets with less than 100,000 people.

97
00:05:18.790 --> 00:05:21.780
Here's how the relationship between
those geographic segments and

98
00:05:21.780 --> 00:05:25.470
the most common customer segments was
illustrated in the later presentation.

99
00:05:27.080 --> 00:05:29.820
With customer segments and
geographic segments defined,

100
00:05:29.820 --> 00:05:32.740
the team was ready to move to
the next phase of the analysis.

101
00:05:32.740 --> 00:05:35.790
Establishing customer value by
segment using internal data.

102
00:05:36.920 --> 00:05:39.830
The approach the team took to get
to the value by segment was more or

103
00:05:39.830 --> 00:05:43.660
less driven by a classic customer
lifetime value calculation.

104
00:05:43.660 --> 00:05:47.052
You may recall we presented a really
simple version of of a customer lifetime

105
00:05:47.052 --> 00:05:49.350
value equation in an earlier video.

106
00:05:49.350 --> 00:05:53.075
Again, this equation basically just
looks at customer level revenues and

107
00:05:53.075 --> 00:05:56.850
cost over time and expresses them to
in today's dollars by accounting for

108
00:05:56.850 --> 00:05:58.300
the time value of money.

109
00:05:58.300 --> 00:06:01.591
Now ideally, we'd be able to get
a complete accounting of all revenues and

110
00:06:01.591 --> 00:06:04.110
cost that could be applied to a customer.

111
00:06:04.110 --> 00:06:08.900
In reality, it tends to be pretty easy
to get revenue, but harder to get cost.

112
00:06:08.900 --> 00:06:12.160
In this case, the team was able to get
good revenue information from the billing

113
00:06:12.160 --> 00:06:15.240
system, but limited the cost
side of the calculation to

114
00:06:15.240 --> 00:06:18.420
things that might differ
significantly across customers.

115
00:06:18.420 --> 00:06:22.980
These included Direct Acquisition Costs,
Activity and Usage-Based Costs and

116
00:06:22.980 --> 00:06:25.490
Uses of Services like Customer Care.

117
00:06:25.490 --> 00:06:29.055
However, the most important part of
the customer lifetime value equation is

118
00:06:29.055 --> 00:06:31.850
lifetime, or n in our version.

119
00:06:31.850 --> 00:06:34.366
The team used two methods
to calculate lifetime.

120
00:06:34.366 --> 00:06:38.580
The first was, a simple Historical
Analysis of customers by segment,

121
00:06:38.580 --> 00:06:42.660
which looked at the lifetime of customers
who had canceled over the past two years.

122
00:06:42.660 --> 00:06:43.430
The second was,

123
00:06:43.430 --> 00:06:47.830
using a Predictive Model that estimated
the lifetime of current customers.

124
00:06:47.830 --> 00:06:48.920
In another case of art and

125
00:06:48.920 --> 00:06:52.830
science coming together, the team blended
the two methods to establish a measure of

126
00:06:52.830 --> 00:06:57.650
lifetime they thought would better reflect
both existing and prospective customers.

127
00:06:57.650 --> 00:07:00.180
From there,
the mechanics were pretty straightforward.

128
00:07:00.180 --> 00:07:03.930
Each customer was assigned a customer
segment and geographic segment.

129
00:07:03.930 --> 00:07:07.150
And expected lifetime value was
calculated for each customer and

130
00:07:07.150 --> 00:07:10.010
aggregated into the average
expected lifetime value for

131
00:07:10.010 --> 00:07:13.240
each combined customer
by geographic segment.

132
00:07:13.240 --> 00:07:15.760
A few other summary metrics
were also computed for

133
00:07:15.760 --> 00:07:20.166
each combined segment, namely the number
of customers and average revenue.

134
00:07:20.166 --> 00:07:23.950
All of this work was done in an analytical
database environment primarily

135
00:07:23.950 --> 00:07:24.810
using SQL coding

136
00:07:44.575 --> 00:07:46.271
In terms of customer lifetime value,

137
00:07:46.271 --> 00:07:49.205
there's one other small
detail worth discussing.

138
00:07:49.205 --> 00:07:52.375
Since only a subset of cost items
were used in the calculation,

139
00:07:52.375 --> 00:07:55.815
the absolute value of that
metric was not very meaningful.

140
00:07:55.815 --> 00:07:56.475
For example,

141
00:07:56.475 --> 00:08:00.250
just because a value was positive did
not mean a customer's profitable.

142
00:08:00.250 --> 00:08:04.020
To avoid confusion, the team normalized
the lifetime value measure into something

143
00:08:04.020 --> 00:08:08.110
they called a Relative Desirability
Measure where positive number showed above

144
00:08:08.110 --> 00:08:12.380
average profitability and negative number
showed below average profitability.

145
00:08:12.380 --> 00:08:14.888
These measures would help the team
characterize four of the seven

146
00:08:14.888 --> 00:08:17.600
scenarios that were requested
from the leadership team.

147
00:08:17.600 --> 00:08:21.500
Namely, maximizing the customer base,
maximizing total revenue,

148
00:08:21.500 --> 00:08:26.210
maximizing revenue per customer and
maximizing customer profitability.

149
00:08:26.210 --> 00:08:30.210
However, there were three other scenarios
that the team needed to consider as well.

150
00:08:30.210 --> 00:08:34.320
Focusing on teens, tweens and young
adults, focusing on minority groups, and

151
00:08:34.320 --> 00:08:35.990
focusing on families.

152
00:08:35.990 --> 00:08:37.260
To construct similar measures,

153
00:08:37.260 --> 00:08:41.500
the team used additional information
from the customer segmentation schema.

154
00:08:41.500 --> 00:08:44.690
Each segment had information
about its demographic makeup,

155
00:08:44.690 --> 00:08:47.310
namely the percentage of
the segment made up by each group.

156
00:08:48.360 --> 00:08:52.220
Age, minority and family segment metrics
were constructed by using simple weighted

157
00:08:52.220 --> 00:08:55.840
averages like the following, which
calculates average lifetime value for

158
00:08:55.840 --> 00:08:57.620
the Young Adults segment.

159
00:08:57.620 --> 00:09:00.419
This was repeated for
each metric across each type of segment.

160
00:09:01.660 --> 00:09:04.740
Now, the team had the performance
metrics they needed for all segments and

161
00:09:04.740 --> 00:09:08.670
scenarios, but they also need to decide
the acquisition opportunities and

162
00:09:08.670 --> 00:09:10.780
did this using external market data.

163
00:09:10.780 --> 00:09:13.410
Broadly speaking,
the overall process of analyzing

164
00:09:13.410 --> 00:09:17.600
external data involve a lot of
qualitative and semi-quantitative work.

165
00:09:17.600 --> 00:09:20.671
As we've mentioned in the last video,
there were different views within

166
00:09:20.671 --> 00:09:24.030
the organization on what strategies
the company should take.

167
00:09:24.030 --> 00:09:26.980
Each of these constituent groups
needed to be attended to.

168
00:09:26.980 --> 00:09:30.810
So, a lot of formative material was
produced to validate assumptions and

169
00:09:30.810 --> 00:09:34.510
explore the nuances of each
scenario relative to the market.

170
00:09:34.510 --> 00:09:36.630
We won't go into all that detail here.

171
00:09:36.630 --> 00:09:39.800
Suffice to say, there were a number
of small ad hoc analyses and

172
00:09:39.800 --> 00:09:42.650
renderings of external data
that contributed to the work.

173
00:09:42.650 --> 00:09:43.450
What we will do is,

174
00:09:43.450 --> 00:09:47.420
walk through the critical analysis that
fed directly into the option summaries.

175
00:09:47.420 --> 00:09:50.990
The critical data used in the external
market analysis was obtained through

176
00:09:50.990 --> 00:09:53.840
a tool provided by the customer
segmentation provider.

177
00:09:53.840 --> 00:09:58.160
This data provided population volumes for
each segment in each market.

178
00:09:58.160 --> 00:10:01.330
It also provided the degree to
which each segment was penetrated

179
00:10:01.330 --> 00:10:03.820
with the type of technology or
company provided.

180
00:10:03.820 --> 00:10:08.440
As well as the likelihood of each segment
to adapt that technology in the near term.

181
00:10:08.440 --> 00:10:11.260
There are a few ways in
which this data was used.

182
00:10:11.260 --> 00:10:14.967
One of them was to construct what the team
called a penetration likelihood map,

183
00:10:14.967 --> 00:10:18.842
which looked at the Market Penetration of
segments versus the company's existing

184
00:10:18.842 --> 00:10:20.180
Market Share.

185
00:10:20.180 --> 00:10:24.280
The basic structure and interpretation
of the map looks something like this.

186
00:10:24.280 --> 00:10:27.320
As an example,
if the company's market share was high and

187
00:10:27.320 --> 00:10:30.220
the penetration was low,
it basically meant that the company

188
00:10:30.220 --> 00:10:35.190
already had most of a lagging segment,
and the acquisition opportunity was low.

189
00:10:35.190 --> 00:10:37.570
Conversely, if the market
share was low but

190
00:10:37.570 --> 00:10:41.510
the penetration was high, there was
an opportunity for acquisition, but

191
00:10:41.510 --> 00:10:45.070
would have to be done by stealing
customers from competitors.

192
00:10:45.070 --> 00:10:47.770
These customers were known as Switchers.

193
00:10:47.770 --> 00:10:50.068
This mapping helped assess
the overall size and

194
00:10:50.068 --> 00:10:53.180
relative probability of acquiring
customers in each segment.

195
00:10:54.520 --> 00:10:58.040
Additional market information about the
relative growth rates in age groups and

196
00:10:58.040 --> 00:11:00.960
minority populations was
incorporated as well

197
00:11:00.960 --> 00:11:04.910
to project the relative growth of
each customer segment in the future.

198
00:11:04.910 --> 00:11:08.270
Mechanically, this was more or
less the opposite of what was done with

199
00:11:08.270 --> 00:11:10.970
the weighted average in
the internal data analysis.

200
00:11:10.970 --> 00:11:15.417
So, the size of a customer segment in a
feature period t, was internally weighted

201
00:11:15.417 --> 00:11:18.833
by the growth rate of each group
within the segment at period t,

202
00:11:18.833 --> 00:11:22.581
relative to the current size of
the group at the time of the analysis.

203
00:11:22.581 --> 00:11:24.366
A combination of hard market data and

204
00:11:24.366 --> 00:11:28.169
strategic assumptions were used to
assemble the external information needed

205
00:11:28.169 --> 00:11:31.030
to bring the overall
analysis back together.

206
00:11:31.030 --> 00:11:35.915
Namely, the populations of each segment in
each market and a set of probabilities of

207
00:11:35.915 --> 00:11:39.965
acquiring those segments based on
the segment market share, penetration and

208
00:11:39.965 --> 00:11:41.865
intention characteristics.

209
00:11:41.865 --> 00:11:45.055
Basically, this told the team how
many customers in each segment

210
00:11:45.055 --> 00:11:47.915
the company could reasonably
plan to get under each scenario.

211
00:11:49.125 --> 00:11:51.377
In our next video,
we'll see how internal and

212
00:11:51.377 --> 00:11:54.917
external analysis were brought together,
summarized and presented.