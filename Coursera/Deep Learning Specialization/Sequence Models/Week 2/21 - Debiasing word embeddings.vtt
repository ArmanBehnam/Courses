WEBVTT

1
00:00:00.870 --> 00:00:05.535
Machine learning and AI algorithms
are increasingly trusted to help with, or

2
00:00:05.535 --> 00:00:07.590
to make, extremely important decisions.

3
00:00:07.590 --> 00:00:11.352
And so we like to make sure that as
much as possible that they're free

4
00:00:11.352 --> 00:00:16.550
of undesirable forms of bias, such as
gender bias, ethnicity bias and so on.

5
00:00:16.550 --> 00:00:20.690
What I want to do in this video is show
you some of the ideas for diminishing or

6
00:00:20.690 --> 00:00:24.415
eliminating these forms of
bias in word embeddings.

7
00:00:24.415 --> 00:00:28.630
When I use the term bias in this video,
I don't mean the bias variants.

8
00:00:28.630 --> 00:00:33.667
Sense the bias, instead I mean gender,
ethnicity, sexual orientation bias.

9
00:00:33.667 --> 00:00:38.353
That's a different sense of bias then is
typically used in the technical discussion

10
00:00:38.353 --> 00:00:40.330
on machine learning.

11
00:00:40.330 --> 00:00:43.430
But mostly the problem,
we talked about how

12
00:00:43.430 --> 00:00:47.700
word embeddings can learn analogies like
man is to woman as king is to queen.

13
00:00:47.700 --> 00:00:52.583
But what if you ask it, man is to
computer programmer as woman is to what?

14
00:00:52.583 --> 00:00:56.244
And so
the authors of this paper Tolga Bolukbasi,

15
00:00:56.244 --> 00:01:00.428
Kai-Wei Chang, James Zou,
Venkatesh Saligrama, and

16
00:01:00.428 --> 00:01:06.098
Adam Kalai found a somewhat horrifying
result where a learned word embedding

17
00:01:06.098 --> 00:01:11.008
might output Man:Computer_Programmer
as Woman:Homemaker.

18
00:01:11.008 --> 00:01:17.145
And that just seems wrong and it enforces
a very unhealthy gender stereotype.

19
00:01:17.145 --> 00:01:20.500
It'd be much more preferable to have
algorithm output man is to computer

20
00:01:20.500 --> 00:01:22.815
programmer as a woman is
to computer programmer.

21
00:01:22.815 --> 00:01:27.618
And they found also,
Father:Doctor as Mother is to what?

22
00:01:27.618 --> 00:01:32.534
And the really unfortunate result is
that some learned word embeddings would

23
00:01:32.534 --> 00:01:34.530
output Mother:Nurse.

24
00:01:34.530 --> 00:01:38.300
So word embeddings can reflect the gender,
ethnicity, age,

25
00:01:38.300 --> 00:01:42.798
sexual orientation, and other biases
of the text used to train the model.

26
00:01:42.798 --> 00:01:48.520
One that I'm especially passionate about
is bias relating to socioeconomic status.

27
00:01:48.520 --> 00:01:52.140
I think that every person,
whether you come from a wealthy family,

28
00:01:52.140 --> 00:01:54.743
or a low income family,
or anywhere in between,

29
00:01:54.743 --> 00:01:57.554
I think everyone should
have great opportunities.

30
00:01:57.554 --> 00:02:02.537
And because machine learning algorithms
are being used to make very important

31
00:02:02.537 --> 00:02:03.375
decisions.

32
00:02:03.375 --> 00:02:08.388
They're influencing everything ranging
from college admissions, to the way people

33
00:02:08.388 --> 00:02:13.464
find jobs, to loan applications, whether
your application for a loan gets approved,

34
00:02:13.464 --> 00:02:17.710
to in the criminal justice system,
even sentencing guidelines.

35
00:02:17.710 --> 00:02:22.560
Learning algorithms are making very
important decisions and so I think it's

36
00:02:22.560 --> 00:02:28.130
important that we try to change learning
algorithms to diminish as much as

37
00:02:28.130 --> 00:02:33.940
is possible, or, ideally, eliminate
these types of undesirable biases.

38
00:02:33.940 --> 00:02:38.770
Now in the case of word embeddings, they
can pick up the biases of the text used

39
00:02:38.770 --> 00:02:42.410
to train the model and so
the biases they pick up or

40
00:02:42.410 --> 00:02:48.460
tend to reflect the biases in
text as is written by people.

41
00:02:48.460 --> 00:02:50.450
Over many decades, over many centuries,

42
00:02:50.450 --> 00:02:55.085
I think humanity has made progress
in reducing these types of bias.

43
00:02:55.085 --> 00:02:59.355
And I think maybe fortunately for AI,
I think we actually have better ideas for

44
00:02:59.355 --> 00:03:02.085
quickly reducing the bias in AI than for

45
00:03:02.085 --> 00:03:05.305
quickly reducing the bias
in the human race.

46
00:03:05.305 --> 00:03:09.690
Although I think we're by no
means done for AI as well and

47
00:03:09.690 --> 00:03:12.080
there's still a lot of research and

48
00:03:12.080 --> 00:03:16.110
hard work to be done to reduce these types
of biases in our learning algorithms.

49
00:03:16.110 --> 00:03:20.621
But what I want to do in this video is
share with you one example of a set

50
00:03:20.621 --> 00:03:24.973
of ideas due to the paper referenced
at the bottom by Bolukbasi and

51
00:03:24.973 --> 00:03:28.241
others on reducing the bias
in word embeddings.

52
00:03:30.356 --> 00:03:32.210
So here's the idea.

53
00:03:32.210 --> 00:03:38.063
Let's say that we've already
learned a word embedding,

54
00:03:38.063 --> 00:03:43.803
so the word babysitter is here,
the word doctor is here.

55
00:03:43.803 --> 00:03:48.403
We have grandmother here,

56
00:03:48.403 --> 00:03:52.403
and grandfather here.

57
00:03:52.403 --> 00:03:56.860
Maybe the word girl is embedded there,
the word boy is embedded there.

58
00:03:56.860 --> 00:04:01.830
And maybe she is embedded here,
and he is embedded there.

59
00:04:01.830 --> 00:04:07.060
So the first thing we're going to
do it is identify the direction

60
00:04:07.060 --> 00:04:11.750
corresponding to a particular bias
we want to reduce or eliminate.

61
00:04:11.750 --> 00:04:15.760
And, for illustration,
I'm going to focus on gender bias but

62
00:04:15.760 --> 00:04:18.450
these ideas are applicable
to all of the other

63
00:04:18.450 --> 00:04:21.050
types of bias that I mention
on the previous slide as well.

64
00:04:26.450 --> 00:04:30.840
And so how do you identify the direction
corresponding to the bias?

65
00:04:30.840 --> 00:04:36.010
For the case of gender, what we can do
is take the embedding vector for he and

66
00:04:36.010 --> 00:04:41.580
subtract the embedding vector for
she, because that differs by gender.

67
00:04:41.580 --> 00:04:46.230
And take e male, subtract e female, and

68
00:04:46.230 --> 00:04:48.720
take a few of these and
average them, right?

69
00:04:48.720 --> 00:04:51.920
And take a few of these differences and
basically average them.

70
00:04:51.920 --> 00:04:56.490
And this will allow you to figure out
in this case that what looks like

71
00:04:56.490 --> 00:05:01.170
this direction is the gender direction,
or the bias direction.

72
00:05:02.640 --> 00:05:10.650
Whereas this direction is unrelated to the
particular bias we're trying to address.

73
00:05:10.650 --> 00:05:16.580
So this is the non-bias direction.

74
00:05:16.580 --> 00:05:22.013
And in this case, the bias direction,
think of this as a 1D subspace whereas

75
00:05:22.013 --> 00:05:27.047
a non-bias direction,
this will be 299-dimensional subspace.

76
00:05:27.047 --> 00:05:31.520
Okay, and I've simplified the description
a little bit in the original paper.

77
00:05:31.520 --> 00:05:34.504
The bias direction can be
higher than 1-dimensional, and

78
00:05:34.504 --> 00:05:38.426
rather than take an average, as I'm
describing it here, it's actually found

79
00:05:38.426 --> 00:05:42.597
using a more complicated algorithm called
a SVU, a singular value decomposition.

80
00:05:42.597 --> 00:05:44.566
Which is closely related to,

81
00:05:44.566 --> 00:05:48.429
if you're familiar with
principle component analysis,

82
00:05:48.429 --> 00:05:53.729
it uses ideas similar to the pc or the
principle component analysis algorithm.

83
00:05:53.729 --> 00:05:57.320
After that,
the next step is a neutralization step.

84
00:05:57.320 --> 00:06:02.710
So for every word that's not definitional,
project it to get rid of bias.

85
00:06:02.710 --> 00:06:07.350
So there are some words that
intrinsically capture gender.

86
00:06:07.350 --> 00:06:08.400
So words like grandmother,

87
00:06:08.400 --> 00:06:13.680
grandfather, girl, boy, she, he,
a gender is intrinsic in the definition.

88
00:06:13.680 --> 00:06:16.431
Whereas there are other
word like doctor and

89
00:06:16.431 --> 00:06:19.341
babysitter that we want
to be gender neutral.

90
00:06:19.341 --> 00:06:24.008
And really, in the more general case,
you might want words like doctor or

91
00:06:24.008 --> 00:06:28.601
babysitter to be ethnicity neutral or
sexual orientation neutral, and

92
00:06:28.601 --> 00:06:32.989
so on, but we'll just use gender
as the illustrating example here.

93
00:06:32.989 --> 00:06:35.733
But so for
every word that is not definitional,

94
00:06:35.733 --> 00:06:39.646
this basically means not words
like grandmother and grandfather,

95
00:06:39.646 --> 00:06:44.450
which really have a very legitimate
gender component, because, by definition,

96
00:06:44.450 --> 00:06:48.190
grandmothers are female,
and grandfathers are male.

97
00:06:48.190 --> 00:06:51.253
So for words like doctor and babysitter,

98
00:06:51.253 --> 00:06:56.330
let's just project them onto this
axis to reduce their components,

99
00:06:56.330 --> 00:07:00.636
or to eliminate their component,
in the bias direction.

100
00:07:00.636 --> 00:07:04.330
So reduce their component in
this horizontal direction.

101
00:07:06.050 --> 00:07:08.311
So that's the second neutralize step.

102
00:07:08.311 --> 00:07:13.192
And then the final step is
called equalization in which

103
00:07:13.192 --> 00:07:17.861
you might have pairs of words
such as grandmother and

104
00:07:17.861 --> 00:07:22.740
grandfather, or girl and
boy, where you want the only

105
00:07:22.740 --> 00:07:27.790
difference in their
embedding to be the gender.

106
00:07:27.790 --> 00:07:30.410
And so, why do you want that?

107
00:07:30.410 --> 00:07:33.910
Well in this example, the distance, or

108
00:07:33.910 --> 00:07:38.500
the similarity, between babysitter and
grandmother is actually

109
00:07:38.500 --> 00:07:42.650
smaller than the distance between
babysitter and grandfather.

110
00:07:42.650 --> 00:07:46.130
And so
this maybe reinforces an unhealthy, or

111
00:07:46.130 --> 00:07:50.830
maybe undesirable, bias that grandmothers
end up babysitting more than grandfathers.

112
00:07:50.830 --> 00:07:53.960
So in the final equalization step,

113
00:07:53.960 --> 00:07:57.170
what we'd like to do is to make sure
that words like grandmother and

114
00:07:57.170 --> 00:08:01.900
grandfather are both exactly the same
similarity, or exactly the same distance,

115
00:08:01.900 --> 00:08:06.720
from words that should be gender neutral,
such as babysitter or such as doctor.

116
00:08:06.720 --> 00:08:09.102
So there are a few linear
algebra steps for that.

117
00:08:09.102 --> 00:08:13.616
But what it will basically
do is move grandmother and

118
00:08:13.616 --> 00:08:20.705
grandfather to a pair of points that are
equidistant from this axis in the middle.

119
00:08:20.705 --> 00:08:24.894
And so the effect of that is that
now the distance between babysitter,

120
00:08:24.894 --> 00:08:28.240
compared to these two words,
will be exactly the same.

121
00:08:29.480 --> 00:08:33.716
And so, in general,
there are many pairs of words like this

122
00:08:33.716 --> 00:08:38.724
grandmother-grandfather, boy-girl,
sorority-fraternity,

123
00:08:38.724 --> 00:08:44.326
girlhood-boyhood, sister-brother,
niece-nephew, daughter-son,

124
00:08:44.326 --> 00:08:49.760
that you might want to carry out
through this equalization step.

125
00:08:49.760 --> 00:08:54.600
So the final detail is,
how do you decide what word to neutralize?

126
00:08:54.600 --> 00:08:59.550
So for example, the word doctor seems
like a word you should neutralize

127
00:08:59.550 --> 00:09:04.270
to make it non-gender-specific or
non-ethnicity-specific.

128
00:09:04.270 --> 00:09:06.360
Whereas the words grandmother and

129
00:09:06.360 --> 00:09:09.900
grandmother should not be
made non-gender-specific.

130
00:09:09.900 --> 00:09:12.950
And there are also words like beard,
right,

131
00:09:12.950 --> 00:09:16.470
that it's just a statistical fact
that men are much more likely to have

132
00:09:16.470 --> 00:09:21.510
beards than women, so maybe beards
should be closer to male than female.

133
00:09:21.510 --> 00:09:26.463
And so what the authors did is
train a classifier to try to figure

134
00:09:26.463 --> 00:09:29.173
out what words are definitional,

135
00:09:29.173 --> 00:09:34.690
what words should be gender-specific and
what words should not be.

136
00:09:34.690 --> 00:09:39.200
And it turns out that most words in
the English language are not definitional,

137
00:09:39.200 --> 00:09:41.770
meaning that gender is not
part of the definition.

138
00:09:41.770 --> 00:09:47.391
And it's such a relatively small subset of
words like this, grandmother-grandfather,

139
00:09:47.391 --> 00:09:52.360
girl-boy, sorority-fraternity, and so
on that should not be neutralized.

140
00:09:53.370 --> 00:09:58.160
And so a linear classifier can tell
you what words to pass through

141
00:09:58.160 --> 00:10:02.774
the neutralization step to
project out this bias direction,

142
00:10:02.774 --> 00:10:08.022
to project it on to this essentially
299-dimensional subspace.

143
00:10:08.022 --> 00:10:12.118
And then, finally,
the number of pairs you want to equalize,

144
00:10:12.118 --> 00:10:17.608
that's actually also relatively small,
and is, at least for the gender example,

145
00:10:17.608 --> 00:10:22.428
it is quite feasible to hand-pick most
of the pairs you want to equalize.

146
00:10:22.428 --> 00:10:26.750
So the full algorithm is a bit more
complicated than I present it here,

147
00:10:26.750 --> 00:10:29.990
you can take a look at the paper for
the full details.

148
00:10:29.990 --> 00:10:32.740
And you also get to play
with a few of these ideas

149
00:10:32.740 --> 00:10:35.510
in the programming exercises as well.

150
00:10:35.510 --> 00:10:39.950
So to summarize, I think that reducing or
eliminating bias of our learning

151
00:10:39.950 --> 00:10:44.190
algorithms is a very important problem
because these algorithms are being asked

152
00:10:44.190 --> 00:10:48.175
to help with or to make more and
more important decisions in society.

153
00:10:48.175 --> 00:10:51.350
In this video I shared
just one set of ideas for

154
00:10:51.350 --> 00:10:53.990
how to go about trying to
address this problem, but

155
00:10:53.990 --> 00:11:00.180
this is still a very much an ongoing area
of active research by many researchers.

156
00:11:00.180 --> 00:11:03.458
So that's it for this week's videos.

157
00:11:03.458 --> 00:11:06.170
Best of luck with this week's
programming exercises and

158
00:11:06.170 --> 00:11:07.660
I look forward to seeing you next week.