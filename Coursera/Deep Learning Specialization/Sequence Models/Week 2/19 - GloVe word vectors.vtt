WEBVTT

1
00:00:00.000 --> 00:00:04.337
You learn about several algorithms for computing words embeddings.

2
00:00:04.337 --> 00:00:10.615
Another algorithm that has some momentum in the NLP community is the GloVe algorithm.

3
00:00:10.615 --> 00:00:14.520
This is not used as much as the Word2Vec or the skip-gram models,

4
00:00:14.520 --> 00:00:16.884
but it has some enthusiasts.

5
00:00:16.884 --> 00:00:19.865
Because I think, in part of its simplicity.

6
00:00:19.865 --> 00:00:21.133
Let's take a look.

7
00:00:21.133 --> 00:00:24.085
The GloVe algorithm was created by Jeffrey Pennington,

8
00:00:24.085 --> 00:00:25.720
Richard Socher, and Chris Manning.

9
00:00:25.720 --> 00:00:30.570
And GloVe stands for global vectors for word representation.

10
00:00:30.570 --> 00:00:33.630
So, previously, we were sampling pairs of words,

11
00:00:33.630 --> 00:00:35.625
context and target words,

12
00:00:35.625 --> 00:00:42.180
by picking two words that appear in close proximity to each other in our text corpus.

13
00:00:42.180 --> 00:00:43.380
So, what the GloVe algorithm does is,

14
00:00:43.380 --> 00:00:46.721
it starts off just by making that explicit.

15
00:00:46.721 --> 00:00:52.515
So, let's say X_ij be the number of times that

16
00:00:52.515 --> 00:01:01.510
a word i appears in the context of j.

17
00:01:01.510 --> 00:01:09.095
And so, here i and j play the role of t and c,

18
00:01:09.095 --> 00:01:13.970
so you can think of X_ij as being x subscript tc.

19
00:01:13.970 --> 00:01:17.420
But, you can go through your training corpus and just count up

20
00:01:17.420 --> 00:01:22.330
how many words does a word i appear in the context of a different word j.

21
00:01:22.330 --> 00:01:27.170
How many times does the word t appear in context of different words c. And

22
00:01:27.170 --> 00:01:33.100
depending on the definition of context and target words,

23
00:01:33.100 --> 00:01:37.240
you might have that X_ij equals X_ji.

24
00:01:37.240 --> 00:01:40.940
And in fact, if you're defining context and target in terms

25
00:01:40.940 --> 00:01:44.600
of whether or not they appear within plus minus 10 words of each other,

26
00:01:44.600 --> 00:01:47.225
then it would be a symmetric relationship.

27
00:01:47.225 --> 00:01:50.880
Although, if your choice of context was that,

28
00:01:50.880 --> 00:01:54.795
the context is always the word immediately before the target word,

29
00:01:54.795 --> 00:01:59.595
then X_ij and X_ji may not be symmetric like this.

30
00:01:59.595 --> 00:02:02.745
But for the purposes of the GloVe algorithm,

31
00:02:02.745 --> 00:02:05.630
we can define context and target as

32
00:02:05.630 --> 00:02:09.155
whether or not the two words appear in close proximity,

33
00:02:09.155 --> 00:02:13.755
say within plus or minus 10 words of each other.

34
00:02:13.755 --> 00:02:22.750
So, X_ij is a count that captures how often do words i and j appear with each other,

35
00:02:22.750 --> 00:02:24.650
or close to each other.

36
00:02:24.650 --> 00:02:26.480
So what the GloVe model does is,

37
00:02:26.480 --> 00:02:28.616
it optimizes the following.

38
00:02:28.616 --> 00:02:36.745
We're going to minimize the difference between theta i

39
00:02:36.745 --> 00:02:45.565
transpose e_j minus log of X_ij squared.

40
00:02:45.565 --> 00:02:48.488
I'm going to fill in some of the parts of this equation.

41
00:02:48.488 --> 00:02:54.300
But again, think of i and j as playing the role of t and c. So this is a bit

42
00:02:54.300 --> 00:03:00.428
like what you saw previously with theta t transpose e_c.

43
00:03:00.428 --> 00:03:05.125
And what you want is, for this to tell you how related are those two words?

44
00:03:05.125 --> 00:03:07.135
How related are words t and c?

45
00:03:07.135 --> 00:03:12.795
How related are words i and j as measured by how often they occur with each other?

46
00:03:12.795 --> 00:03:17.635
Which is affected by this X_ij.

47
00:03:17.635 --> 00:03:20.775
And so, what we're going to do is,

48
00:03:20.775 --> 00:03:28.320
solve for parameters theta and e using gradient descent to minimize the sum

49
00:03:28.320 --> 00:03:36.500
over i equals one to 10,000 sum over j from one to 10,000 of this difference.

50
00:03:36.500 --> 00:03:38.340
So you just want to learn vectors,

51
00:03:38.340 --> 00:03:44.565
so that their end product is a good predictor for how often the two words occur together.

52
00:03:44.565 --> 00:03:47.386
Now, just some additional details,

53
00:03:47.386 --> 00:03:49.556
if X_ij is equal to zero,

54
00:03:49.556 --> 00:03:52.505
then log of 0 is undefined, is negative infinity.

55
00:03:52.505 --> 00:03:53.740
And so, what we do is,

56
00:03:53.740 --> 00:03:59.050
we want sum over the terms where X_ij is equal to zero.

57
00:03:59.050 --> 00:04:00.910
And so, what we're going to do is,

58
00:04:00.910 --> 00:04:03.590
add an extra weighting term.

59
00:04:03.590 --> 00:04:07.895
So this is going to be a weighting term,

60
00:04:07.895 --> 00:04:17.925
and this will be equal to zero if X_ij is equal to zero.

61
00:04:17.925 --> 00:04:26.135
And we're going to use a convention that zero log zero is equal to zero.

62
00:04:26.135 --> 00:04:27.152
So what this means is,

63
00:04:27.152 --> 00:04:29.195
that if X_ij is equal to zero,

64
00:04:29.195 --> 00:04:32.433
just don't bother to sum over that X_ij pair.

65
00:04:32.433 --> 00:04:35.315
So then this log of zero term is not relevant.

66
00:04:35.315 --> 00:04:40.190
So this means the sum is sum only over the pairs of

67
00:04:40.190 --> 00:04:46.220
words that have co-occurred at least once in that context-target relationship.

68
00:04:46.220 --> 00:04:48.650
The other thing that F(X_ij) does is that,

69
00:04:48.650 --> 00:04:52.350
there are some words they just appear very often in the English language like,

70
00:04:52.350 --> 00:04:55.005
this, is, of, a, and so on.

71
00:04:55.005 --> 00:04:57.110
Sometimes we used to call them stop words but there's really

72
00:04:57.110 --> 00:04:59.810
a continuum between frequent and infrequent words.

73
00:04:59.810 --> 00:05:02.665
And then there are also some infrequent words like durion,

74
00:05:02.665 --> 00:05:04.460
which you actually still want to take into account,

75
00:05:04.460 --> 00:05:09.480
but not as frequently as the more common words.

76
00:05:09.480 --> 00:05:12.620
And so, the weighting factor can be a function

77
00:05:12.620 --> 00:05:16.976
that gives a meaningful amount of computation,

78
00:05:16.976 --> 00:05:20.090
even to the less frequent words like durion,

79
00:05:20.090 --> 00:05:25.128
and gives more weight but not an unduly large amount of weight to words like,

80
00:05:25.128 --> 00:05:28.745
this, is, of, a, which just appear lost in language.

81
00:05:28.745 --> 00:05:33.960
And so, there are various heuristics for choosing this weighting function F that

82
00:05:33.960 --> 00:05:37.110
need or gives these words too much weight

83
00:05:37.110 --> 00:05:41.020
nor gives the infrequent words too little weight.

84
00:05:41.020 --> 00:05:43.725
You can take a look at the GloVe paper,

85
00:05:43.725 --> 00:05:45.141
they are referenced in the previous slide,

86
00:05:45.141 --> 00:05:51.720
if you want the details of how F can be chosen to be a heuristic to accomplish this.

87
00:05:51.720 --> 00:05:56.070
And then, finally, one funny thing about this algorithm is

88
00:05:56.070 --> 00:06:01.215
that the roles of theta and e are now completely symmetric.

89
00:06:01.215 --> 00:06:07.595
So, theta i and e_j are symmetric in that,

90
00:06:07.595 --> 00:06:08.912
if you look at the math,

91
00:06:08.912 --> 00:06:12.403
they play pretty much the same role and you could reverse them or sort them around,

92
00:06:12.403 --> 00:06:17.293
and they actually end up with the same optimization objective.

93
00:06:17.293 --> 00:06:21.571
One way to train the algorithm is to initialize theta and e

94
00:06:21.571 --> 00:06:26.180
both uniformly around gradient descent to minimize its objective,

95
00:06:26.180 --> 00:06:30.567
and then when you're done for every word,

96
00:06:30.567 --> 00:06:31.660
to then take the average.

97
00:06:31.660 --> 00:06:33.580
For a given words w,

98
00:06:33.580 --> 00:06:36.910
you can have e final to be equal

99
00:06:36.910 --> 00:06:41.410
to the embedding that was trained through this gradient descent procedure,

100
00:06:41.410 --> 00:06:46.999
plus theta trained through this gradient descent procedure divided by two,

101
00:06:46.999 --> 00:06:50.770
because theta and e in this particular formulation play

102
00:06:50.770 --> 00:06:54.910
symmetric roles unlike the earlier models we saw in the previous videos,

103
00:06:54.910 --> 00:06:59.890
where theta and e actually play different roles and couldn't just be averaged like that.

104
00:06:59.890 --> 00:07:02.320
That's it for the GloVe algorithm.

105
00:07:02.320 --> 00:07:04.450
I think one confusing part of this algorithm is,

106
00:07:04.450 --> 00:07:07.073
if you look at this equation, it seems almost too simple.

107
00:07:07.073 --> 00:07:09.040
How could it be that just minimizing

108
00:07:09.040 --> 00:07:12.767
a square cost function like this allows you to learn meaningful word embeddings?

109
00:07:12.767 --> 00:07:15.675
But it turns out that this works.

110
00:07:15.675 --> 00:07:18.480
And the way that the inventors end up with this algorithm was,

111
00:07:18.480 --> 00:07:19.865
they were building on the history of

112
00:07:19.865 --> 00:07:23.643
much more complicated algorithms like the newer language model,

113
00:07:23.643 --> 00:07:28.340
and then later, there came the Word2Vec skip-gram model,

114
00:07:28.340 --> 00:07:30.625
and then this came later.

115
00:07:30.625 --> 00:07:34.960
And we really hope to simplify all of the earlier algorithms.

116
00:07:34.960 --> 00:07:41.200
Before concluding our discussion of algorithms concerning word embeddings,

117
00:07:41.200 --> 00:07:47.215
there's one more property of them that we should discuss briefly.

118
00:07:47.215 --> 00:07:50.935
Which is that? We started off with this featurization view

119
00:07:50.935 --> 00:07:54.633
as the motivation for learning word vectors.

120
00:07:54.633 --> 00:07:58.803
We said, "Well, maybe the first component of the embedding vector to represent gender,

121
00:07:58.803 --> 00:08:02.147
the second component to represent how royal it is,

122
00:08:02.147 --> 00:08:06.175
then the age and then whether it's a food, and so on."

123
00:08:06.175 --> 00:08:10.780
But when you learn a word embedding using one of the algorithms that we've seen,

124
00:08:10.780 --> 00:08:15.580
such as the GloVe algorithm that we just saw on the previous slide,

125
00:08:15.580 --> 00:08:18.910
what happens is, you cannot guarantee that

126
00:08:18.910 --> 00:08:22.933
the individual components of the embeddings are interpretable.

127
00:08:22.933 --> 00:08:27.190
Why is that? Well, let's say that there is some space where the first axis is

128
00:08:27.190 --> 00:08:32.188
gender and the second axis is royal.

129
00:08:32.188 --> 00:08:37.545
What you can do is guarantee that the first axis of the embedding vector is

130
00:08:37.545 --> 00:08:43.210
aligned with this axis of meaning, of gender, royal, age and food.

131
00:08:43.210 --> 00:08:46.480
And in particular,

132
00:08:46.480 --> 00:08:51.850
the learning algorithm might choose this to be axis of the first dimension.

133
00:08:51.850 --> 00:08:54.343
So, given maybe a context of words,

134
00:08:54.343 --> 00:08:58.993
so the first dimension might be this axis and the second dimension might be this.

135
00:08:58.993 --> 00:09:00.895
Or it might not even be orthogonal,

136
00:09:00.895 --> 00:09:05.420
maybe it'll be a second non-orthogonal axis,

137
00:09:05.420 --> 00:09:09.230
could be the second component of the word embeddings you actually learn.

138
00:09:09.230 --> 00:09:10.530
And when we see this,

139
00:09:10.530 --> 00:09:17.105
if you have a subsequent understanding of linear algebra is that,

140
00:09:17.105 --> 00:09:19.850
if there was some invertible matrix A,

141
00:09:19.850 --> 00:09:26.330
then this could just as easily be replaced with A times

142
00:09:26.330 --> 00:09:35.604
theta i transpose A inverse transpose e_j.

143
00:09:35.604 --> 00:09:36.790
Because we expand this out,

144
00:09:36.790 --> 00:09:44.008
this is equal to theta i transpose A transpose A inverse transpose times e_j.

145
00:09:44.008 --> 00:09:46.215
And so, the middle term cancels out and we're left

146
00:09:46.215 --> 00:09:49.530
with theta i transpose e_j, same as before.

147
00:09:49.530 --> 00:09:51.993
Don't worry if you didn't follow the linear algebra,

148
00:09:51.993 --> 00:09:56.540
but that's a brief proof that shows that with an algorithm like this,

149
00:09:56.540 --> 00:10:00.965
you can't guarantee that the axis used to represent

150
00:10:00.965 --> 00:10:06.975
the features will be well-aligned with what might be easily humanly interpretable axis.

151
00:10:06.975 --> 00:10:10.205
In particular, the first feature might be a combination of gender,

152
00:10:10.205 --> 00:10:11.565
and royal, and age, and food,

153
00:10:11.565 --> 00:10:14.575
and cost, and size,

154
00:10:14.575 --> 00:10:16.781
is it a noun or an action verb,

155
00:10:16.781 --> 00:10:17.975
and all the other features.

156
00:10:17.975 --> 00:10:21.620
It's very difficult to look at individual components,

157
00:10:21.620 --> 00:10:27.670
individual rows of the embedding matrix and assign the human interpretation to that.

158
00:10:27.670 --> 00:10:30.710
But despite this type of linear transformation,

159
00:10:30.710 --> 00:10:33.620
the parallelogram map that we worked out

160
00:10:33.620 --> 00:10:36.660
when we were describing analogies, that still works.

161
00:10:36.660 --> 00:10:42.950
And so, despite this potentially arbitrary linear transformation of the features,

162
00:10:42.950 --> 00:10:51.220
you end up learning the parallelogram map for figure analogies still works.

163
00:10:51.220 --> 00:10:54.170
So, that's it for learning word embeddings.

164
00:10:54.170 --> 00:10:57.860
You've now seen a variety of algorithms for learning these word embeddings and you get

165
00:10:57.860 --> 00:11:01.802
to play them more in this week's programming exercise as well.

166
00:11:01.802 --> 00:11:03.680
Next, I'd like to show you how you can use

167
00:11:03.680 --> 00:11:06.640
these algorithms to carry out sentiment classification.

168
00:11:06.640 --> 00:11:08.000
Let's go onto the next video.