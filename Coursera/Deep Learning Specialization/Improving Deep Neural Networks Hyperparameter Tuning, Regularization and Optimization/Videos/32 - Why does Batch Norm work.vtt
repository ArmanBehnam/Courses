WEBVTT

1
00:00:00.000 --> 00:00:02.415
So, why does batch norm work?

2
00:00:02.415 --> 00:00:06.680
Here's one reason, you've seen how normalizing the input features,

3
00:00:06.680 --> 00:00:09.380
the X's, to mean zero and variance one,

4
00:00:09.380 --> 00:00:10.740
how that can speed up learning.

5
00:00:10.740 --> 00:00:13.920
So rather than having some features that range from zero to one,

6
00:00:13.920 --> 00:00:15.735
and some from one to a 1,000,

7
00:00:15.735 --> 00:00:18.835
by normalizing all the features, input features X,

8
00:00:18.835 --> 00:00:22.975
to take on a similar range of values that can speed up learning.

9
00:00:22.975 --> 00:00:25.770
So, one intuition behind why batch norm works is,

10
00:00:25.770 --> 00:00:27.750
this is doing a similar thing,

11
00:00:27.750 --> 00:00:32.770
but further values in your hidden units and not just for your input there.

12
00:00:32.770 --> 00:00:37.380
Now, this is just a partial picture for what batch norm is doing.

13
00:00:37.380 --> 00:00:39.180
There are a couple of further intuitions,

14
00:00:39.180 --> 00:00:43.210
that will help you gain a deeper understanding of what batch norm is doing.

15
00:00:43.210 --> 00:00:46.320
Let's take a look at those in this video.

16
00:00:46.320 --> 00:00:48.490
A second reason why batch norm works,

17
00:00:48.490 --> 00:00:50.295
is it makes weights,

18
00:00:50.295 --> 00:00:52.575
later or deeper than your network,

19
00:00:52.575 --> 00:00:56.095
say the weight on layer 10, more robust to changes to

20
00:00:56.095 --> 00:01:00.300
weights in earlier layers of the neural network, say, in layer one.

21
00:01:00.300 --> 00:01:01.810
To explain what I mean,

22
00:01:01.810 --> 00:01:04.775
let's look at this most vivid example.

23
00:01:04.775 --> 00:01:06.420
Let's see a training on network,

24
00:01:06.420 --> 00:01:07.770
maybe a shallow network,

25
00:01:07.770 --> 00:01:11.715
like logistic regression or maybe a neural network,

26
00:01:11.715 --> 00:01:17.260
maybe a shallow network like this regression or maybe a deep network,

27
00:01:17.260 --> 00:01:21.120
on our famous cat detection toss.

28
00:01:21.120 --> 00:01:26.745
But let's say that you've trained your data sets on all images of black cats.

29
00:01:26.745 --> 00:01:31.240
If you now try to apply this network to

30
00:01:31.240 --> 00:01:33.325
data with colored cats where

31
00:01:33.325 --> 00:01:36.895
the positive examples are not just black cats like on the left,

32
00:01:36.895 --> 00:01:40.442
but to color cats like on the right,

33
00:01:40.442 --> 00:01:43.160
then your cosfa might not do very well.

34
00:01:43.160 --> 00:01:47.907
So in pictures, if your training set looks like this,

35
00:01:47.907 --> 00:01:52.383
where you have positive examples here and negative examples here,

36
00:01:52.383 --> 00:01:54.970
but you were to try to generalize it,

37
00:01:54.970 --> 00:02:02.335
to a data set where maybe positive examples are here and the negative examples are here,

38
00:02:02.335 --> 00:02:05.800
then you might not expect a module trained on the data

39
00:02:05.800 --> 00:02:09.430
on the left to do very well on the data on the right.

40
00:02:09.430 --> 00:02:13.901
Even though there might be the same function that actually works well,

41
00:02:13.901 --> 00:02:19.230
but you wouldn't expect your learning algorithm to discover that green decision boundary,

42
00:02:19.230 --> 00:02:21.500
just looking at the data on the left.

43
00:02:21.500 --> 00:02:26.410
So, this idea of your data distribution changing goes

44
00:02:26.410 --> 00:02:31.657
by the somewhat fancy name, covariate shift.

45
00:02:31.657 --> 00:02:33.695
And the idea is that,

46
00:02:33.695 --> 00:02:35.910
if you've learned some X to Y mapping,

47
00:02:35.910 --> 00:02:38.435
if the distribution of X changes,

48
00:02:38.435 --> 00:02:41.545
then you might need to retrain your learning algorithm.

49
00:02:41.545 --> 00:02:43.925
And this is true even if the function,

50
00:02:43.925 --> 00:02:45.230
the ground true function,

51
00:02:45.230 --> 00:02:46.775
mapping from X to Y,

52
00:02:46.775 --> 00:02:49.430
remains unchanged, which it is in this example,

53
00:02:49.430 --> 00:02:51.630
because the ground true function is,

54
00:02:51.630 --> 00:02:53.990
is this picture a cat or not.

55
00:02:53.990 --> 00:02:57.590
And the need to retain your function becomes even more

56
00:02:57.590 --> 00:03:03.510
acute or it becomes even worse if the ground true function shifts as well.

57
00:03:03.510 --> 00:03:08.720
So, how does this problem of covariate shift apply to a neural network?

58
00:03:08.720 --> 00:03:10.880
Consider a deep network like this,

59
00:03:10.880 --> 00:03:15.225
and let's look at the learning process from the perspective of this certain layer,

60
00:03:15.225 --> 00:03:16.995
the third hidden layer.

61
00:03:16.995 --> 00:03:22.145
So this network has learned the parameters W3 and B3.

62
00:03:22.145 --> 00:03:24.860
And from the perspective of the third hidden layer,

63
00:03:24.860 --> 00:03:27.665
it gets some set of values from the earlier layers,

64
00:03:27.665 --> 00:03:30.020
and then it has to do some stuff to hopefully make

65
00:03:30.020 --> 00:03:34.305
the output Y-hat close to the ground true value Y.

66
00:03:34.305 --> 00:03:38.340
So let me cover up the nose on the left for a second.

67
00:03:38.340 --> 00:03:41.785
So from the perspective of this third hidden layer,

68
00:03:41.785 --> 00:03:44.265
it gets some values,

69
00:03:44.265 --> 00:03:48.520
let's call them A_2_1, A_2_2, A_2_3, and A_2_4.

70
00:03:48.520 --> 00:03:57.102
But these values might as well be features X1, X2, X3,

71
00:03:57.102 --> 00:04:02.240
X4, and the job of the third hidden layer is to

72
00:04:02.240 --> 00:04:08.225
take these values and find a way to map them to Y-hat.

73
00:04:08.225 --> 00:04:10.760
So you can imagine doing great intercepts,

74
00:04:10.760 --> 00:04:17.525
so that these parameters W_3_B_3 as well as maybe W_4_B_4,

75
00:04:17.525 --> 00:04:21.800
and even W_5_B_5, maybe try and learn those parameters,

76
00:04:21.800 --> 00:04:23.360
so the network does a good job,

77
00:04:23.360 --> 00:04:29.460
mapping from the values I drew in black on the left to the output values Y-hat.

78
00:04:29.460 --> 00:04:33.470
But now let's uncover the left of the network again.

79
00:04:33.470 --> 00:04:42.226
The network is also adapting parameters W_2_B_2 and W_1B_1,

80
00:04:42.226 --> 00:04:45.305
and so as these parameters change,

81
00:04:45.305 --> 00:04:49.795
these values, A_2, will also change.

82
00:04:49.795 --> 00:04:53.080
So from the perspective of the third hidden layer,

83
00:04:53.080 --> 00:04:56.735
these hidden unit values are changing all the time,

84
00:04:56.735 --> 00:04:59.090
and so it's suffering from the problem of

85
00:04:59.090 --> 00:05:02.435
covariate shift that we talked about on the previous slide.

86
00:05:02.435 --> 00:05:04.115
So what batch norm does,

87
00:05:04.115 --> 00:05:10.730
is it reduces the amount that the distribution of these hidden unit values shifts around.

88
00:05:10.730 --> 00:05:14.825
And if it were to plot the distribution of these hidden unit values,

89
00:05:14.825 --> 00:05:17.948
maybe this is technically renormalizer Z,

90
00:05:17.948 --> 00:05:24.970
so this is actually Z_2_1 and Z_2_2,

91
00:05:24.970 --> 00:05:27.862
and I also plot two values instead of four values,

92
00:05:27.862 --> 00:05:29.670
so we can visualize in 2D.

93
00:05:29.670 --> 00:05:32.015
What batch norm is saying is that,

94
00:05:32.015 --> 00:05:35.745
the values for Z_2_1 Z and Z_2_2 can change,

95
00:05:35.745 --> 00:05:37.514
and indeed they will change when the neural network updates

96
00:05:37.514 --> 00:05:41.215
the parameters in the earlier layers.

97
00:05:41.215 --> 00:05:44.930
But what batch norm ensures is that no matter how it changes,

98
00:05:44.930 --> 00:05:55.050
the mean and variance of Z_2_1 and Z_2_2 will remain the same.

99
00:05:55.050 --> 00:05:59.900
So even if the exact values of Z_2_1 and Z_2_2 change,

100
00:05:59.900 --> 00:06:07.115
their mean and variance will at least stay same mean zero and variance one.

101
00:06:07.115 --> 00:06:09.940
Or, not necessarily mean zero and variance one,

102
00:06:09.940 --> 00:06:17.295
but whatever value is governed by beta two and gamma two.

103
00:06:17.295 --> 00:06:19.228
Which, if the neural networks chooses,

104
00:06:19.228 --> 00:06:22.270
can force it to be mean zero and variance one.

105
00:06:22.270 --> 00:06:24.655
Or, really, any other mean and variance.

106
00:06:24.655 --> 00:06:26.305
But what this does is,

107
00:06:26.305 --> 00:06:32.290
it limits the amount to which updating the parameters in the earlier layers can

108
00:06:32.290 --> 00:06:35.110
affect the distribution of values that

109
00:06:35.110 --> 00:06:38.790
the third layer now sees and therefore has to learn on.

110
00:06:38.790 --> 00:06:44.370
And so, batch norm reduces the problem of the input values changing,

111
00:06:44.370 --> 00:06:48.895
it really causes these values to become more stable,

112
00:06:48.895 --> 00:06:55.155
so that the later layers of the neural network has more firm ground to stand on.

113
00:06:55.155 --> 00:06:57.655
And even though the input distribution changes a bit,

114
00:06:57.655 --> 00:07:00.580
it changes less, and what this does is,

115
00:07:00.580 --> 00:07:03.325
even as the earlier layers keep learning,

116
00:07:03.325 --> 00:07:06.640
the amounts that this forces the later layers to

117
00:07:06.640 --> 00:07:10.180
adapt to as early as layer changes is reduced or,

118
00:07:10.180 --> 00:07:12.925
if you will, it weakens the coupling between

119
00:07:12.925 --> 00:07:15.445
what the early layers parameters has to do

120
00:07:15.445 --> 00:07:18.020
and what the later layers parameters have to do.

121
00:07:18.020 --> 00:07:22.147
And so it allows each layer of the network to learn by itself,

122
00:07:22.147 --> 00:07:25.210
a little bit more independently of other layers,

123
00:07:25.210 --> 00:07:29.145
and this has the effect of speeding up of learning in the whole network.

124
00:07:29.145 --> 00:07:32.161
So I hope this gives some better intuition,

125
00:07:32.161 --> 00:07:35.620
but the takeaway is that batch norm means that,

126
00:07:35.620 --> 00:07:39.010
especially from the perspective of one of the later layers of the neural network,

127
00:07:39.010 --> 00:07:43.090
the earlier layers don't get to shift around as much,

128
00:07:43.090 --> 00:07:46.320
because they're constrained to have the same mean and variance.

129
00:07:46.320 --> 00:07:50.260
And so this makes the job of learning on the later layers easier.

130
00:07:50.260 --> 00:07:52.669
It turns out batch norm has a second effect,

131
00:07:52.669 --> 00:07:55.940
it has a slight regularization effect.

132
00:07:55.940 --> 00:07:59.885
So one non-intuitive thing of a batch norm is that each mini-batch,

133
00:07:59.885 --> 00:08:02.090
I will say mini-batch X_t,

134
00:08:02.090 --> 00:08:04.660
has the values Z_t,

135
00:08:04.660 --> 00:08:07.225
has the values Z_l,

136
00:08:07.225 --> 00:08:12.730
scaled by the mean and variance computed on just that one mini-batch.

137
00:08:12.730 --> 00:08:15.895
Now, because the mean and variance computed on

138
00:08:15.895 --> 00:08:20.245
just that mini-batch as opposed to computed on the entire data set,

139
00:08:20.245 --> 00:08:22.960
that mean and variance has a little bit of noise in it,

140
00:08:22.960 --> 00:08:25.540
because it's computed just on your mini-batch of,

141
00:08:25.540 --> 00:08:28.145
say, 64, or 128,

142
00:08:28.145 --> 00:08:32.335
or maybe 256 or larger training examples.

143
00:08:32.335 --> 00:08:35.935
So because the mean and variance is a little bit noisy because it's estimated with

144
00:08:35.935 --> 00:08:40.195
just a relatively small sample of data, the scaling process,

145
00:08:40.195 --> 00:08:43.363
going from Z_l to Z_2_l,

146
00:08:43.363 --> 00:08:46.135
that process is a little bit noisy as well,

147
00:08:46.135 --> 00:08:51.420
because it's computed, using a slightly noisy mean and variance.

148
00:08:51.420 --> 00:08:54.817
So similar to dropout,

149
00:08:54.817 --> 00:08:57.980
it adds some noise to each hidden layer's activations.

150
00:08:57.980 --> 00:08:59.905
The way dropout has noises,

151
00:08:59.905 --> 00:09:04.180
it takes a hidden unit and it multiplies it by zero with some probability.

152
00:09:04.180 --> 00:09:06.870
And multiplies it by one with some probability.

153
00:09:06.870 --> 00:09:12.350
And so your dropout has multiple of noise because it's multiplied by zero or one,

154
00:09:12.350 --> 00:09:18.360
whereas batch norm has multiples of noise because of scaling by the standard deviation,

155
00:09:18.360 --> 00:09:21.655
as well as additive noise because it's subtracting the mean.

156
00:09:21.655 --> 00:09:25.825
Well, here the estimates of the mean and the standard deviation are noisy.

157
00:09:25.825 --> 00:09:29.785
And so, similar to dropout,

158
00:09:29.785 --> 00:09:33.220
batch norm therefore has a slight regularization effect.

159
00:09:33.220 --> 00:09:35.435
Because by adding noise to the hidden units,

160
00:09:35.435 --> 00:09:41.280
it's forcing the downstream hidden units not to rely too much on any one hidden unit.

161
00:09:41.280 --> 00:09:43.025
And so similar to dropout,

162
00:09:43.025 --> 00:09:47.620
it adds noise to the hidden layers and therefore has a very slight regularization effect.

163
00:09:47.620 --> 00:09:50.064
Because the noise added is quite small,

164
00:09:50.064 --> 00:09:52.572
this is not a huge regularization effect,

165
00:09:52.572 --> 00:09:56.760
and you might choose to use batch norm together with dropout,

166
00:09:56.760 --> 00:09:59.880
and you might use batch norm together with dropouts if

167
00:09:59.880 --> 00:10:03.060
you want the more powerful regularization effect of dropout.

168
00:10:03.060 --> 00:10:06.195
And maybe one other slightly non-intuitive effect is that,

169
00:10:06.195 --> 00:10:08.454
if you use a bigger mini-batch size,

170
00:10:08.454 --> 00:10:11.200
right, so if you use use a mini-batch size of, say,

171
00:10:11.200 --> 00:10:13.725
512 instead of 64,

172
00:10:13.725 --> 00:10:15.934
by using a larger mini-batch size,

173
00:10:15.934 --> 00:10:20.940
you're reducing this noise and therefore also reducing this regularization effect.

174
00:10:20.940 --> 00:10:24.030
So that's one strange property of dropout

175
00:10:24.030 --> 00:10:27.435
which is that by using a bigger mini-batch size,

176
00:10:27.435 --> 00:10:29.870
you reduce the regularization effect.

177
00:10:29.870 --> 00:10:33.833
Having said this, I wouldn't really use batch norm as a regularizer,

178
00:10:33.833 --> 00:10:36.625
that's really not the intent of batch norm,

179
00:10:36.625 --> 00:10:44.250
but sometimes it has this extra intended or unintended effect on your learning algorithm.

180
00:10:44.250 --> 00:10:48.390
But, really, don't turn to batch norm as a regularization.

181
00:10:48.390 --> 00:10:50.070
Use it as a way to normalize

182
00:10:50.070 --> 00:10:53.770
your hidden units activations and therefore speed up learning.

183
00:10:53.770 --> 00:10:57.900
And I think the regularization is an almost unintended side effect.

184
00:10:57.900 --> 00:11:02.430
So I hope that gives you better intuition about what batch norm is doing.

185
00:11:02.430 --> 00:11:04.580
Before we wrap up the discussion on batch norm,

186
00:11:04.580 --> 00:11:06.855
there's one more detail I want to make sure you know,

187
00:11:06.855 --> 00:11:11.254
which is that batch norm handles data one mini-batch at a time.

188
00:11:11.254 --> 00:11:14.520
It computes mean and variances on mini-batches.

189
00:11:14.520 --> 00:11:15.720
So at test time,

190
00:11:15.720 --> 00:11:18.150
you try and make predictors, try and evaluate the neural network,

191
00:11:18.150 --> 00:11:20.400
you might not have a mini-batch of examples,

192
00:11:20.400 --> 00:11:24.035
you might be processing one single example at the time.

193
00:11:24.035 --> 00:11:26.400
So, at test time you need to do something

194
00:11:26.400 --> 00:11:29.430
slightly differently to make sure your predictions make sense.

195
00:11:29.430 --> 00:11:32.197
Like in the next and final video on batch norm,

196
00:11:32.197 --> 00:11:35.490
let's talk over the details of what you need to do in order to take

197
00:11:35.490 --> 00:11:39.290
your neural network trained using batch norm to make predictions.