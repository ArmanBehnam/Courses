WEBVTT

1
00:00:00.000 --> 00:00:02.340
During the history of deep learning,

2
00:00:02.340 --> 00:00:05.700
many researchers including some very well-known researchers,

3
00:00:05.700 --> 00:00:07.790
sometimes proposed optimization algorithms

4
00:00:07.790 --> 00:00:09.825
and showed that they worked well in a few problems.

5
00:00:09.825 --> 00:00:13.440
But those optimization algorithms subsequently were shown not to really

6
00:00:13.440 --> 00:00:18.130
generalize that well to the wide range of neural networks you might want to train.

7
00:00:18.130 --> 00:00:21.360
So over time, I think the deep learning community actually developed

8
00:00:21.360 --> 00:00:25.597
some amount of skepticism about new optimization algorithms.

9
00:00:25.597 --> 00:00:29.350
And a lot of people felt that gradient descent with momentum really works well,

10
00:00:29.350 --> 00:00:32.720
was difficult to propose things that work much better.

11
00:00:32.720 --> 00:00:36.070
So, rms prop and the Adam optimization algorithm,

12
00:00:36.070 --> 00:00:37.730
which we'll talk about in this video,

13
00:00:37.730 --> 00:00:41.460
is one of those rare algorithms that has really stood up,

14
00:00:41.460 --> 00:00:47.250
and has been shown to work well across a wide range of deep learning architectures So,

15
00:00:47.250 --> 00:00:50.150
this is one of the algorithms that I wouldn't hesitate to recommend you try

16
00:00:50.150 --> 00:00:54.625
because many people have tried it and seen it work well on many problems.

17
00:00:54.625 --> 00:00:57.720
And the Adam optimization algorithm is basically taking

18
00:00:57.720 --> 00:01:01.250
momentum and rms prop and putting them together.

19
00:01:01.250 --> 00:01:03.105
So, let's see how that works.

20
00:01:03.105 --> 00:01:05.695
To implement Adam you would initialize:

21
00:01:05.695 --> 00:01:15.877
Vdw=0, Sdw=0, and similarly Vdb, Sdb=0.

22
00:01:15.877 --> 00:01:19.810
And then on iteration T,

23
00:01:19.810 --> 00:01:30.170
you would compute the derivatives: compute dw, db using current mini-batch.

24
00:01:30.170 --> 00:01:33.775
So usually, you do this with mini-batch gradient descent.

25
00:01:33.775 --> 00:01:41.480
And then you do the momentum exponentially weighted average. So Vdw = ß.

26
00:01:41.480 --> 00:01:46.410
But now I'm going to this ß1 to distinguish it from the hyper parameter

27
00:01:46.410 --> 00:01:52.660
ß2 we'll use for the rms prop proportion of this.

28
00:01:52.660 --> 00:01:58.180
So, this is exactly what we had when we're implementing

29
00:01:58.180 --> 00:02:03.788
momentum except it now called hyper parameter ß1 instead of ß.

30
00:02:03.788 --> 00:02:14.312
And similarly, you have VDB as follows: 1 - ß1 x db.

31
00:02:14.312 --> 00:02:18.685
And then you do the rms prop update as well.

32
00:02:18.685 --> 00:02:26.630
So now, you have a different hyperparemeter ß2 plus one minus ß2 dw².

33
00:02:26.630 --> 00:02:33.325
And again, the squaring there is element y squaring of your derivatives dw.

34
00:02:33.325 --> 00:02:44.005
And then sdb is equal to this plus one minus ß2 times db.

35
00:02:44.005 --> 00:02:49.145
So this is the momentum like update with hyper parameter

36
00:02:49.145 --> 00:02:55.318
ß1 and this is the rms prop like update with hyper parameter ß2.

37
00:02:55.318 --> 00:02:58.599
In the typical implementation of Adam,

38
00:02:58.599 --> 00:03:01.255
you do implement bias correction.

39
00:03:01.255 --> 00:03:04.215
So you're going to have v corrected.

40
00:03:04.215 --> 00:03:06.705
Corrected means after bias correction.

41
00:03:06.705 --> 00:03:16.244
Dw = vdw divided by 1 minus ß1 to the power of T if you've done T iterations.

42
00:03:16.244 --> 00:03:25.040
And similarly, vdb corrected equals vdb divided by 1 minus ß1 to the power of T.

43
00:03:25.040 --> 00:03:30.756
And then similarly, you implement this bias correction on S as well.

44
00:03:30.756 --> 00:03:37.405
So, that's sdw divided by one minus ß2 to the T and sdb

45
00:03:37.405 --> 00:03:48.700
corrected equals sdb divided by 1 minus ß2 to the T.

46
00:03:48.700 --> 00:03:50.660
Finally, you perform the update.

47
00:03:50.660 --> 00:03:55.060
So W gets updated as W minus alpha times.

48
00:03:55.060 --> 00:03:59.870
So if you're just implementing momentum you'd use vdw,

49
00:03:59.870 --> 00:04:03.408
vw or maybe vdw corrected.

50
00:04:03.408 --> 00:04:06.615
But now, we add in the rms prop portion of this.

51
00:04:06.615 --> 00:04:13.390
So we're also going to divide by square roots of sdw corrected plus epsilon.

52
00:04:13.390 --> 00:04:18.232
And similarly, B gets updated as a similar formula,

53
00:04:18.232 --> 00:04:24.070
vdb corrected, divided by square root S,

54
00:04:24.070 --> 00:04:28.595
corrected, db, plus epsilon.

55
00:04:28.595 --> 00:04:33.070
And so, this algorithm combines the effect of gradient descent

56
00:04:33.070 --> 00:04:37.572
with momentum together with gradient descent with rms prop.

57
00:04:37.572 --> 00:04:41.740
And this is a commonly used learning algorithm that is proven to be very

58
00:04:41.740 --> 00:04:46.640
effective for many different neural networks of a very wide variety of architectures.

59
00:04:46.640 --> 00:04:49.805
So, this algorithm has a number of hyper parameters.

60
00:04:49.805 --> 00:04:57.330
The learning with hyper parameter alpha is still important and usually needs to be tuned.

61
00:04:57.330 --> 00:05:01.675
So you just have to try a range of values and see what works.

62
00:05:01.675 --> 00:05:06.090
A common choice really the default choice for ß1 is 0.9.

63
00:05:06.090 --> 00:05:08.065
So this is a moving average,

64
00:05:08.065 --> 00:05:12.220
weighted average of dw right this is the momentum light term.

65
00:05:12.220 --> 00:05:15.455
The hyper parameter for ß2,

66
00:05:15.455 --> 00:05:16.950
the authors of the Adam paper,

67
00:05:16.950 --> 00:05:20.014
inventors of the Adam algorithm recommend 0.999.

68
00:05:20.014 --> 00:05:26.485
Again this is computing the moving weighted average of dw2 as well as db squares.

69
00:05:26.485 --> 00:05:31.030
And then Epsilon, the choice of epsilon doesn't matter very much.

70
00:05:31.030 --> 00:05:34.755
But the authors of the Adam paper recommended it 10 to the minus 8.

71
00:05:34.755 --> 00:05:38.230
But this parameter you really don't

72
00:05:38.230 --> 00:05:42.555
need to set it and it doesn't affect performance much at all.

73
00:05:42.555 --> 00:05:44.280
But when implementing Adam,

74
00:05:44.280 --> 00:05:47.030
what people usually do is just use the default value.

75
00:05:47.030 --> 00:05:49.960
So, ß1 and ß2 as well as epsilon.

76
00:05:49.960 --> 00:05:52.300
I don't think anyone ever really tunes Epsilon.

77
00:05:52.300 --> 00:05:56.335
And then, try a range of values of Alpha to see what works best.

78
00:05:56.335 --> 00:05:59.140
You could also tune ß1 and ß2 but it's not

79
00:05:59.140 --> 00:06:02.440
done that often among the practitioners I know.

80
00:06:02.440 --> 00:06:06.100
So, where does the term 'Adam' come from?

81
00:06:06.100 --> 00:06:15.267
Adam stands for Adaptive Moment Estimation.

82
00:06:15.267 --> 00:06:18.175
So ß1 is computing the mean of the derivatives.

83
00:06:18.175 --> 00:06:19.780
This is called the first moment.

84
00:06:19.780 --> 00:06:21.975
And ß2 is used to compute

85
00:06:21.975 --> 00:06:25.830
exponentially weighted average of the ²s and that's called the second moment.

86
00:06:25.830 --> 00:06:29.380
So that gives rise to the name adaptive moment estimation.

87
00:06:29.380 --> 00:06:32.875
But everyone just calls it the Adam authorization algorithm.

88
00:06:32.875 --> 00:06:37.800
And, by the way, one of my long term friends and collaborators is call Adam Coates.

89
00:06:37.800 --> 00:06:40.425
As far as I know, this algorithm doesn't have anything to do with him,

90
00:06:40.425 --> 00:06:43.525
except for the fact that I think he uses it sometimes.

91
00:06:43.525 --> 00:06:45.847
But sometimes I get asked that question,

92
00:06:45.847 --> 00:06:47.945
so just in case you're wondering.

93
00:06:47.945 --> 00:06:51.187
So, that's it for the Adam optimization algorithm.

94
00:06:51.187 --> 00:06:54.435
With it, I think you will be able to train your neural networks much more quickly.

95
00:06:54.435 --> 00:06:56.055
But before we wrap up for this week,

96
00:06:56.055 --> 00:06:58.950
let's keep talking about hyper parameter tuning,

97
00:06:58.950 --> 00:07:01.465
as well as gain some more intuitions about what

98
00:07:01.465 --> 00:07:04.230
the optimization problem for neural networks looks like.

99
00:07:04.230 --> 00:07:07.260
In the next video, we'll talk about learning rate decay.