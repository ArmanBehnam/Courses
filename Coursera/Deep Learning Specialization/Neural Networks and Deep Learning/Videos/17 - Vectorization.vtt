WEBVTT

1
00:00:00.000 --> 00:00:03.195
>> Welcome back. Vectorization is basically

2
00:00:03.195 --> 00:00:07.315
the art of getting rid of explicit folders in your code.

3
00:00:07.315 --> 00:00:11.835
In the deep learning era safety in deep learning in practice,

4
00:00:11.835 --> 00:00:15.210
you often find yourself training on relatively large data sets,

5
00:00:15.210 --> 00:00:18.475
because that's when deep learning algorithms tend to shine.

6
00:00:18.475 --> 00:00:22.790
And so, it's important that your code very quickly because otherwise,

7
00:00:22.790 --> 00:00:24.525
if it's running on a big data set,

8
00:00:24.525 --> 00:00:27.000
your code might take a long time to run then you just find

9
00:00:27.000 --> 00:00:30.255
yourself waiting a very long time to get the result.

10
00:00:30.255 --> 00:00:32.035
So in the deep learning era,

11
00:00:32.035 --> 00:00:37.490
I think the ability to perform vectorization has become a key skill.

12
00:00:37.490 --> 00:00:40.010
Let's start with an example.

13
00:00:40.010 --> 00:00:42.225
So, what is Vectorization?

14
00:00:42.225 --> 00:00:48.780
In logistic regression you need to compute Z equals W transpose X plus B,

15
00:00:48.780 --> 00:00:55.405
where W was this column vector and X is also this vector.

16
00:00:55.405 --> 00:00:58.000
Maybe there are very large vectors if you have a lot of features.

17
00:00:58.000 --> 00:01:07.080
So, W and X were both these R and no R, NX dimensional vectors.

18
00:01:07.080 --> 00:01:10.170
So, to compute W transpose X,

19
00:01:10.170 --> 00:01:15.660
if you had a non-vectorized implementation,

20
00:01:15.660 --> 00:01:18.725
you would do something like Z equals zero.

21
00:01:18.725 --> 00:01:24.860
And then for I in range of X.

22
00:01:24.860 --> 00:01:27.330
So, for I equals 1, 2 NX,

23
00:01:27.330 --> 00:01:34.040
Z plus equals W I times XI.

24
00:01:34.040 --> 00:01:37.100
And then maybe you do Z plus equal B at the end.

25
00:01:37.100 --> 00:01:39.855
So, that's a non-vectorized implementation.

26
00:01:39.855 --> 00:01:43.090
Then you find that that's going to be really slow.

27
00:01:43.090 --> 00:01:48.560
In contrast, a vectorized implementation would just compute W transpose X directly.

28
00:01:48.560 --> 00:01:52.085
In Python or a numpy,

29
00:01:52.085 --> 00:02:01.428
the command you use for that is Z equals np.W,

30
00:02:01.428 --> 00:02:06.270
X, so this computes W transpose X.

31
00:02:06.270 --> 00:02:09.075
And you can also just add B to that directly.

32
00:02:09.075 --> 00:02:12.400
And you find that this is much faster.

33
00:02:12.400 --> 00:02:17.075
Let's actually illustrate this with a little demo.

34
00:02:17.075 --> 00:02:21.960
So, here's my Jupiter notebook in which I'm going to write some Python code.

35
00:02:21.960 --> 00:02:28.041
So, first, let me import the numpy library to import.

36
00:02:28.041 --> 00:02:30.000
Send P. And so, for example,

37
00:02:30.000 --> 00:02:36.570
I can create A as an array as follows.

38
00:02:36.570 --> 00:02:39.560
Let's say print A.

39
00:02:39.560 --> 00:02:41.160
Now, having written this chunk of code,

40
00:02:41.160 --> 00:02:43.170
if I hit shift enter,

41
00:02:43.170 --> 00:02:44.847
then it executes the code.

42
00:02:44.847 --> 00:02:47.970
So, it created the array A and it prints it out.

43
00:02:47.970 --> 00:02:50.580
Now, let's do the Vectorization demo.

44
00:02:50.580 --> 00:02:51.990
I'm going to import the time libraries,

45
00:02:51.990 --> 00:02:53.580
since we use that,

46
00:02:53.580 --> 00:02:56.565
in order to time how long different operations take.

47
00:02:56.565 --> 00:02:59.139
Can they create an array A?

48
00:02:59.139 --> 00:03:02.905
Those random thought round.

49
00:03:02.905 --> 00:03:10.065
This creates a million dimensional array with random values.

50
00:03:10.065 --> 00:03:13.300
b = np.random.rand.

51
00:03:13.300 --> 00:03:16.120
Another million dimensional array.

52
00:03:16.120 --> 00:03:20.810
And, now, tic=time.time, so this measure the current time,

53
00:03:20.810 --> 00:03:26.395
c = np.dot (a, b).

54
00:03:26.395 --> 00:03:28.649
toc = time.time.

55
00:03:28.649 --> 00:03:31.950
And this print,

56
00:03:31.950 --> 00:03:34.857
it is the vectorized version.

57
00:03:34.857 --> 00:03:37.685
It's a vectorize version.

58
00:03:37.685 --> 00:03:41.985
And so, let's print out.

59
00:03:41.985 --> 00:03:45.060
Let's see the last time,

60
00:03:45.060 --> 00:03:48.330
so there's toc - tic x 1000,

61
00:03:48.330 --> 00:03:52.075
so that we can express this in milliseconds.

62
00:03:52.075 --> 00:03:54.075
So, ms is milliseconds.

63
00:03:54.075 --> 00:03:56.435
I'm going to hit Shift Enter.

64
00:03:56.435 --> 00:04:01.890
So, that code took about three milliseconds or this time 1.5,

65
00:04:01.890 --> 00:04:06.170
maybe about 1.5 or 3.5 milliseconds at a time.

66
00:04:06.170 --> 00:04:08.370
It varies a little bit as I run it,

67
00:04:08.370 --> 00:04:12.085
but looks like maybe on average it's taking like 1.5 milliseconds,

68
00:04:12.085 --> 00:04:15.665
maybe two milliseconds as I run this.

69
00:04:15.665 --> 00:04:16.967
All right.

70
00:04:16.967 --> 00:04:19.005
Let's keep adding to this block of code.

71
00:04:19.005 --> 00:04:22.270
That's not implementing non-vectorize version.

72
00:04:22.270 --> 00:04:24.151
Let's see, c = 0,

73
00:04:24.151 --> 00:04:27.750
then tic = time.time.

74
00:04:27.750 --> 00:04:29.335
Now, let's implement a folder.

75
00:04:29.335 --> 00:04:35.348
For I in range of 1 million,

76
00:04:35.348 --> 00:04:38.676
I'll pick out the number of zeros right.

77
00:04:38.676 --> 00:04:43.936
C += (a,i) x (b,

78
00:04:43.936 --> 00:04:50.775
i), and then toc = time.time.

79
00:04:50.775 --> 00:04:57.725
Finally, print more than explicit full loop.

80
00:04:57.725 --> 00:05:15.225
The time it takes is this 1000 x toc - tic + "ms"

81
00:05:15.225 --> 00:05:17.505
to know that we're doing this in milliseconds.

82
00:05:17.505 --> 00:05:19.735
Let's do one more thing.

83
00:05:19.735 --> 00:05:22.802
Let's just print out the value of C we

84
00:05:22.802 --> 00:05:27.960
compute it to make sure that it's the same value in both cases.

85
00:05:27.960 --> 00:05:35.770
I'm going to hit shift enter to run this and check that out.

86
00:05:35.770 --> 00:05:38.305
In both cases, the vectorize version

87
00:05:38.305 --> 00:05:41.125
and the non-vectorize version computed the same values,

88
00:05:41.125 --> 00:05:45.355
as you know, 2.50 to 6.99, so on.

89
00:05:45.355 --> 00:05:48.670
The vectorize version took 1.5 milliseconds.

90
00:05:48.670 --> 00:05:57.555
The explicit for loop and non-vectorize version took about 400, almost 500 milliseconds.

91
00:05:57.555 --> 00:06:01.285
The non-vectorize version took something like 300

92
00:06:01.285 --> 00:06:05.660
times longer than the vectorize version.

93
00:06:05.660 --> 00:06:11.230
With this example you see that if only you remember to vectorize your code,

94
00:06:11.230 --> 00:06:15.120
your code actually runs over 300 times faster.

95
00:06:15.120 --> 00:06:16.540
Let's just run it again.

96
00:06:16.540 --> 00:06:18.930
Just run it again.

97
00:06:18.930 --> 00:06:22.235
Yeah. Vectorize version 1.5 milliseconds seconds and the four loop.

98
00:06:22.235 --> 00:06:25.960
So 481 milliseconds, again,

99
00:06:25.960 --> 00:06:29.535
about 300 times slower to do the explicit four loop.

100
00:06:29.535 --> 00:06:30.980
If the engine x slows down,

101
00:06:30.980 --> 00:06:33.880
it's the difference between your code taking maybe one minute to

102
00:06:33.880 --> 00:06:37.615
run versus taking say five hours to run.

103
00:06:37.615 --> 00:06:41.410
And when you are implementing deep learning algorithms,

104
00:06:41.410 --> 00:06:43.300
you can really get a result back faster.

105
00:06:43.300 --> 00:06:46.590
It will be much faster if you vectorize your code.

106
00:06:46.590 --> 00:06:49.300
Some of you might have heard that a lot of

107
00:06:49.300 --> 00:06:54.260
scaleable deep learning implementations are done on a GPU or a graphics processing unit.

108
00:06:54.260 --> 00:06:59.515
But all the demos I did just now in the Jupiter notebook where actually on the CPU.

109
00:06:59.515 --> 00:07:04.530
And it turns out that both GPU and CPU have parallelization instructions.

110
00:07:04.530 --> 00:07:07.530
They're sometimes called SIMD instructions.

111
00:07:07.530 --> 00:07:11.190
This stands for a single instruction multiple data.

112
00:07:11.190 --> 00:07:13.045
But what this basically means is that,

113
00:07:13.045 --> 00:07:16.835
if you use built-in functions such as this

114
00:07:16.835 --> 00:07:23.495
np.function or other functions that don't require you explicitly implementing a for loop.

115
00:07:23.495 --> 00:07:28.150
It enables Phyton Pi to take

116
00:07:28.150 --> 00:07:33.640
much better advantage of parallelism to do your computations much faster.

117
00:07:33.640 --> 00:07:38.610
And this is true both computations on CPUs and computations on GPUs.

118
00:07:38.610 --> 00:07:41.070
It's just that GPUs are remarkably good at

119
00:07:41.070 --> 00:07:44.980
these SIMD calculations but CPU is actually also not too bad at that.

120
00:07:44.980 --> 00:07:47.510
Maybe just not as good as GPUs.

121
00:07:47.510 --> 00:07:51.660
You're seeing how vectorization can significantly speed up your code.

122
00:07:51.660 --> 00:07:54.685
The rule of thumb to remember is whenever possible,

123
00:07:54.685 --> 00:07:57.425
avoid using explicit four loops.

124
00:07:57.425 --> 00:07:59.980
Let's go onto the next video to see some more examples of

125
00:07:59.980 --> 00:08:04.000
vectorization and also start to vectorize logistic regression.