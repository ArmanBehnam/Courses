WEBVTT

1
00:00:00.060 --> 00:00:02.669
being effective in developing your deep

2
00:00:02.669 --> 00:00:04.380
neural Nets requires that you not only

3
00:00:04.380 --> 00:00:06.870
organize your parameters well but also

4
00:00:06.870 --> 00:00:09.269
your hyper parameters so what are hyper

5
00:00:09.269 --> 00:00:11.759
parameters let's take a look so the

6
00:00:11.759 --> 00:00:15.170
parameters your model are W and B and

7
00:00:15.170 --> 00:00:17.820
there are other things you need to tell

8
00:00:17.820 --> 00:00:21.720
your learning algorithm such as the

9
00:00:21.720 --> 00:00:26.220
learning rate alpha because on we need

10
00:00:26.220 --> 00:00:28.920
to set alpha and that in turn will

11
00:00:28.920 --> 00:00:32.329
determine how your parameters evolve or

12
00:00:32.329 --> 00:00:34.890
maybe the number of iterations of

13
00:00:34.890 --> 00:00:38.190
gradient descent you carry out your

14
00:00:38.190 --> 00:00:40.170
learning algorithm has other you know

15
00:00:40.170 --> 00:00:42.629
numbers that you need to set such as the

16
00:00:42.629 --> 00:00:47.340
number of hidden layers so we call that

17
00:00:47.340 --> 00:00:50.629
capital L or the number of hidden units

18
00:00:50.629 --> 00:00:56.039
right such as zero and one and two and

19
00:00:56.039 --> 00:00:59.670
so on and then you also have the choice

20
00:00:59.670 --> 00:01:03.329
of activation function do you want to

21
00:01:03.329 --> 00:01:05.610
use a rel you or ten age or a sigma

22
00:01:05.610 --> 00:01:06.869
little something especially in the

23
00:01:06.869 --> 00:01:11.760
hidden layers and so all of these things

24
00:01:11.760 --> 00:01:13.590
are things that you need to tell your

25
00:01:13.590 --> 00:01:15.990
learning algorithm and so these are

26
00:01:15.990 --> 00:01:19.640
parameters that control the ultimate

27
00:01:19.640 --> 00:01:22.200
parameters W and B and so we call all of

28
00:01:22.200 --> 00:01:25.640
these things below hyper parameters

29
00:01:25.640 --> 00:01:29.340
because these things like alpha the

30
00:01:29.340 --> 00:01:30.750
learning rate the number of iterations

31
00:01:30.750 --> 00:01:32.369
number of hidden layers and so on these

32
00:01:32.369 --> 00:01:36.000
are all parameters that control W and B

33
00:01:36.000 --> 00:01:39.290
so we call these things hyper parameters

34
00:01:39.290 --> 00:01:41.970
because it is the hyper parameters that

35
00:01:41.970 --> 00:01:44.250
you know somehow determine the final

36
00:01:44.250 --> 00:01:46.950
value of the parameters W and B that you

37
00:01:46.950 --> 00:01:50.100
end up with in fact deep learning has a

38
00:01:50.100 --> 00:01:53.520
lot of different hyper parameters later

39
00:01:53.520 --> 00:01:55.470
in the later course we'll see other

40
00:01:55.470 --> 00:01:57.899
hyper parameters as well such as the

41
00:01:57.899 --> 00:02:05.150
momentum term the mini batch size

42
00:02:05.150 --> 00:02:07.220
various forms of regularization

43
00:02:07.220 --> 00:02:13.020
parameters and so on and if none of

44
00:02:13.020 --> 00:02:14.700
these terms at the bottom make sense yet

45
00:02:14.700 --> 00:02:16.020
don't worry about it we'll talk about

46
00:02:16.020 --> 00:02:18.810
them in the second course because deep

47
00:02:18.810 --> 00:02:21.870
learning has so many hyper parameters in

48
00:02:21.870 --> 00:02:24.120
contrast to earlier errors of machine

49
00:02:24.120 --> 00:02:26.370
learning I'm going to try to be very

50
00:02:26.370 --> 00:02:28.890
consistent in calling the learning rate

51
00:02:28.890 --> 00:02:31.050
alpha a hyper parameter rather than

52
00:02:31.050 --> 00:02:33.480
calling the parameter I think in earlier

53
00:02:33.480 --> 00:02:35.250
eras of machine learning when we didn't

54
00:02:35.250 --> 00:02:37.920
have so many hyper parameters most of us

55
00:02:37.920 --> 00:02:39.600
used to be a bit slow up here and just

56
00:02:39.600 --> 00:02:42.120
call alpha a parameter and technically

57
00:02:42.120 --> 00:02:44.610
alpha is a parameter but is a parameter

58
00:02:44.610 --> 00:02:47.580
that determines the real parameters our

59
00:02:47.580 --> 00:02:50.280
childhood consistent in calling these

60
00:02:50.280 --> 00:02:51.570
things like alpha the number of

61
00:02:51.570 --> 00:02:54.180
iterations and so on hyper parameters so

62
00:02:54.180 --> 00:02:55.769
when you're training a deep net for your

63
00:02:55.769 --> 00:02:57.810
own application you find that there may

64
00:02:57.810 --> 00:02:59.940
be a lot of possible settings for the

65
00:02:59.940 --> 00:03:01.560
hyper parameters that you need to just

66
00:03:01.560 --> 00:03:04.440
try out so apply deep learning today is

67
00:03:04.440 --> 00:03:07.230
a very imperiled process where often you

68
00:03:07.230 --> 00:03:09.840
might have an idea for example you might

69
00:03:09.840 --> 00:03:12.150
have an idea for the best value for the

70
00:03:12.150 --> 00:03:13.549
learning rate you might say well maybe

71
00:03:13.549 --> 00:03:16.739
alpha equals 0.01 I want to try that

72
00:03:16.739 --> 00:03:20.670
then you implemented try it out and then

73
00:03:20.670 --> 00:03:22.530
see how that works and then based on

74
00:03:22.530 --> 00:03:23.910
that outcome you might say you know what

75
00:03:23.910 --> 00:03:25.890
I've changed online I want to increase

76
00:03:25.890 --> 00:03:28.620
the learning rate to 0.05 and so if

77
00:03:28.620 --> 00:03:30.930
you're not sure what's the best value

78
00:03:30.930 --> 00:03:32.970
for the learning ready-to-use you might

79
00:03:32.970 --> 00:03:35.010
try one value of the learning rate alpha

80
00:03:35.010 --> 00:03:37.680
and see their cost function j go down

81
00:03:37.680 --> 00:03:39.690
like this then you might try a larger

82
00:03:39.690 --> 00:03:41.820
value for the learning rate alpha and

83
00:03:41.820 --> 00:03:43.650
see the cost function blow up and

84
00:03:43.650 --> 00:03:45.060
diverge then you might try another

85
00:03:45.060 --> 00:03:47.250
version and see it go down really fast

86
00:03:47.250 --> 00:03:49.709
it's inverse to higher value you might

87
00:03:49.709 --> 00:03:51.780
try another version and see it you know

88
00:03:51.780 --> 00:03:53.670
see the cost function J do that then

89
00:03:53.670 --> 00:03:55.530
I'll be China so the values you might

90
00:03:55.530 --> 00:03:57.840
say okay looks like this the value of

91
00:03:57.840 --> 00:04:00.870
alpha gives me a pretty fast learning

92
00:04:00.870 --> 00:04:02.790
and allows me to converge to a lower

93
00:04:02.790 --> 00:04:04.290
cost function jennice I'm going to use

94
00:04:04.290 --> 00:04:06.720
this value of alpha you saw in a

95
00:04:06.720 --> 00:04:08.040
previous slide that there are a lot of

96
00:04:08.040 --> 00:04:10.170
different hybrid parameters and it turns

97
00:04:10.170 --> 00:04:11.489
out that when you're starting on the new

98
00:04:11.489 --> 00:04:13.830
application I should find it very

99
00:04:13.830 --> 00:04:15.450
difficult to know in advance exactly

100
00:04:15.450 --> 00:04:17.940
what's the best value of the hyper

101
00:04:17.940 --> 00:04:20.580
parameters so what often happen is you

102
00:04:20.580 --> 00:04:22.169
just have to try out many different

103
00:04:22.169 --> 00:04:24.570
values and go around this cycle your

104
00:04:24.570 --> 00:04:26.970
trial some value really try five hidden

105
00:04:26.970 --> 00:04:28.440
layers with this many number of hidden

106
00:04:28.440 --> 00:04:31.140
units implement that see if it works and

107
00:04:31.140 --> 00:04:34.140
then iterate so the title of this slide

108
00:04:34.140 --> 00:04:36.180
is that apply deep learning is very

109
00:04:36.180 --> 00:04:38.340
empirical process and empirical process

110
00:04:38.340 --> 00:04:40.740
is maybe a fancy way of saying you just

111
00:04:40.740 --> 00:04:42.419
have to try a lot of things and see what

112
00:04:42.419 --> 00:04:45.330
works another effect I've seen is that

113
00:04:45.330 --> 00:04:47.190
deep learning today is applied to so

114
00:04:47.190 --> 00:04:48.810
many problems ranging from computer

115
00:04:48.810 --> 00:04:51.990
vision to speech recognition to natural

116
00:04:51.990 --> 00:04:53.789
language processing to a lot of

117
00:04:53.789 --> 00:04:55.500
structured data applications such as

118
00:04:55.500 --> 00:04:59.250
maybe a online advertising or web search

119
00:04:59.250 --> 00:05:02.430
or product recommendations and so on and

120
00:05:02.430 --> 00:05:05.640
what I've seen is that first I've seen

121
00:05:05.640 --> 00:05:08.190
researchers from one discipline any one

122
00:05:08.190 --> 00:05:10.080
of these try to go to a different one

123
00:05:10.080 --> 00:05:12.060
and sometimes the intuitions about hyper

124
00:05:12.060 --> 00:05:14.400
parameters carries over and sometimes it

125
00:05:14.400 --> 00:05:16.590
doesn't so I often advise people

126
00:05:16.590 --> 00:05:17.849
especially when starting on a new

127
00:05:17.849 --> 00:05:20.970
problem to just try out a range of

128
00:05:20.970 --> 00:05:23.550
values and see what works and then mix

129
00:05:23.550 --> 00:05:25.500
course we'll see a systematic way we'll

130
00:05:25.500 --> 00:05:27.930
see some systematic ways for trying out

131
00:05:27.930 --> 00:05:30.780
a range of values all right and second

132
00:05:30.780 --> 00:05:32.070
even if you're working on one

133
00:05:32.070 --> 00:05:33.570
application for a long time you know

134
00:05:33.570 --> 00:05:35.220
maybe you're working on online

135
00:05:35.220 --> 00:05:37.979
advertising as you make progress on the

136
00:05:37.979 --> 00:05:39.930
problem is quite possible there the best

137
00:05:39.930 --> 00:05:41.580
value for the learning rate a number of

138
00:05:41.580 --> 00:05:43.830
hidden units and so on might change so

139
00:05:43.830 --> 00:05:46.440
even if you tune your system to the best

140
00:05:46.440 --> 00:05:49.229
value of hyper parameters to daily as

141
00:05:49.229 --> 00:05:51.750
possible you find that the best value

142
00:05:51.750 --> 00:05:53.430
might change a year from now maybe

143
00:05:53.430 --> 00:05:55.650
because the computer infrastructure I'd

144
00:05:55.650 --> 00:05:57.840
be it you know CPUs or the type of GPU

145
00:05:57.840 --> 00:05:59.789
running on or something has changed but

146
00:05:59.789 --> 00:06:01.560
so maybe one rule of thumb is you know

147
00:06:01.560 --> 00:06:03.659
every now and then maybe every few

148
00:06:03.659 --> 00:06:05.070
months if you're working on a problem

149
00:06:05.070 --> 00:06:06.659
for an extended period of time for many

150
00:06:06.659 --> 00:06:09.030
years just try a few values for the

151
00:06:09.030 --> 00:06:10.800
hyper parameters and double check if

152
00:06:10.800 --> 00:06:12.570
there's a better value for the hyper

153
00:06:12.570 --> 00:06:15.150
parameters and as you do so you slowly

154
00:06:15.150 --> 00:06:17.280
gain intuition as well about the hyper

155
00:06:17.280 --> 00:06:18.779
parameters that work best for your

156
00:06:18.779 --> 00:06:19.870
problems

157
00:06:19.870 --> 00:06:21.820
and I know that this might seem like an

158
00:06:21.820 --> 00:06:24.010
unsatisfying part of deep learning that

159
00:06:24.010 --> 00:06:25.510
you just have to try on all the values

160
00:06:25.510 --> 00:06:27.940
for these hyper parameters but maybe

161
00:06:27.940 --> 00:06:30.160
this is one area where deep learning

162
00:06:30.160 --> 00:06:32.200
research is still advancing and maybe

163
00:06:32.200 --> 00:06:33.850
over time we'll be able to give better

164
00:06:33.850 --> 00:06:36.190
guidance for the best hyper parameters

165
00:06:36.190 --> 00:06:38.350
to use but it's also possible that

166
00:06:38.350 --> 00:06:41.260
because CPUs and GPUs and networks and

167
00:06:41.260 --> 00:06:43.630
data says are all changing and it is

168
00:06:43.630 --> 00:06:45.910
possible that the guidance won't to

169
00:06:45.910 --> 00:06:47.680
converge for some time and you just need

170
00:06:47.680 --> 00:06:49.360
to keep trying out different values and

171
00:06:49.360 --> 00:06:50.860
evaluate them on a hold on

172
00:06:50.860 --> 00:06:52.479
cross-validation set or something and

173
00:06:52.479 --> 00:06:54.100
pick the value that works for your

174
00:06:54.100 --> 00:06:56.350
problems so that was a brief discussion

175
00:06:56.350 --> 00:06:58.870
of hyper parameters in the second course

176
00:06:58.870 --> 00:07:01.030
we'll also give some suggestions for how

177
00:07:01.030 --> 00:07:03.280
to systematically explore the space of

178
00:07:03.280 --> 00:07:06.040
hyper parameters but by now you actually

179
00:07:06.040 --> 00:07:07.570
have pretty much all the tools you need

180
00:07:07.570 --> 00:07:09.430
to do their programming exercise before

181
00:07:09.430 --> 00:07:11.470
you do that adjust or share view one

182
00:07:11.470 --> 00:07:14.050
more set of ideas which is I often ask

183
00:07:14.050 --> 00:07:16.150
what does deep learning have to do the

184
00:07:16.150 --> 00:07:18.660
human brain