WEBVTT

1
00:00:00.000 --> 00:00:01.440
In the previous video,

2
00:00:01.440 --> 00:00:05.700
you saw how you can use vectorization to compute their predictions.

3
00:00:05.700 --> 00:00:11.485
The lowercase a's for an entire training set all at the same time.

4
00:00:11.485 --> 00:00:15.030
In this video, you see how you can use vectorization to also

5
00:00:15.030 --> 00:00:19.205
perform the gradient computations for all M training samples.

6
00:00:19.205 --> 00:00:21.380
Again, all sort of at the same time.

7
00:00:21.380 --> 00:00:22.890
And then at the end of this video,

8
00:00:22.890 --> 00:00:26.175
we'll put it all together and show how you can derive

9
00:00:26.175 --> 00:00:29.730
a very efficient implementation of logistic regression.

10
00:00:29.730 --> 00:00:32.505
So, you may remember that for the gradient computation,

11
00:00:32.505 --> 00:00:36.910
what we did was we computed dz1 for the first example,

12
00:00:36.910 --> 00:00:43.870
which could be a1 minus y1 and then dz2 equals

13
00:00:43.870 --> 00:00:52.134
a2 minus y2 and so on.

14
00:00:52.134 --> 00:00:56.425
And so on for all M training examples.

15
00:00:56.425 --> 00:01:01.218
So, what we're going to do is define a new variable,

16
00:01:01.218 --> 00:01:08.595
dZ is going to be dz1, dz2, dzm.

17
00:01:08.595 --> 00:01:13.910
Again, all the D lowercase z variables stacked horizontally.

18
00:01:13.910 --> 00:01:21.200
So, this would be 1 by m matrix or alternatively a m dimensional row vector.

19
00:01:21.200 --> 00:01:23.520
Now recall that from the previous slide,

20
00:01:23.520 --> 00:01:28.405
we'd already figured out how to compute capital A which was this: a1 through

21
00:01:28.405 --> 00:01:36.735
am and we had defined capital Y as y1 through ym.

22
00:01:36.735 --> 00:01:39.200
Also you know, stacked horizontally.

23
00:01:39.200 --> 00:01:42.780
So, based on these definitions,

24
00:01:42.780 --> 00:01:46.770
maybe you can see for yourself that dz can be computed as

25
00:01:46.770 --> 00:01:52.750
just A minus Y because it's going to be equal to a1 - y1.

26
00:01:52.750 --> 00:01:55.670
So, the first element, a2 - y2,

27
00:01:55.670 --> 00:01:59.980
so in the second element and so on.

28
00:01:59.980 --> 00:02:06.115
And, so this first element a1 - y1 is exactly the definition of dz1.

29
00:02:06.115 --> 00:02:11.670
The second element is exactly the definition of dz2 and so on.

30
00:02:11.670 --> 00:02:13.965
So, with just one line of code,

31
00:02:13.965 --> 00:02:20.095
you can compute all of this at the same time.

32
00:02:20.095 --> 00:02:24.010
Now, in the previous implementation,

33
00:02:24.010 --> 00:02:27.695
we've gotten rid of one for loop already but we still had

34
00:02:27.695 --> 00:02:31.600
this second for loop over training examples.

35
00:02:31.600 --> 00:02:35.440
So we initialize dw to zero to a vector of zeroes.

36
00:02:35.440 --> 00:02:38.905
But then we still have to loop over 20 examples where we have

37
00:02:38.905 --> 00:02:43.015
dw plus equals x1 times dz1,

38
00:02:43.015 --> 00:02:50.440
for the first training example dw plus equals x2 dz2 and so on.

39
00:02:50.440 --> 00:02:56.980
So we do the M times and then dw divide equals by M and similarly for B, right?

40
00:02:56.980 --> 00:03:03.370
db was initialized as 0 and db plus equals dz1.

41
00:03:03.370 --> 00:03:09.120
db plus equals dz2 down to you know

42
00:03:09.120 --> 00:03:16.835
dz(m) and db divide equals M. So that's what we had in the previous implementation.

43
00:03:16.835 --> 00:03:18.700
We'd already got rid of one for loop.

44
00:03:18.700 --> 00:03:25.045
So, at least now dw is a vector and we went separately updating dw1,

45
00:03:25.045 --> 00:03:26.850
dw2 and so on.

46
00:03:26.850 --> 00:03:29.860
So, we got rid of that already but we still

47
00:03:29.860 --> 00:03:33.630
had the for loop over the M examples in the training set.

48
00:03:33.630 --> 00:03:36.290
So, let's take these operations and vectorize them.

49
00:03:36.290 --> 00:03:38.380
Here's what we can do, for

50
00:03:38.380 --> 00:03:42.745
the vectorized implementation of db, what it's doing is basically summing up,

51
00:03:42.745 --> 00:03:47.940
all of these dzs and then dividing by m. So,

52
00:03:47.940 --> 00:03:51.580
db is basically one over m,

53
00:03:51.580 --> 00:03:56.530
sum from I equals one through m of dzi and

54
00:03:56.530 --> 00:04:03.055
well all the dzs are in that row vector and so in Python,

55
00:04:03.055 --> 00:04:04.765
what you do is implement you know,

56
00:04:04.765 --> 00:04:08.155
1 over a m times np.

57
00:04:08.155 --> 00:04:12.210
sum of dz.

58
00:04:12.210 --> 00:04:15.115
So, you just take this variable and call the np.

59
00:04:15.115 --> 00:04:19.195
sum function on it and that would give you db.

60
00:04:19.195 --> 00:04:22.330
How about dw? I'll just write

61
00:04:22.330 --> 00:04:26.375
out the correct equations who can verify is the right thing to do.

62
00:04:26.375 --> 00:04:28.164
DW turns out to be one over M,

63
00:04:28.164 --> 00:04:34.485
times the matrix X times dz transpose.

64
00:04:34.485 --> 00:04:37.990
And, so kind of see why that's the case.

65
00:04:37.990 --> 00:04:41.806
This is equal to one over m then the matrix X's,

66
00:04:41.806 --> 00:04:48.325
x1 through xm stacked up in columns like that and dz

67
00:04:48.325 --> 00:04:56.040
transpose is going to be dz1 down to dz(m) like so.

68
00:04:56.040 --> 00:05:00.900
And so, if you figure out what this matrix times this vector works out to be,

69
00:05:00.900 --> 00:05:05.585
it is turns out to be one over m times x1

70
00:05:05.585 --> 00:05:12.523
dz1 plus... plus xm dzm.

71
00:05:12.523 --> 00:05:21.405
And so, this is a n/1 vector and this is what you actually end up with,

72
00:05:21.405 --> 00:05:24.725
with dw because dw was taking these you know,

73
00:05:24.725 --> 00:05:27.710
xi dzi and adding them up and so that's what exactly

74
00:05:27.710 --> 00:05:32.300
this matrix vector multiplication is doing and so again,

75
00:05:32.300 --> 00:05:35.655
with one line of code you can compute dw.

76
00:05:35.655 --> 00:05:40.010
So, the vectorized implementation of the derivative calculations is just this,

77
00:05:40.010 --> 00:05:44.540
you use this line to implement db and use

78
00:05:44.540 --> 00:05:50.540
this line to implement dw and notice that without a for loop over the training set,

79
00:05:50.540 --> 00:05:55.265
you can now compute the updates you want to your parameters.

80
00:05:55.265 --> 00:06:01.185
So now, let's put all together into how you would actually implement logistic regression.

81
00:06:01.185 --> 00:06:02.550
So, this is our original,

82
00:06:02.550 --> 00:06:07.866
highly inefficient non vectorize implementation.

83
00:06:07.866 --> 00:06:11.775
So, the first thing we've done in the previous video was get rid of this volume, right?

84
00:06:11.775 --> 00:06:14.400
So, instead of looping over dw1,

85
00:06:14.400 --> 00:06:15.755
dw2 and so on,

86
00:06:15.755 --> 00:06:23.595
we have replaced this with a vector value dw which is dw+= xi,

87
00:06:23.595 --> 00:06:28.775
which is now a vector times dz(i).

88
00:06:28.775 --> 00:06:32.000
But now, we will see that we can also get rid of not

89
00:06:32.000 --> 00:06:36.670
just a for loop below but also get rid of this for loop.

90
00:06:36.670 --> 00:06:38.654
So, here is how you do it.

91
00:06:38.654 --> 00:06:42.925
So, using what we have from the previous slides,

92
00:06:42.925 --> 00:06:46.085
you would say, capitalZ,

93
00:06:46.085 --> 00:06:57.625
Z equal to w transpose X + B and the code you is write capital Z equals np.

94
00:06:57.625 --> 00:07:07.315
w transpose X + B and then a equals sigmoid of capital Z.

95
00:07:07.315 --> 00:07:12.710
So, you have now computed all of this and all of this for all the values of I.

96
00:07:12.710 --> 00:07:14.715
Next on the previous slide,

97
00:07:14.715 --> 00:07:21.070
we said you would compute dz equals A - Y.

98
00:07:21.070 --> 00:07:24.460
So, now you computed all of this for all the values of i.

99
00:07:24.460 --> 00:07:31.495
Then, finally dw equals 1/m x

100
00:07:31.495 --> 00:07:39.700
dz transpose and db equals 1/m of you know, np.

101
00:07:39.700 --> 00:07:43.328
sum dz.

102
00:07:43.328 --> 00:07:49.120
So, you've just done forward propagation and back propagation,

103
00:07:49.120 --> 00:07:53.030
really computing the predictions and computing the derivatives on

104
00:07:53.030 --> 00:07:57.340
all M training examples without using a for loop.

105
00:07:57.340 --> 00:08:00.835
And so the gradient descent update then would be you know W

106
00:08:00.835 --> 00:08:04.462
gets updated as w minus the learning rate times

107
00:08:04.462 --> 00:08:12.020
dw which was just computed above and B is update as B minus the learning rate times db.

108
00:08:12.020 --> 00:08:17.341
Sometimes is putting colons to that to denote that as is an assignment,

109
00:08:17.341 --> 00:08:21.675
but I guess I haven't been totally consistent with that notation.

110
00:08:21.675 --> 00:08:25.450
But with this, you have just implemented

111
00:08:25.450 --> 00:08:29.635
a single iteration of gradient descent for logistic regression.

112
00:08:29.635 --> 00:08:32.308
Now, I know I said that we should get rid of

113
00:08:32.308 --> 00:08:35.260
explicit full loops whenever you can but if you want to

114
00:08:35.260 --> 00:08:38.230
implement multiple iterations as

115
00:08:38.230 --> 00:08:42.880
a gradient descent then you still need a full loop over the number of iterations.

116
00:08:42.880 --> 00:08:47.860
So, if you want to have a thousand iterations of gradient descent,

117
00:08:47.860 --> 00:08:53.675
you might still need a full loop over the iteration number.

118
00:08:53.675 --> 00:08:55.870
There is an outermost full loop like that then I

119
00:08:55.870 --> 00:08:59.210
don't think there is any way to get rid of that full loop.

120
00:08:59.210 --> 00:09:02.390
But I do think it's incredibly cool that you can implement

121
00:09:02.390 --> 00:09:07.117
at least one iteration of gradient descent without needing to use a full loop.

122
00:09:07.117 --> 00:09:09.880
So, that's it you now have a highly vectorize and

123
00:09:09.880 --> 00:09:14.745
highly efficient implementation of gradient descent for logistic regression.

124
00:09:14.745 --> 00:09:18.850
There is just one more detail that I want to talk about in the next video,

125
00:09:18.850 --> 00:09:24.155
which is in our description here I briefly alluded to this technique called broadcasting.

126
00:09:24.155 --> 00:09:28.240
Broadcasting turns out to be a technique that Python and

127
00:09:28.240 --> 00:09:32.915
numpy allows you to use to make certain parts of your code also much more efficient.

128
00:09:32.915 --> 00:09:37.000
So, let's see some more details of broadcasting in the next video.