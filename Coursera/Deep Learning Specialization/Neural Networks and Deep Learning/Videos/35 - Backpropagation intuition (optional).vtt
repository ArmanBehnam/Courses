WEBVTT

1
00:00:00.000 --> 00:00:01.620
In the last video you saw

2
00:00:01.620 --> 00:00:03.690
the equations for
backpropagation.

3
00:00:03.690 --> 00:00:06.510
In this video let's go
over some intuition,

4
00:00:06.510 --> 00:00:08.325
using the computation graph

5
00:00:08.325 --> 00:00:10.470
for how those equations
were derived.

6
00:00:10.470 --> 00:00:12.390
This video is completely optional

7
00:00:12.390 --> 00:00:14.130
so if you free to
watch it or not,

8
00:00:14.130 --> 00:00:16.170
you should be able to do
the whole work either way.

9
00:00:16.170 --> 00:00:18.420
So recall that when
we talked about

10
00:00:18.420 --> 00:00:21.855
logistic regression we
had this forward pass,

11
00:00:21.855 --> 00:00:26.250
where we'll compute z
then a and then a loss,

12
00:00:26.250 --> 00:00:27.870
and entity derivatives we had

13
00:00:27.870 --> 00:00:32.520
this backward pass where
we could first compute da,

14
00:00:32.520 --> 00:00:35.295
and then go on to compute dz,

15
00:00:35.295 --> 00:00:40.770
and then go on to
compute dw and db.

16
00:00:40.770 --> 00:00:48.530
So, the definition for the loss
was l of a comma y equals

17
00:00:48.530 --> 00:00:57.010
negative y log a minus one
minus y times log 1 minus a.

18
00:00:57.010 --> 00:00:59.690
So if you're familiar with

19
00:00:59.690 --> 00:01:01.040
calculus and you take

20
00:01:01.040 --> 00:01:03.770
the derivative of this
with respect to a,

21
00:01:03.770 --> 00:01:06.195
that will give you
the formula for da.

22
00:01:06.195 --> 00:01:08.825
So da is equal to that.

23
00:01:08.825 --> 00:01:10.485
If you actually figure out the

24
00:01:10.485 --> 00:01:12.270
calculus you could show that this

25
00:01:12.270 --> 00:01:18.990
is negative y over a plus one
minus y over one minus a.

26
00:01:18.990 --> 00:01:20.510
Just derived at from

27
00:01:20.510 --> 00:01:22.750
calculus by taking
derivatives of this.

28
00:01:22.750 --> 00:01:24.470
It turns out when you take

29
00:01:24.470 --> 00:01:26.480
another step backwards
to compute dz,

30
00:01:26.480 --> 00:01:29.600
we then worked out that
dz is equal to a minus

31
00:01:29.600 --> 00:01:32.389
y. I did explain why previously,

32
00:01:32.389 --> 00:01:36.200
but it turns out that from
the chain rule of calculus,

33
00:01:36.200 --> 00:01:44.955
dz is equal to da
times g prime of z.

34
00:01:44.955 --> 00:01:50.550
Where here, g of z
equals sigmoid of z,

35
00:01:50.550 --> 00:01:53.030
is activation function for

36
00:01:53.030 --> 00:01:55.790
this output unit in
logistic regression.

37
00:01:55.790 --> 00:01:59.210
So just remember this is
still logistic regression.

38
00:01:59.210 --> 00:02:00.710
We have x_1, x_2,

39
00:02:00.710 --> 00:02:03.780
x_3 and then just
one sigmoid unit

40
00:02:03.780 --> 00:02:05.925
and then that gives us a,

41
00:02:05.925 --> 00:02:07.365
gives us y hat.

42
00:02:07.365 --> 00:02:11.590
So here the activation function
was a sigmoid function.

43
00:02:11.590 --> 00:02:14.470
As in aside only for
those of you familiar

44
00:02:14.470 --> 00:02:17.105
with the chain rule of calculus,

45
00:02:17.105 --> 00:02:20.665
the reason for this is
because a is equal to

46
00:02:20.665 --> 00:02:25.015
sigmoid of z and so partial of

47
00:02:25.015 --> 00:02:29.290
L with respect to z is
equal to partial of

48
00:02:29.290 --> 00:02:35.580
L with respect to a times da,

49
00:02:35.580 --> 00:02:39.750
dz but since a is
equal to sigmoid of z,

50
00:02:39.750 --> 00:02:41.820
this is equal to d,

51
00:02:41.820 --> 00:02:48.815
dz g of z which is
equal to g prime of z.

52
00:02:48.815 --> 00:02:51.319
So that's why this expression,

53
00:02:51.319 --> 00:02:53.285
which is dz in our codes,

54
00:02:53.285 --> 00:02:55.940
is equal to this expression
which is da in

55
00:02:55.940 --> 00:02:59.205
our codes times g prime of

56
00:02:59.205 --> 00:03:06.210
z. and so this is just that.

57
00:03:06.210 --> 00:03:09.830
So that last deviation will
make sense only if you're

58
00:03:09.830 --> 00:03:11.300
familiar with calculus and

59
00:03:11.300 --> 00:03:13.520
specifically the chain rule
from calculus,

60
00:03:13.520 --> 00:03:15.290
but if not don't worry about it.

61
00:03:15.290 --> 00:03:18.830
I'll try to explain the
intuition wherever is needed.

62
00:03:18.830 --> 00:03:21.200
Then finally having
computed dz for

63
00:03:21.200 --> 00:03:24.745
logistic regression
we will compute dw,

64
00:03:24.745 --> 00:03:27.630
which it turned out
was dz times x and

65
00:03:27.630 --> 00:03:28.950
db which is just

66
00:03:28.950 --> 00:03:31.245
dz where you have
a single chain example.

67
00:03:31.245 --> 00:03:33.560
So that was logistic regression.

68
00:03:33.560 --> 00:03:35.780
So what we're going to do when

69
00:03:35.780 --> 00:03:38.390
computing backpropagation
for a neural network

70
00:03:38.390 --> 00:03:39.890
is a calculation a lot

71
00:03:39.890 --> 00:03:42.155
like this but only we'll do it

72
00:03:42.155 --> 00:03:45.110
twice because now we have not

73
00:03:45.110 --> 00:03:48.020
x going to an output unit
but x going to

74
00:03:48.020 --> 00:03:51.060
a hidden layer and then
going to an output unit.

75
00:03:51.060 --> 00:03:53.990
So instead of instead of

76
00:03:53.990 --> 00:03:58.010
this computation being
one step as we have here,

77
00:03:58.010 --> 00:04:00.455
we'll have two steps here

78
00:04:00.455 --> 00:04:04.205
in this kind of neural network
with two layers.

79
00:04:04.205 --> 00:04:07.310
So in this two layer
neural network

80
00:04:07.310 --> 00:04:08.510
that is with the input layer,

81
00:04:08.510 --> 00:04:10.175
a hidden layer, and
an output layer

82
00:04:10.175 --> 00:04:12.065
remember the steps
of our computation.

83
00:04:12.065 --> 00:04:17.230
First, you compute z_1
using this equation,

84
00:04:17.230 --> 00:04:19.530
and then compute a_1,

85
00:04:19.530 --> 00:04:21.675
and then you compute z_2.

86
00:04:21.675 --> 00:04:25.685
Notice z_2 also depends on
the parameters w_2 and b_2.

87
00:04:25.685 --> 00:04:28.670
Then based on z_2 compute a_2

88
00:04:28.670 --> 00:04:32.410
and finally that
gives you the loss.

89
00:04:32.410 --> 00:04:35.320
So what backpropagation does is,

90
00:04:35.320 --> 00:04:42.440
it will go backwards to
compute da_2 and then dz_2.

91
00:04:42.440 --> 00:04:48.815
Looking to go back to
compute dw_2 and db_2,

92
00:04:48.815 --> 00:04:57.060
go backwards to compute
da_1, dz_1 and so on.

93
00:04:57.110 --> 00:05:00.280
We don't need to take
derivatives respect to

94
00:05:00.280 --> 00:05:02.020
the input x since the input x

95
00:05:02.020 --> 00:05:03.850
for supervised learning is fixed.

96
00:05:03.850 --> 00:05:05.470
So we're not trying
to optimize x.

97
00:05:05.470 --> 00:05:07.870
So we won't bother
to take derivatives

98
00:05:07.870 --> 00:05:10.780
at least for supervised learning
with respect to x.

99
00:05:10.780 --> 00:05:16.225
So I'm going to skip
explicitly computing da_2.

100
00:05:16.225 --> 00:05:17.770
If you want you can actually

101
00:05:17.770 --> 00:05:19.945
compute da_2 and then
use that to compute

102
00:05:19.945 --> 00:05:22.360
dz_2 but in practice you
could collapse both of

103
00:05:22.360 --> 00:05:25.420
these steps into one step so

104
00:05:25.420 --> 00:05:27.775
you end up at dz_2 is equal

105
00:05:27.775 --> 00:05:31.725
to a_2 minus y, same as before.

106
00:05:31.725 --> 00:05:38.610
You also going to write dw_2
and db_2 down here below.

107
00:05:38.610 --> 00:05:42.300
You have that dW_2 is equal to

108
00:05:42.300 --> 00:05:51.865
dz_2 times a_1 transpose
and db_2 equals dz_2.

109
00:05:51.865 --> 00:05:53.750
So this step is quite similar

110
00:05:53.750 --> 00:05:56.480
for logistic regression
where we had that

111
00:05:56.480 --> 00:06:01.845
dw was equal to dz times x.

112
00:06:01.845 --> 00:06:05.130
Except that now, a_1 plays

113
00:06:05.130 --> 00:06:06.330
the role of x and there's

114
00:06:06.330 --> 00:06:08.720
an extra transpose there because

115
00:06:08.720 --> 00:06:11.870
the relationship between
the capital matrix W

116
00:06:11.870 --> 00:06:13.730
and our individual parameters

117
00:06:13.730 --> 00:06:16.220
w there's a transpose there.

118
00:06:16.220 --> 00:06:21.790
Because w is equal
to a row vector,

119
00:06:21.790 --> 00:06:23.450
in the case of
logistic regression with

120
00:06:23.450 --> 00:06:26.690
a single output dw_2
is like that,

121
00:06:26.690 --> 00:06:28.430
whereas w here was
a column vector.

122
00:06:28.430 --> 00:06:32.180
So that's why there's
an extra transpose for

123
00:06:32.180 --> 00:06:36.485
a_1 whereas we didn't for x
here for logistic regression.

124
00:06:36.485 --> 00:06:40.415
So this completes half
of backpropagation.

125
00:06:40.415 --> 00:06:42.545
Then again you can compute

126
00:06:42.545 --> 00:06:45.335
da_1 if you wish
although in practice

127
00:06:45.335 --> 00:06:49.080
the computation for da_1

128
00:06:49.080 --> 00:06:53.550
and dz_1 usually collapse
into one step and so what you

129
00:06:53.550 --> 00:06:56.535
could actually implement
is that dz_1 is equal to

130
00:06:56.535 --> 00:07:01.710
w_2 transpose times dz_2 and

131
00:07:01.710 --> 00:07:10.955
then times an element-wise
product of g_1 prime of z_1.

132
00:07:10.955 --> 00:07:13.850
Just to do a check
on the dimensions,

133
00:07:13.850 --> 00:07:19.410
if you have a neural network
that looks like this,

134
00:07:20.360 --> 00:07:22.980
outputs y like so.

135
00:07:22.980 --> 00:07:26.250
If you have n_0 and x equals

136
00:07:26.250 --> 00:07:31.515
n_0 input features and
one hidden unit, and n_2.

137
00:07:31.515 --> 00:07:35.520
So far, and n_2

138
00:07:35.520 --> 00:07:40.050
In our case, just
one output unit then

139
00:07:40.050 --> 00:07:48.780
the matrix W_2 is n_2
by n_1 dimensional,

140
00:07:48.780 --> 00:07:53.025
z_2 and therefore, dz_2

141
00:07:53.025 --> 00:07:57.450
are going to be n_2
by one-dimensional.

142
00:07:57.450 --> 00:07:59.555
There's really going
to be a one-by-one

143
00:07:59.555 --> 00:08:02.610
when we're doing
binary classification.

144
00:08:02.610 --> 00:08:04.995
Z_1, and therefore also,

145
00:08:04.995 --> 00:08:09.720
dz_1 are going to be
n_1 by one-dimensional.

146
00:08:09.720 --> 00:08:12.530
Note that for any variable foo,

147
00:08:12.530 --> 00:08:16.035
foo and dfoo always have
the same dimensions.

148
00:08:16.035 --> 00:08:20.040
So that's why w and dw always
have the same dimension.

149
00:08:20.040 --> 00:08:22.140
Similarly for b, and db,

150
00:08:22.140 --> 00:08:23.625
and z, and dz, and so on.

151
00:08:23.625 --> 00:08:26.865
So to make sure that the
dimensions of this all match up,

152
00:08:26.865 --> 00:08:35.490
we have that dz_1 is equal
to w_2 transpose times dz_2.

153
00:08:35.490 --> 00:08:41.519
Then this is an
element-wise product times

154
00:08:41.519 --> 00:08:44.395
g_1 prime of z_1.

155
00:08:44.395 --> 00:08:46.740
So matching the
dimensions from above,

156
00:08:46.740 --> 00:08:50.195
this is going to be n_1 by one,

157
00:08:50.195 --> 00:08:52.595
is equal to w_2 transpose,

158
00:08:52.595 --> 00:08:53.620
we transpose of this.

159
00:08:53.620 --> 00:08:58.625
So it's just going to be
n_1 by n_2 dimensional.

160
00:08:58.625 --> 00:09:04.625
dz_2 is going to be n_2
by one-dimensional.

161
00:09:04.625 --> 00:09:07.235
Then this, this
the same dimension as z_1.

162
00:09:07.235 --> 00:09:08.450
So this is also

163
00:09:08.450 --> 00:09:12.650
n_1 by one dimensional,
so element-wise product.

164
00:09:12.650 --> 00:09:14.295
So the dimensions too make sense.

165
00:09:14.295 --> 00:09:18.120
n_1 one by 1 dimensional
vector can be obtained

166
00:09:18.120 --> 00:09:21.845
by an n_1 by n_2 dimensional
matrix times n_2 by n_1,

167
00:09:21.845 --> 00:09:24.490
because the product of
these two things gives

168
00:09:24.490 --> 00:09:27.640
you an n_1 by
one-dimensional matrix.

169
00:09:27.640 --> 00:09:31.060
So this becomes
the element-wise product of

170
00:09:31.060 --> 00:09:34.520
two and one by
one dimensional vectors

171
00:09:34.520 --> 00:09:36.360
and so dimensions do match up.

172
00:09:36.360 --> 00:09:38.640
One tip when implementing

173
00:09:38.640 --> 00:09:41.500
a backdrop: If you just

174
00:09:41.500 --> 00:09:44.730
make sure that the dimensions
of your matrices match up,

175
00:09:44.730 --> 00:09:47.230
so if you think through
what are the dimensions of

176
00:09:47.230 --> 00:09:50.670
your various matrices
including w_1, w_2, z_1,

177
00:09:50.670 --> 00:09:54.180
z_2, a_1, a_2, and so
on and just make sure

178
00:09:54.180 --> 00:09:56.310
that the dimensions of

179
00:09:56.310 --> 00:09:58.730
these matrix operations
may match up.

180
00:09:58.730 --> 00:10:00.750
Sometimes, that will already

181
00:10:00.750 --> 00:10:03.230
eliminate quite a lot
of bugs and back-prop.

182
00:10:03.230 --> 00:10:05.745
All right. So this gives us dz_1.

183
00:10:05.745 --> 00:10:11.640
Then finally, just to
wrap up dw_1 and db_1,

184
00:10:11.640 --> 00:10:13.940
we should write
them here I guess.

185
00:10:13.940 --> 00:10:15.400
But since I'm running
out of space,

186
00:10:15.400 --> 00:10:17.160
I'll write them on
the right of the slide.

187
00:10:17.160 --> 00:10:21.575
dw_1 and db_1 are given by
the following formulas.

188
00:10:21.575 --> 00:10:25.950
This is going to be equal
to dz_1 times x transpose.

189
00:10:25.950 --> 00:10:28.560
This is going to be equal to dz,

190
00:10:28.560 --> 00:10:31.180
and you might notice
a similarity between

191
00:10:31.180 --> 00:10:34.045
these equations and
these equations,

192
00:10:34.045 --> 00:10:36.355
which is really
no coincidence because

193
00:10:36.355 --> 00:10:39.380
x plays the role of a_0.

194
00:10:39.380 --> 00:10:41.605
So x transpose is a_0 transpose.

195
00:10:41.605 --> 00:10:45.575
So those equations are actually
very similar. All right.

196
00:10:45.575 --> 00:10:47.345
So that gives a sense for

197
00:10:47.345 --> 00:10:50.255
how back-propagation is derived.

198
00:10:50.255 --> 00:10:54.545
We have six key
equations here for dz_2,

199
00:10:54.545 --> 00:11:00.220
dw_2, db_2, dz_1, dw_1, and db_1.

200
00:11:00.220 --> 00:11:02.640
So let me just take these six
equations and copy them

201
00:11:02.640 --> 00:11:05.850
over to the next slide.
Here they are.

202
00:11:05.850 --> 00:11:09.390
So far we have derived
back-propagation for

203
00:11:09.390 --> 00:11:13.605
ABA training on a single
training example at the time.

204
00:11:13.605 --> 00:11:17.410
But it should come
as no surprise that

205
00:11:17.410 --> 00:11:21.525
rather than working on
a single example at a time,

206
00:11:21.525 --> 00:11:24.315
we would like to vectorize

207
00:11:24.315 --> 00:11:27.670
across different
training examples.

208
00:11:27.670 --> 00:11:30.890
So you remember that
for propagation,

209
00:11:30.890 --> 00:11:33.540
when we're operating on
one example at a time,

210
00:11:33.540 --> 00:11:35.705
we had equations like this,

211
00:11:35.705 --> 00:11:41.645
as let us say, a_1
equals g_1 of z_1.

212
00:11:41.645 --> 00:11:43.605
In order to vectorize,

213
00:11:43.605 --> 00:11:47.175
we took say the z's

214
00:11:47.175 --> 00:11:52.850
and stack them up in
columns like this,

215
00:11:52.850 --> 00:12:00.905
z_1_m, and call this capital Z.

216
00:12:00.905 --> 00:12:04.960
Then we found that by
stacking things up in columns

217
00:12:04.960 --> 00:12:10.145
and defining the capital
uppercase version of this,

218
00:12:10.145 --> 00:12:13.630
we then just had z_1 equals

219
00:12:13.630 --> 00:12:20.310
w_1_x plus b and a_1 equals

220
00:12:20.310 --> 00:12:25.160
g_1 of z_1.

221
00:12:25.160 --> 00:12:26.845
We define the notation
very carefully

222
00:12:26.845 --> 00:12:28.995
in this course to make sure that

223
00:12:28.995 --> 00:12:32.430
stacking examples into
different columns

224
00:12:32.430 --> 00:12:35.300
of a matrix makes
all this workout.

225
00:12:35.300 --> 00:12:37.425
So it turns out that

226
00:12:37.425 --> 00:12:40.085
if you go through
the math carefully,

227
00:12:40.085 --> 00:12:43.585
the same trick also works
for back-propagation.

228
00:12:43.585 --> 00:12:47.205
So the vectorized equations
are as follows: First,

229
00:12:47.205 --> 00:12:49.250
if you take these

230
00:12:49.250 --> 00:12:52.230
d_z's for different training
examples and stack

231
00:12:52.230 --> 00:12:54.970
them as the different columns

232
00:12:54.970 --> 00:12:58.125
of the matrix and same for
this and same for this,

233
00:12:58.125 --> 00:13:01.320
then this is the
vectorized implementation.

234
00:13:01.320 --> 00:13:03.510
Then here's the definition or

235
00:13:03.510 --> 00:13:06.140
here's how you can compute dW_2.

236
00:13:06.140 --> 00:13:08.250
There is this extra one over m

237
00:13:08.250 --> 00:13:11.105
because the cost function J is

238
00:13:11.105 --> 00:13:13.980
this one over m of sum

239
00:13:13.980 --> 00:13:18.210
from I equals one
through m of the losses.

240
00:13:18.210 --> 00:13:20.615
So when computing derivatives,

241
00:13:20.615 --> 00:13:23.375
we have that extra one over
m term just as we did when we

242
00:13:23.375 --> 00:13:25.170
were computing the weight

243
00:13:25.170 --> 00:13:27.370
updates for logistic regression.

244
00:13:27.370 --> 00:13:31.405
Then that's the update
you get for db_2.

245
00:13:31.405 --> 00:13:34.950
Again, some of the d_z's
and then we have a one over

246
00:13:34.950 --> 00:13:39.995
m. Then dz_1 is
computed as follows.

247
00:13:39.995 --> 00:13:45.260
Once again, this is
an element-wise product,

248
00:13:45.440 --> 00:13:50.190
only whereas
previously, we saw on

249
00:13:50.190 --> 00:13:52.750
the previous slide that this was

250
00:13:52.750 --> 00:13:56.445
an n_1 by one dimensional vector.

251
00:13:56.445 --> 00:14:02.835
Now, this is a n_1 by
m dimensional matrix.

252
00:14:02.835 --> 00:14:08.900
Both of these are also
n_1 by m dimensional.

253
00:14:08.900 --> 00:14:11.855
So that's why that asterisk is

254
00:14:11.855 --> 00:14:17.760
a element-wise
product. All right.

255
00:14:17.760 --> 00:14:21.540
Then finally, the
remaining two updates.

256
00:14:21.540 --> 00:14:24.410
Perhaps, it shouldn't
look too surprising.

257
00:14:24.500 --> 00:14:27.490
So I hope that gives
you some intuition for

258
00:14:27.490 --> 00:14:30.140
how the back-propagation
algorithm is derived.

259
00:14:30.140 --> 00:14:32.070
In all of machine learning,

260
00:14:32.070 --> 00:14:35.030
I think the derivation of
the back-propagation algorithm

261
00:14:35.030 --> 00:14:37.115
is actually one of
the most complicated pieces

262
00:14:37.115 --> 00:14:38.540
of math I've seen.

263
00:14:38.540 --> 00:14:42.450
It requires knowing
both linear algebra as well as

264
00:14:42.450 --> 00:14:44.430
the derivative of
matrices to really

265
00:14:44.430 --> 00:14:46.715
derive it from scratch
from first principles.

266
00:14:46.715 --> 00:14:50.154
If you are an expert
in matrix calculus,

267
00:14:50.154 --> 00:14:52.170
using this process, you

268
00:14:52.170 --> 00:14:54.120
might want to derive
the algorithm yourself.

269
00:14:54.120 --> 00:14:55.620
But I think that
there are actually

270
00:14:55.620 --> 00:14:57.465
plenty of deep
learning practitioners

271
00:14:57.465 --> 00:15:00.039
that have seen the derivation

272
00:15:00.039 --> 00:15:02.340
at about the level you've
seen in this video,

273
00:15:02.340 --> 00:15:05.190
and are already able to have
all the right intuitions

274
00:15:05.190 --> 00:15:08.390
and be able to implement
this algorithm very effectively.

275
00:15:08.390 --> 00:15:10.700
So if you are
an expert in calculus,

276
00:15:10.700 --> 00:15:13.250
do see if you can divide
the whole thing from scratch.

277
00:15:13.250 --> 00:15:15.815
It is one of the very
hardest pieces of map on

278
00:15:15.815 --> 00:15:17.584
the very hardest derivations

279
00:15:17.584 --> 00:15:19.845
that I've seen in all
of machine learning.

280
00:15:19.845 --> 00:15:22.535
But either way, if
you implement this,

281
00:15:22.535 --> 00:15:24.450
this will work and
I think you have

282
00:15:24.450 --> 00:15:27.240
enough intuitions to tune
in and get it to work.

283
00:15:27.240 --> 00:15:30.780
So there's just one
last detail I will

284
00:15:30.780 --> 00:15:34.140
share of you before you
implement your neural network,

285
00:15:34.140 --> 00:15:35.315
which is how to

286
00:15:35.315 --> 00:15:37.590
initialize the weights
of your neural network.

287
00:15:37.590 --> 00:15:40.890
It turns out that initializing
your parameters not to

288
00:15:40.890 --> 00:15:42.995
zero but randomly turns

289
00:15:42.995 --> 00:15:45.485
out to be very important
for training video network.

290
00:15:45.485 --> 00:15:48.210
In the next video,
you'll see why.