WEBVTT

1
00:00:00.000 --> 00:00:03.116
You see me draw a few
pictures of neural networks.

2
00:00:03.116 --> 00:00:05.712
In this video, we'll talk about
exactly what those pictures means.

3
00:00:05.712 --> 00:00:06.728
In other words,

4
00:00:06.728 --> 00:00:11.235
exactly what those neural networks
that we've been drawing represent.

5
00:00:11.235 --> 00:00:15.014
And we'll start with focusing on
the case of neural networks with

6
00:00:15.014 --> 00:00:17.290
what was called a single hidden layer.

7
00:00:17.290 --> 00:00:19.667
Here's a picture of a neural network.

8
00:00:19.667 --> 00:00:22.986
Let's give different parts of
these pictures some names.

9
00:00:22.986 --> 00:00:27.447
We have the input features, x1,
x2, x3 stacked up vertically.

10
00:00:27.447 --> 00:00:30.694
And this is called the input
layer of the neural network.

11
00:00:30.694 --> 00:00:35.764
So maybe not surprisingly, this contains
the inputs to the neural network.

12
00:00:35.764 --> 00:00:37.990
Then there's another layer of circles.

13
00:00:37.990 --> 00:00:41.663
And this is called a hidden
layer of the neural network.

14
00:00:41.663 --> 00:00:45.414
I'll come back in a second to
say what the word hidden means.

15
00:00:45.414 --> 00:00:49.509
But the final layer here is formed by,
in this case, just one node.

16
00:00:49.509 --> 00:00:53.894
And this single-node layer is called
the output layer, and is responsible for

17
00:00:53.894 --> 00:00:57.000
generating the predicted value y hat.

18
00:00:57.000 --> 00:00:59.932
In a neural network that
you train with supervised learning,

19
00:00:59.932 --> 00:01:05.237
the training set contains values of the
inputs x as well as the target outputs y.

20
00:01:05.237 --> 00:01:09.239
So the term hidden layer refers to
the fact that in the training set,

21
00:01:09.239 --> 00:01:12.702
the true values for
these nodes in the middle are not observed.

22
00:01:12.702 --> 00:01:15.280
That is, you don't see what they
should be in the training set.

23
00:01:15.280 --> 00:01:16.640
You see what the inputs are.

24
00:01:16.640 --> 00:01:18.094
You see what the output should be.

25
00:01:18.094 --> 00:01:20.992
But the things in the hidden layer
are not seen in the training set.

26
00:01:20.992 --> 00:01:25.542
So that kind of explains the name
hidden layer; just because you

27
00:01:25.542 --> 00:01:27.050
don't see it in the training set.

28
00:01:27.050 --> 00:01:28.400
Let's introduce a bit more notation.

29
00:01:28.400 --> 00:01:34.660
Whereas previously, we were using the
vector X to denote the input features and

30
00:01:34.660 --> 00:01:36.400
alternative notation for

31
00:01:36.400 --> 00:01:41.680
the values of the input features will
be A superscript square bracket 0.

32
00:01:41.680 --> 00:01:44.934
And the term A also stands for
activations, and

33
00:01:44.934 --> 00:01:47.733
it refers to the values
that different layers

34
00:01:47.733 --> 00:01:51.651
of the neural network are passing
on to the subsequent layers.

35
00:01:51.651 --> 00:01:55.998
So the input layer passes on
the value x to the hidden layer, so

36
00:01:55.998 --> 00:02:01.110
we're going to call that activations
of the input layer A super script 0.

37
00:02:01.110 --> 00:02:05.990
The next layer, the hidden layer, will
in turn generate some set of activations,

38
00:02:05.990 --> 00:02:09.601
which I'm going to write as
A superscript square bracket 1.

39
00:02:09.601 --> 00:02:13.306
So in particular,
this first unit or this first node,

40
00:02:13.306 --> 00:02:17.824
we generate a value A superscript
square bracket 1 subscript 1.

41
00:02:17.824 --> 00:02:20.735
This second node we generate a value.

42
00:02:20.735 --> 00:02:23.311
Now we have a subscript 2 and so on.

43
00:02:23.311 --> 00:02:26.488
And so, A superscript square bracket 1,

44
00:02:26.488 --> 00:02:30.350
this is a four dimensional vector 
you want in Python

45
00:02:30.350 --> 00:02:34.707
because the 4x1 matrix, or
a 4 column vector, which looks like this.

46
00:02:34.707 --> 00:02:39.205
And it's four dimensional, because
in this case we have four nodes, or

47
00:02:39.205 --> 00:02:43.330
four units, or
four hidden units in this hidden layer.

48
00:02:43.330 --> 00:02:46.800
And then finally,
the open layer regenerates some value A2,

49
00:02:46.800 --> 00:02:48.440
which is just a real number.

50
00:02:48.440 --> 00:02:53.100
And so
y hat is going to take on the value of A2.

51
00:02:53.100 --> 00:02:57.950
So this is analogous to how in
logistic regression we have y hat equals a and

52
00:02:57.950 --> 00:03:02.560
in logistic regression which we
only had that one output layer, so

53
00:03:02.560 --> 00:03:04.500
we don't use the superscript
square brackets.

54
00:03:04.500 --> 00:03:07.916
But with our neural network,
we now going to use the superscript square

55
00:03:07.916 --> 00:03:11.653
bracket to explicitly indicate
which layer it came from.

56
00:03:11.653 --> 00:03:15.468
One funny thing about notational
conventions in neural networks

57
00:03:15.468 --> 00:03:20.194
is that this network that you've seen here
is called a two layer neural network.

58
00:03:20.194 --> 00:03:24.360
And the reason is that when we
count layers in neural networks,

59
00:03:24.360 --> 00:03:25.990
we don't count the input layer.

60
00:03:25.990 --> 00:03:30.680
So the hidden layer is layer one and
the output layer is layer two.

61
00:03:30.680 --> 00:03:34.700
In our notational convention, we're
calling the input layer layer zero, so

62
00:03:34.700 --> 00:03:37.700
technically maybe there are three
layers in this neural network.

63
00:03:37.700 --> 00:03:40.260
Because there's the input layer,
the hidden layer, and the output layer.

64
00:03:40.260 --> 00:03:44.600
But in conventional usage, if you
read research papers and elsewhere in

65
00:03:44.600 --> 00:03:48.700
the course, you see people refer to this
particular neural network as a two layer

66
00:03:48.700 --> 00:03:52.630
neural network, because we don't count
the input layer as an official layer.

67
00:03:52.630 --> 00:03:55.912
Finally, something that we'll get to
later is that the hidden layer and

68
00:03:55.912 --> 00:03:59.670
the output layers will have
parameters associated with them.

69
00:03:59.670 --> 00:04:04.560
So the hidden layer will have
associated with it parameters w and b.

70
00:04:04.560 --> 00:04:08.218
And I'm going to write superscripts
square bracket 1 to indicate that these

71
00:04:08.218 --> 00:04:12.395
are parameters associated with
layer one with the hidden layer.

72
00:04:12.395 --> 00:04:15.416
We'll see later that w will
be a 4 by 3 matrix and

73
00:04:15.416 --> 00:04:19.830
b will be a 4 by 1 vector in this example.

74
00:04:19.830 --> 00:04:22.750
Where the first coordinate four
comes from the fact that we have

75
00:04:22.750 --> 00:04:25.300
four nodes of our hidden units and
a layer, and

76
00:04:25.300 --> 00:04:28.120
three comes from the fact that
we have three input features.

77
00:04:28.120 --> 00:04:31.600
We'll talk later about
the dimensions of these matrices.

78
00:04:31.600 --> 00:04:33.540
And it might make more sense at that time.

79
00:04:33.540 --> 00:04:37.813
But in some of the output layers has
associated with it also, parameters w

80
00:04:37.813 --> 00:04:42.400
superscript square bracket 2 and
b superscript square bracket 2.

81
00:04:42.400 --> 00:04:45.747
And it turns out the dimensions
of these are 1 by 4 and 1 by 1.

82
00:04:45.747 --> 00:04:49.297
And these 1 by 4 is because the hidden
layer has four hidden units,

83
00:04:49.297 --> 00:04:51.177
the output layer has just one unit.

84
00:04:51.177 --> 00:04:56.378
But we will go over the dimension of these
matrices and vectors in a later video.

85
00:04:56.378 --> 00:04:59.839
So you've just seen what a two
layered neural network looks like.

86
00:04:59.839 --> 00:05:03.108
That is a neural network
with one hidden layer.

87
00:05:03.108 --> 00:05:04.260
In the next video,

88
00:05:04.260 --> 00:05:08.513
let's go deeper into exactly what
this neural network is computing.

89
00:05:08.513 --> 00:05:11.223
That is how this neural
network inputs x and

90
00:05:11.223 --> 00:05:14.169
goes all the way to
computing its output y hat.