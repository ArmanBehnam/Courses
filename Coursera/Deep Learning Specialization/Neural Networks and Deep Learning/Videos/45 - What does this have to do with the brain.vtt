WEBVTT

1
00:00:00.000 --> 00:00:03.050
So, what does deep learning have to do with the brain?

2
00:00:03.050 --> 00:00:05.005
At the risk of giving away the punchline,

3
00:00:05.005 --> 00:00:06.885
I would say not a whole lot.

4
00:00:06.885 --> 00:00:09.750
But let's take a quick look at why people keep making

5
00:00:09.750 --> 00:00:13.185
the analogy between deep learning and the human brain.

6
00:00:13.185 --> 00:00:15.345
When you implement a neural network,

7
00:00:15.345 --> 00:00:16.470
this is what you do,

8
00:00:16.470 --> 00:00:18.270
forward prop and back prop.

9
00:00:18.270 --> 00:00:21.930
I think because it's been difficult to convey intuitions about

10
00:00:21.930 --> 00:00:26.850
what these equations are doing really gradient descent on a very complex function,

11
00:00:26.850 --> 00:00:29.940
the analogy that is like the brain has

12
00:00:29.940 --> 00:00:34.365
become really an oversimplified explanation for what this is doing,

13
00:00:34.365 --> 00:00:40.010
but the simplicity of this makes it seductive for people to just say it publicly,

14
00:00:40.010 --> 00:00:42.275
as well as, for media to report it,

15
00:00:42.275 --> 00:00:44.825
and certainly caught the popular imagination.

16
00:00:44.825 --> 00:00:48.020
There is a very loose analogy between,

17
00:00:48.020 --> 00:00:53.315
let's say a logistic regression unit with a sigmoid activation function,

18
00:00:53.315 --> 00:00:58.130
and here's a cartoon of a single neuron in the brain.

19
00:00:58.130 --> 00:01:02.555
In this picture of a biological neuron, this neuron,

20
00:01:02.555 --> 00:01:03.905
which is a cell in your brain,

21
00:01:03.905 --> 00:01:07.945
receives electric signals from your other neurons,

22
00:01:07.945 --> 00:01:11.100
X_1, X_2, X_3, or maybe from other neurons A_1, A_2,

23
00:01:11.100 --> 00:01:14.595
A_3, does a simple thresholding computation,

24
00:01:14.595 --> 00:01:17.110
and then if this neuron fires,

25
00:01:17.110 --> 00:01:20.150
it sends a pulse of electricity down the axon,

26
00:01:20.150 --> 00:01:23.330
down this long wire perhaps to other neurons.

27
00:01:23.330 --> 00:01:27.540
So, there is a very simplistic analogy between

28
00:01:27.540 --> 00:01:34.540
a single neuron in a neural network and a biological neuron-like that shown on the right,

29
00:01:34.540 --> 00:01:37.865
but I think that today even neuroscientists have

30
00:01:37.865 --> 00:01:41.390
almost no idea what even a single neuron is doing.

31
00:01:41.390 --> 00:01:44.120
A single neuron appears to be much more complex

32
00:01:44.120 --> 00:01:47.545
than we are able to characterize with neuroscience,

33
00:01:47.545 --> 00:01:52.595
and while some of what is doing is a little bit like logistic regression,

34
00:01:52.595 --> 00:01:58.795
there's still a lot about what even a single neuron does that no human today understands.

35
00:01:58.795 --> 00:02:02.210
For example, exactly how neurons in the human brain learns,

36
00:02:02.210 --> 00:02:05.155
is still a very mysterious process.

37
00:02:05.155 --> 00:02:09.410
It's completely unclear today whether the human brain uses an algorithm,

38
00:02:09.410 --> 00:02:12.590
does anything like back propagation or gradient descent or if there's

39
00:02:12.590 --> 00:02:17.915
some fundamentally different learning principle that the human brain uses?

40
00:02:17.915 --> 00:02:20.635
So, when I think of deep learning,

41
00:02:20.635 --> 00:02:25.160
I think of it as being very good at learning very flexible functions,

42
00:02:25.160 --> 00:02:29.090
very complex functions to learn X to Y mappings,

43
00:02:29.090 --> 00:02:32.320
to learn input-output mappings in supervised learning.

44
00:02:32.320 --> 00:02:35.199
Whereas this is like the brain analogy,

45
00:02:35.199 --> 00:02:36.965
maybe that was useful ones.

46
00:02:36.965 --> 00:02:40.040
I think the field has moved to the point where

47
00:02:40.040 --> 00:02:45.135
that analogy is breaking down and I tend not to use that analogy much anymore.

48
00:02:45.135 --> 00:02:48.620
So, that's it for neural networks and the brain.

49
00:02:48.620 --> 00:02:51.830
I do think that maybe the few that computer vision has taken

50
00:02:51.830 --> 00:02:54.455
a bit more inspiration from the human brain than

51
00:02:54.455 --> 00:02:57.290
other disciplines that also apply deep learning,

52
00:02:57.290 --> 00:03:02.465
but I personally use the analogy to the human brain less than I used to.

53
00:03:02.465 --> 00:03:05.090
So, that's it for this video.

54
00:03:05.090 --> 00:03:07.940
You now know how to implement forward prop and back

55
00:03:07.940 --> 00:03:10.940
prop and gradient descent even for deep neural networks.

56
00:03:10.940 --> 00:03:12.905
Best of luck with the problem exercise,

57
00:03:12.905 --> 00:03:17.130
and I look forward to sharing more of these ideas with you in the second course.