WEBVTT

1
00:00:00.000 --> 00:00:04.740
In the last video, you saw what a single hidden layer neural network looks like.

2
00:00:04.740 --> 00:00:07.020
In this video, let's go through the details of

3
00:00:07.020 --> 00:00:10.140
exactly how this neural network computes these outputs.

4
00:00:10.140 --> 00:00:13.530
What you see is that is like logistic regression,

5
00:00:13.530 --> 00:00:15.585
the repeater a lot of times.

6
00:00:15.585 --> 00:00:19.725
Let's take a look. So, this is what a two-layer neural network looks.

7
00:00:19.725 --> 00:00:23.850
Let's go more deeply into exactly what this neural network computes.

8
00:00:23.850 --> 00:00:26.985
Now, we've said before that logistic regression,

9
00:00:26.985 --> 00:00:28.980
the circle in logistic regression,

10
00:00:28.980 --> 00:00:31.515
really represents two steps of computation rows.

11
00:00:31.515 --> 00:00:33.825
You compute z as follows, and a second,

12
00:00:33.825 --> 00:00:37.290
you compute the activation as a sigmoid function of z.

13
00:00:37.290 --> 00:00:40.715
So, a neural network just does this a lot more times.

14
00:00:40.715 --> 00:00:44.460
Let's start by focusing on just one of the nodes in the hidden layer.

15
00:00:44.460 --> 00:00:46.385
Let's look at the first node in the hidden layer.

16
00:00:46.385 --> 00:00:48.095
So, I've grayed out the other nodes for now.

17
00:00:48.095 --> 00:00:50.810
So, similar to logistic regression on the left,

18
00:00:50.810 --> 00:00:54.470
this nodes in the hidden layer does two steps of computation.

19
00:00:54.470 --> 00:00:57.905
The first step and think of as the left half of this node,

20
00:00:57.905 --> 00:01:02.570
it computes z equals w transpose x plus b,

21
00:01:02.570 --> 00:01:05.060
and the notation we'll use is,

22
00:01:05.060 --> 00:01:08.300
these are all quantities associated with the first hidden layer.

23
00:01:08.300 --> 00:01:10.720
So, that's why we have a bunch of square brackets there.

24
00:01:10.720 --> 00:01:13.610
This is the first node in the hidden layer.

25
00:01:13.610 --> 00:01:16.700
So, that's why we have the subscript one over there.

26
00:01:16.700 --> 00:01:18.005
So first, it does that,

27
00:01:18.005 --> 00:01:19.130
and then the second step,

28
00:01:19.130 --> 00:01:24.605
is it computes a_[1]_1 equals sigmoid of z_[1]_1, like so.

29
00:01:24.605 --> 00:01:26.030
So, for both z and a,

30
00:01:26.030 --> 00:01:29.690
the notational convention is that a, l, i,

31
00:01:29.690 --> 00:01:32.270
the l here in superscript square brackets,

32
00:01:32.270 --> 00:01:33.635
refers to the layer number,

33
00:01:33.635 --> 00:01:35.060
and the i subscript here,

34
00:01:35.060 --> 00:01:37.830
refers to the nodes in that layer.

35
00:01:37.830 --> 00:01:40.115
So, the node we'll be looking at is layer one,

36
00:01:40.115 --> 00:01:42.035
that is a hidden layer node one.

37
00:01:42.035 --> 00:01:45.965
So, that's why the superscripts and subscripts were both one, one.

38
00:01:45.965 --> 00:01:47.375
So, that little circle,

39
00:01:47.375 --> 00:01:49.385
that first node in the neural network,

40
00:01:49.385 --> 00:01:52.640
represents carrying out these two steps of computation.

41
00:01:52.640 --> 00:01:55.190
Now, let's look at the second node in the neural network,

42
00:01:55.190 --> 00:01:58.340
or the second node in the hidden layer of the neural network.

43
00:01:58.340 --> 00:02:01.370
Similar to the logistic regression unit on the left,

44
00:02:01.370 --> 00:02:04.850
this little circle represents two steps of computation.

45
00:02:04.850 --> 00:02:07.550
The first step is it computes z.

46
00:02:07.550 --> 00:02:08.735
This is still layer one,

47
00:02:08.735 --> 00:02:12.395
but now as a second node equals w transpose x,

48
00:02:12.395 --> 00:02:19.190
plus b_[1]_2, and then a_[1] two equals sigmoid of z_[1]_2.

49
00:02:19.190 --> 00:02:21.200
Again, feel free to pause the video if you want,

50
00:02:21.200 --> 00:02:23.840
but you can double-check that the superscript and

51
00:02:23.840 --> 00:02:28.430
subscript notation is consistent with what we have written here above in purple.

52
00:02:28.430 --> 00:02:32.845
So, we've talked through the first two hidden units in a neural network,

53
00:02:32.845 --> 00:02:36.940
having units three and four also represents some computations.

54
00:02:36.940 --> 00:02:40.205
So now, let me take this pair of equations,

55
00:02:40.205 --> 00:02:42.320
and this pair of equations,

56
00:02:42.320 --> 00:02:44.210
and let's copy them to the next slide.

57
00:02:44.210 --> 00:02:45.380
So, here's our neural network,

58
00:02:45.380 --> 00:02:46.550
and here's the first,

59
00:02:46.550 --> 00:02:49.550
and here's the second equations that we've worked out

60
00:02:49.550 --> 00:02:54.155
previously for the first and the second hidden units.

61
00:02:54.155 --> 00:02:57.650
If you then go through and write out the corresponding equations

62
00:02:57.650 --> 00:03:01.475
for the third and fourth hidden units, you get the following.

63
00:03:01.475 --> 00:03:03.635
So, let me show this notation is clear,

64
00:03:03.635 --> 00:03:06.680
this is the vector w_[1]_1,

65
00:03:06.680 --> 00:03:09.335
this is a vector transpose times x.

66
00:03:09.335 --> 00:03:12.050
So, that's what the superscript T there represents.

67
00:03:12.050 --> 00:03:13.535
It's a vector transpose.

68
00:03:13.535 --> 00:03:15.085
Now, as you might have guessed,

69
00:03:15.085 --> 00:03:17.195
if you're actually implementing a neural network,

70
00:03:17.195 --> 00:03:20.285
doing this with a for loop, seems really inefficient.

71
00:03:20.285 --> 00:03:21.530
So, what we're going to do,

72
00:03:21.530 --> 00:03:25.220
is take these four equations and vectorize.

73
00:03:25.220 --> 00:03:29.050
So, we're going to start by showing how to compute z as a vector,

74
00:03:29.050 --> 00:03:30.935
it turns out you could do it as follows.

75
00:03:30.935 --> 00:03:34.910
Let me take these w's and stack them into a matrix,

76
00:03:34.910 --> 00:03:37.980
then you have w_[1]_1 transpose,

77
00:03:37.980 --> 00:03:39.450
so that's a row vector,

78
00:03:39.450 --> 00:03:42.500
or this column vector transpose gives you a row vector, then w_[1]_2,

79
00:03:42.500 --> 00:03:48.980
transpose, w_[1]_3 transpose, w_[1]_4 transpose.

80
00:03:48.980 --> 00:03:53.150
So, by stacking those four w vectors together,

81
00:03:53.150 --> 00:03:54.575
you end up with a matrix.

82
00:03:54.575 --> 00:03:58.985
So, another way to think of this is that we have four logistic regression units there,

83
00:03:58.985 --> 00:04:01.190
and each of the logistic regression units,

84
00:04:01.190 --> 00:04:03.290
has a corresponding parameter vector,

85
00:04:03.290 --> 00:04:06.080
w. By stacking those four vectors together,

86
00:04:06.080 --> 00:04:08.930
you end up with this four by three matrix.

87
00:04:08.930 --> 00:04:14.180
So, if you then take this matrix and multiply it by your input features x1,

88
00:04:14.180 --> 00:04:18.650
x2, x3, you end up with by how matrix multiplication works.

89
00:04:18.650 --> 00:04:20.870
You end up with w_[1]_1 transpose x,

90
00:04:20.870 --> 00:04:28.775
w_2_[1] transpose x, w_3_[1] transpose x, w_4_[1] transpose x.

91
00:04:28.775 --> 00:04:31.010
Then, let's not figure the b's.

92
00:04:31.010 --> 00:04:39.450
So, we now add to this a vector b_[1]_1 one, b_[1]_2, b_[1]_3, b_[1]_4.

93
00:04:39.450 --> 00:04:40.860
So, that's basically this,

94
00:04:40.860 --> 00:04:46.500
then this is b_[1]_1, b_[1]_2, b_[1]_3, b_[1]_4.

95
00:04:46.500 --> 00:04:48.950
So, you see that each of the four rows of

96
00:04:48.950 --> 00:04:53.195
this outcome correspond exactly to each of these four rows,

97
00:04:53.195 --> 00:04:55.610
each of these four quantities that we had above.

98
00:04:55.610 --> 00:04:59.870
So, in other words, we've just shown that this thing is therefore equal to

99
00:04:59.870 --> 00:05:05.695
z_[1]_1, z_[1]_2, z_[1]_3, z_[1]_4, as defined here.

100
00:05:05.695 --> 00:05:09.830
Maybe not surprisingly, we're going to call this whole thing, the vector z_[1],

101
00:05:09.830 --> 00:05:15.095
which is taken by stacking up these individuals of z's into a column vector.

102
00:05:15.095 --> 00:05:20.300
When we're vectorizing, one of the rules of thumb that might help you navigate this,

103
00:05:20.300 --> 00:05:22.450
is that while we have different nodes in the layer,

104
00:05:22.450 --> 00:05:23.965
we'll stack them vertically.

105
00:05:23.965 --> 00:05:27.100
So, that's why we have z_[1]_1 through z_[1]_4,

106
00:05:27.100 --> 00:05:30.430
those corresponded to four different nodes in the hidden layer,

107
00:05:30.430 --> 00:05:35.540
and so we stacked these four numbers vertically to form the vector z[1].

108
00:05:35.540 --> 00:05:37.410
To use one more piece of notation,

109
00:05:37.410 --> 00:05:44.210
this four by three matrix here which we obtained by stacking the lowercase w_[1]_1,

110
00:05:44.210 --> 00:05:48.260
w_[1]_2, and so on, we're going to call this matrix W capital [1].

111
00:05:48.260 --> 00:05:52.625
Similarly, this vector, we're going to call b superscript [1] square bracket.

112
00:05:52.625 --> 00:05:54.590
So, this is a four by one vector.

113
00:05:54.590 --> 00:05:59.600
So now, we've computed z using this vector matrix notation,

114
00:05:59.600 --> 00:06:03.300
the last thing we need to do is also compute these values of a.

115
00:06:03.300 --> 00:06:08.120
So, prior won't surprise you to see that we're going to define a_[1],

116
00:06:08.120 --> 00:06:09.710
as just stacking together,

117
00:06:09.710 --> 00:06:11.330
those activation values, a [1],

118
00:06:11.330 --> 00:06:13.175
1 through a [1], 4.

119
00:06:13.175 --> 00:06:18.260
So, just take these four values and stack them together in a vector called a[1].

120
00:06:18.260 --> 00:06:21.050
This is going to be a sigmoid of z[1],

121
00:06:21.050 --> 00:06:23.360
where this now has been implementation of

122
00:06:23.360 --> 00:06:26.900
the sigmoid function that takes in the four elements of z,

123
00:06:26.900 --> 00:06:30.460
and applies the sigmoid function element-wise to it.

124
00:06:30.460 --> 00:06:31.950
So, just a recap,

125
00:06:31.950 --> 00:06:38.820
we figured out that z_[1] is equal to w_[1] times the vector x plus the vector b_[1],

126
00:06:38.820 --> 00:06:42.000
and a_[1] is sigmoid times z_[1].

127
00:06:42.000 --> 00:06:44.055
Let's just copy this to the next slide.

128
00:06:44.055 --> 00:06:48.050
What we see is that for the first layer of the neural network given an input x,

129
00:06:48.050 --> 00:06:52.070
we have that z_[1] is equal to w_[1] times x plus b_[1],

130
00:06:52.070 --> 00:06:55.740
and a_[1] is sigmoid of z_[1].

131
00:06:55.740 --> 00:06:59.060
The dimensions of this are four by one equals,

132
00:06:59.060 --> 00:07:05.255
this was a four by three matrix times a three by one vector plus a four by one vector b,

133
00:07:05.255 --> 00:07:08.480
and this is four by one same dimension as end.

134
00:07:08.480 --> 00:07:12.890
Remember, that we said x is equal to a_[0].

135
00:07:12.890 --> 00:07:16.535
Just say y hat is also equal to a two.

136
00:07:16.535 --> 00:07:21.060
If you want, you can actually take this x and replace it with a_[0],

137
00:07:21.060 --> 00:07:25.460
since a_[0] is if you want as an alias for the vector of input features, x.

138
00:07:25.460 --> 00:07:27.230
Now, through a similar derivation,

139
00:07:27.230 --> 00:07:31.580
you can figure out that the representation for the next layer can

140
00:07:31.580 --> 00:07:36.125
also be written similarly where what the output layer does is,

141
00:07:36.125 --> 00:07:37.945
it has associated with it,

142
00:07:37.945 --> 00:07:40.890
so the parameters w_[2] and b_[2].

143
00:07:40.890 --> 00:07:44.600
So, w_[2] in this case is going to be a one by four matrix,

144
00:07:44.600 --> 00:07:47.240
and b_[2] is just a real number as one by on.

145
00:07:47.240 --> 00:07:52.130
So, z_[2] is going to be a real number we'll write as a one by one matrix.

146
00:07:52.130 --> 00:07:56.090
Is going to be a one by four thing times a was four by one,

147
00:07:56.090 --> 00:07:57.770
plus b_[2] as one by one,

148
00:07:57.770 --> 00:07:59.805
so this gives you just a real number.

149
00:07:59.805 --> 00:08:02.900
If you think of this last upper unit as just being

150
00:08:02.900 --> 00:08:07.295
analogous to logistic regression which have parameters w and b,

151
00:08:07.295 --> 00:08:12.560
w really plays an analogous role to w_[2] transpose,

152
00:08:12.560 --> 00:08:16.790
or w_[2] is really W transpose and b is equal to b_[2].

153
00:08:16.790 --> 00:08:21.840
I said we want to cover up the left of this network and ignore all that for now,

154
00:08:21.840 --> 00:08:26.300
then this last upper unit is a lot like logistic regression,

155
00:08:26.300 --> 00:08:29.375
except that instead of writing the parameters as w and b,

156
00:08:29.375 --> 00:08:32.700
we're writing them as w_[2] and b_[2],

157
00:08:32.700 --> 00:08:35.535
with dimensions one by four and one by one.

158
00:08:35.535 --> 00:08:37.365
So, just a recap.

159
00:08:37.365 --> 00:08:41.690
For logistic regression, to implement the output or to implement prediction,

160
00:08:41.690 --> 00:08:44.945
you compute z equals w transpose x plus b,

161
00:08:44.945 --> 00:08:48.200
and a or y hat equals a,

162
00:08:48.200 --> 00:08:50.705
equals sigmoid of z.

163
00:08:50.705 --> 00:08:54.330
When you have a neural network with one hidden layer,

164
00:08:54.330 --> 00:08:55.530
what you need to implement,

165
00:08:55.530 --> 00:08:59.085
is to computer this output is just these four equations.

166
00:08:59.085 --> 00:09:03.020
You can think of this as a vectorized implementation of computing

167
00:09:03.020 --> 00:09:08.015
the output of first these for logistic regression units in the hidden layer,

168
00:09:08.015 --> 00:09:09.350
that's what this does, and

169
00:09:09.350 --> 00:09:13.730
then this logistic regression in the output layer which is what this does.

170
00:09:13.730 --> 00:09:15.680
I hope this description made sense,

171
00:09:15.680 --> 00:09:19.240
but the takeaway is to compute the output of this neural network,

172
00:09:19.240 --> 00:09:21.785
all you need is those four lines of code.

173
00:09:21.785 --> 00:09:25.280
So now, you've seen how given a single input feature,

174
00:09:25.280 --> 00:09:27.755
vector a, you can with four lines of code,

175
00:09:27.755 --> 00:09:30.185
compute the output of this neural network.

176
00:09:30.185 --> 00:09:32.600
Similar to what we did for logistic regression,

177
00:09:32.600 --> 00:09:36.635
we'll also want to vectorize across multiple training examples.

178
00:09:36.635 --> 00:09:41.690
We'll see that by stacking up training examples in different columns in the matrix,

179
00:09:41.690 --> 00:09:44.615
with just slight modification to this, you also,

180
00:09:44.615 --> 00:09:46.610
similar to what you saw in this regression,

181
00:09:46.610 --> 00:09:50.170
be able to compute the output of this neural network,

182
00:09:50.170 --> 00:09:52.030
not just a one example at a time,

183
00:09:52.030 --> 00:09:55.010
prolong your, say your entire training set at a time.

184
00:09:55.010 --> 00:09:57.940
So, let's see the details of that in the next video.