WEBVTT

1
00:00:00.000 --> 00:00:01.619
When you change your neural network,

2
00:00:01.619 --> 00:00:03.955
it's important to initialize
the weights randomly.

3
00:00:03.955 --> 00:00:08.426
For logistic regression, it was okay
to initialize the weights to zero.

4
00:00:08.426 --> 00:00:12.258
But for a neural network of initialize
the weights to parameters to all zero and

5
00:00:12.258 --> 00:00:14.706
then applied gradient descent,
it won't work.

6
00:00:14.706 --> 00:00:15.289
Let's see why.

7
00:00:15.289 --> 00:00:20.173
So you have here two input features, so

8
00:00:20.173 --> 00:00:25.953
n0=2, and two hidden units, so n1=2.

9
00:00:25.953 --> 00:00:31.547
And so the matrix associated
with the hidden layer,

10
00:00:31.547 --> 00:00:35.373
w 1, is going to be two-by-two.

11
00:00:35.373 --> 00:00:41.230
Let's say that you initialize it to
all 0s, so 0 0 0 0, two-by-two matrix.

12
00:00:41.230 --> 00:00:45.531
And let's say B1 is also equal to 0 0.

13
00:00:45.531 --> 00:00:50.788
It turns out initializing the bias
terms b to 0 is actually okay,

14
00:00:50.788 --> 00:00:54.240
but initializing w to all 0s is a problem.

15
00:00:54.240 --> 00:00:59.625
So the problem with this
formalization is that for

16
00:00:59.625 --> 00:01:05.522
any example you give it,
you'll have that a1,1 and

17
00:01:05.522 --> 00:01:09.253
a1,2, will be equal, right?

18
00:01:09.253 --> 00:01:12.613
So this activation and
this activation will be the same,

19
00:01:12.613 --> 00:01:17.170
because both of these hidden units
are computing exactly the same function.

20
00:01:17.170 --> 00:01:21.810
And then,
when you compute backpropagation,

21
00:01:21.810 --> 00:01:24.478
it turns out that dz11 and

22
00:01:24.478 --> 00:01:30.165
dz12 will also be the same
colored by symmetry, right?

23
00:01:30.165 --> 00:01:33.720
Both of these hidden units
will initialize the same way.

24
00:01:33.720 --> 00:01:36.080
Technically, for what I'm saying,

25
00:01:36.080 --> 00:01:39.851
I'm assuming that the outgoing weights or
also identical.

26
00:01:39.851 --> 00:01:45.122
So that's w2 is equal to 0 0.

27
00:01:45.122 --> 00:01:48.691
But if you initialize
the neural network this way,

28
00:01:48.691 --> 00:01:53.590
then this hidden unit and
this hidden unit are completely identical.

29
00:01:53.590 --> 00:01:57.011
Sometimes you say they're
completely symmetric,

30
00:01:57.011 --> 00:02:01.687
which just means that they're
completing exactly the same function.

31
00:02:01.687 --> 00:02:03.765
And by kind of a proof by induction,

32
00:02:03.765 --> 00:02:08.064
it turns out that after every single
iteration of training your two hidden

33
00:02:08.064 --> 00:02:11.272
units are still computing
exactly the same function.

34
00:02:11.272 --> 00:02:17.521
Since [INAUDIBLE] show that dw will
be a matrix that looks like this.

35
00:02:17.521 --> 00:02:20.681
Where every row takes on the same value.

36
00:02:20.681 --> 00:02:23.318
So we perform a weight update.

37
00:02:23.318 --> 00:02:30.163
So when you perform a weight update,
w1 gets updated as w1- alpha times dw.

38
00:02:30.163 --> 00:02:33.740
You find that w1, after every iteration,

39
00:02:33.740 --> 00:02:37.616
will have the first row
equal to the second row.

40
00:02:37.616 --> 00:02:41.487
So it's possible to construct
a proof by induction that if you

41
00:02:41.487 --> 00:02:44.688
initialize all the ways,
all the values of w to 0,

42
00:02:44.688 --> 00:02:49.164
then because both hidden units start
off computing the same function.

43
00:02:49.164 --> 00:02:53.541
And both hidden the units have
the same influence on the output unit,

44
00:02:53.541 --> 00:02:57.542
then after one iteration,
that same statement is still true,

45
00:02:57.542 --> 00:03:00.273
the two hidden units are still symmetric.

46
00:03:00.273 --> 00:03:04.507
And therefore, by induction, after two
iterations, three iterations and so on,

47
00:03:04.507 --> 00:03:07.013
no matter how long you
train your neural network,

48
00:03:07.013 --> 00:03:10.373
both hidden units are still
computing exactly the same function.

49
00:03:10.373 --> 00:03:15.212
And so in this case, there's really no
point to having more than one hidden unit.

50
00:03:15.212 --> 00:03:17.692
Because they are all
computing the same thing.

51
00:03:17.692 --> 00:03:22.378
And of course, for larger neural networks,
let's say of three features and

52
00:03:22.378 --> 00:03:24.972
maybe a very large number of hidden units,

53
00:03:24.972 --> 00:03:29.239
a similar argument works to show that
with a neural network like this.

54
00:03:29.239 --> 00:03:34.107
[INAUDIBLE] drawing all the edges,
if you initialize the weights to zero,

55
00:03:34.107 --> 00:03:37.103
then all of your hidden
units are symmetric.

56
00:03:37.103 --> 00:03:40.603
And no matter how long
you're upgrading the center,

57
00:03:40.603 --> 00:03:44.037
all continue to compute
exactly the same function.

58
00:03:44.037 --> 00:03:48.785
So that's not helpful,
because you want the different

59
00:03:48.785 --> 00:03:52.835
hidden units to compute
different functions.

60
00:03:52.835 --> 00:03:57.748
The solution to this is to
initialize your parameters randomly.

61
00:03:57.748 --> 00:03:58.677
So here's what you do.

62
00:03:58.677 --> 00:04:04.053
You can set w1 = np.random.randn.

63
00:04:04.053 --> 00:04:07.037
This generates a gaussian
random variable (2,2).

64
00:04:07.037 --> 00:04:12.358
And then usually, you multiply this
by very small number, such as 0.01.

65
00:04:12.358 --> 00:04:14.951
So you initialize it to
very small random values.

66
00:04:14.951 --> 00:04:20.590
And then b, it turns out that b
does not have the symmetry problem,

67
00:04:20.590 --> 00:04:24.735
what's called the symmetry
breaking problem.

68
00:04:24.735 --> 00:04:29.370
So it's okay to initialize
b to just zeros.

69
00:04:29.370 --> 00:04:32.166
Because so
long as w is initialized randomly,

70
00:04:32.166 --> 00:04:36.769
you start off with the different hidden
units computing different things.

71
00:04:36.769 --> 00:04:40.912
And so you no longer have this
symmetry breaking problem.

72
00:04:40.912 --> 00:04:43.795
And then similarly, for w2,
you're going to initialize that randomly.

73
00:04:43.795 --> 00:04:48.858
And b2, you can initialize that to 0.

74
00:04:48.858 --> 00:04:55.321
So you might be wondering, where did this
constant come from and why is it 0.01?

75
00:04:55.321 --> 00:04:58.478
Why not put the number 100 or 1000?

76
00:04:58.478 --> 00:05:02.313
Turns out that we usually
prefer to initialize

77
00:05:02.313 --> 00:05:05.763
the weights to very small random values.

78
00:05:05.763 --> 00:05:10.443
Because if you are using a tanh or
sigmoid activation function, or

79
00:05:10.443 --> 00:05:14.047
the other sigmoid,
even just at the output layer.

80
00:05:14.047 --> 00:05:17.922
If the weights are too large,

81
00:05:17.922 --> 00:05:23.967
then when you compute
the activation values,

82
00:05:23.967 --> 00:05:28.621
remember that z[1]=w1 x + b.

83
00:05:28.621 --> 00:05:34.094
And then a1 is the activation
function applied to z1.

84
00:05:34.094 --> 00:05:39.097
So if w is very big,
z will be very, or at least some

85
00:05:39.097 --> 00:05:44.235
values of z will be either very large or
very small.

86
00:05:44.235 --> 00:05:49.789
And so in that case, you're more likely
to end up at these fat parts of the tanh

87
00:05:49.789 --> 00:05:55.699
function or the sigmoid function, where
the slope or the gradient is very small.

88
00:05:55.699 --> 00:05:58.302
Meaning that gradient
descent will be very slow.

89
00:05:58.302 --> 00:05:59.730
So learning was very slow.

90
00:05:59.730 --> 00:06:04.133
So just a recap, if w is too large,
you're more likely to end up

91
00:06:04.133 --> 00:06:08.633
even at the very start of training,
with very large values of z.

92
00:06:08.633 --> 00:06:13.525
Which causes your tanh or your sigmoid
activation function to be saturated,

93
00:06:13.525 --> 00:06:15.418
thus slowing down learning.

94
00:06:15.418 --> 00:06:17.231
If you don't have any sigmoid or

95
00:06:17.231 --> 00:06:22.149
tanh activation functions throughout your
neural network, this is less of an issue.

96
00:06:22.149 --> 00:06:26.506
But if you're doing binary classification,
and your output unit is a sigmoid

97
00:06:26.506 --> 00:06:30.806
function, then you just don't want
the initial parameters to be too large.

98
00:06:30.806 --> 00:06:35.435
So that's why multiplying by 0.01 would
be something reasonable to try, or

99
00:06:35.435 --> 00:06:36.872
any other small number.

100
00:06:36.872 --> 00:06:38.536
And same for w2, right?

101
00:06:38.536 --> 00:06:44.295
This can be random.random.

102
00:06:44.295 --> 00:06:49.545
I guess this would be 1 by 2
in this example, times 0.01.

103
00:06:49.545 --> 00:06:51.404
Missing an s there.

104
00:06:51.404 --> 00:07:00.085
So finally, it turns out that sometimes
they can be better constants than 0.01.

105
00:07:00.085 --> 00:07:04.304
When you're training a neural
network with just one hidden layer,

106
00:07:04.304 --> 00:07:09.129
it is a relatively shallow neural network,
without too many hidden layers.

107
00:07:09.129 --> 00:07:12.392
Set it to 0.01 will probably work okay.

108
00:07:12.392 --> 00:07:15.705
But when you're training a very
very deep neural network,

109
00:07:15.705 --> 00:07:19.294
then you might want to pick
a different constant than 0.01.

110
00:07:19.294 --> 00:07:23.642
And in next week's material,
we'll talk a little bit about how and

111
00:07:23.642 --> 00:07:27.925
when you might want to choose
a different constant than 0.01.

112
00:07:27.925 --> 00:07:32.008
But either way, it will usually end
up being a relatively small number.

113
00:07:32.008 --> 00:07:34.584
So that's it for this week's videos.

114
00:07:34.584 --> 00:07:38.348
You now know how to set up a neural
network of a hidden layer,

115
00:07:38.348 --> 00:07:42.430
initialize the parameters,
make predictions using.

116
00:07:42.430 --> 00:07:45.445
As well as compute derivatives and
implement gradient descent,

117
00:07:45.445 --> 00:07:46.275
using backprop.

118
00:07:46.275 --> 00:07:48.654
So that,
you should be able to do the quizzes,

119
00:07:48.654 --> 00:07:51.166
as well as this week's
programming exercises.

120
00:07:51.166 --> 00:07:52.143
Best of luck with that.

121
00:07:52.143 --> 00:07:54.802
I hope you have fun with
the problem exercise, and

122
00:07:54.802 --> 00:07:57.728
look forward to seeing you
in the week four materials.