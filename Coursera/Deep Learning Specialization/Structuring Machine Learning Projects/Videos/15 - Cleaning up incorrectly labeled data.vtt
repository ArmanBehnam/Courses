WEBVTT

1
00:00:00.000 --> 00:00:06.985
The data for your supervised learning problem comprises input X and output labels Y.

2
00:00:06.985 --> 00:00:09.730
What if you going through your data and you find that some of

3
00:00:09.730 --> 00:00:12.240
these output labels Y are incorrect,

4
00:00:12.240 --> 00:00:14.740
you have data which is incorrectly labeled?

5
00:00:14.740 --> 00:00:19.540
Is it worth your while to go in to fix up some of these labels? Let's take a look.

6
00:00:19.540 --> 00:00:21.640
In the cat classification problem,

7
00:00:21.640 --> 00:00:25.295
Y equals one for cats and zero for non cats.

8
00:00:25.295 --> 00:00:28.560
So, let's say you're looking through some data and that's a cat,

9
00:00:28.560 --> 00:00:30.400
that's not a cat, that's a cat,

10
00:00:30.400 --> 00:00:33.275
that's a cat, that's not a cat, that's at a cat.

11
00:00:33.275 --> 00:00:35.480
No, wait. That's actually not a cat.

12
00:00:35.480 --> 00:00:41.310
So this is an example with an incorrect label.

13
00:00:41.310 --> 00:00:43.900
So I've used the term, mislabeled examples,

14
00:00:43.900 --> 00:00:48.235
to refer to if your learning algorithm outputs the wrong value of Y.

15
00:00:48.235 --> 00:00:50.800
But I'm going to say, incorrectly labeled examples,

16
00:00:50.800 --> 00:00:53.590
to refer to if in the data

17
00:00:53.590 --> 00:00:56.740
set you have in the training set or the dev set or the test set,

18
00:00:56.740 --> 00:00:59.320
the label for Y, whatever a human label

19
00:00:59.320 --> 00:01:02.228
assigned to this piece of data, is actually incorrect.

20
00:01:02.228 --> 00:01:06.430
And that's actually a dog so that Y really should have been zero.

21
00:01:06.430 --> 00:01:10.115
But maybe the labeler got that one wrong.

22
00:01:10.115 --> 00:01:14.755
So if you find that your data has some incorrectly labeled examples,

23
00:01:14.755 --> 00:01:16.440
what should you do?

24
00:01:16.440 --> 00:01:21.177
Well, first, let's consider the training set.

25
00:01:21.177 --> 00:01:24.170
It turns out that deep learning algorithms

26
00:01:24.170 --> 00:01:27.610
are quite robust to random errors in the training set.

27
00:01:27.610 --> 00:01:32.450
So long as your errors or your incorrectly labeled examples,

28
00:01:32.450 --> 00:01:35.420
so long as those errors are not too far from random,

29
00:01:35.420 --> 00:01:41.495
maybe sometimes the labeler just wasn't paying attention or they accidentally,

30
00:01:41.495 --> 00:01:44.205
randomly hit the wrong key on the keyboard.

31
00:01:44.205 --> 00:01:46.400
If the errors are reasonably random,

32
00:01:46.400 --> 00:01:49.130
then it's probably okay to just leave

33
00:01:49.130 --> 00:01:53.065
the errors as they are and not spend too much time fixing them.

34
00:01:53.065 --> 00:01:55.040
There's certainly no harm to going into

35
00:01:55.040 --> 00:01:57.920
your training set and be examining the labels and fixing them.

36
00:01:57.920 --> 00:02:02.285
Sometimes that is worth doing but your effort might be okay even if you don't.

37
00:02:02.285 --> 00:02:05.390
So long as the total data set size is big

38
00:02:05.390 --> 00:02:10.470
enough and the actual percentage of errors is maybe not too high.

39
00:02:10.470 --> 00:02:15.230
So I see a lot of machine learning algorithms that trained even when we know that

40
00:02:15.230 --> 00:02:21.115
there are few X mistakes in the training set labels and usually works okay.

41
00:02:21.115 --> 00:02:24.770
There is one caveat to this which is

42
00:02:24.770 --> 00:02:28.500
that deep learning algorithms are robust to random errors.

43
00:02:28.500 --> 00:02:34.955
They are less robust to systematic errors.

44
00:02:34.955 --> 00:02:40.555
So for example, if your labeler consistently labels white dogs as cats,

45
00:02:40.555 --> 00:02:43.390
then that is a problem because your classifier will

46
00:02:43.390 --> 00:02:46.970
learn to classify all white colored dogs as cats.

47
00:02:46.970 --> 00:02:50.260
But random errors or near random errors are

48
00:02:50.260 --> 00:02:54.575
usually not too bad for most deep learning algorithms.

49
00:02:54.575 --> 00:02:57.730
Now, this discussion has focused on what to do

50
00:02:57.730 --> 00:03:00.905
about incorrectly labeled examples in your training set.

51
00:03:00.905 --> 00:03:04.895
How about incorrectly labeled examples in your dev set or test set?

52
00:03:04.895 --> 00:03:07.195
If you're worried about the impact of

53
00:03:07.195 --> 00:03:10.445
incorrectly labeled examples on your dev set or test set,

54
00:03:10.445 --> 00:03:14.440
what they recommend you do is during error analysis to add

55
00:03:14.440 --> 00:03:17.920
one extra column so that you can also count up

56
00:03:17.920 --> 00:03:22.080
the number of examples where the label Y was incorrect.

57
00:03:22.080 --> 00:03:29.075
So for example, maybe when you count up the impact on a 100 mislabeled dev set examples,

58
00:03:29.075 --> 00:03:31.300
so you're going to find a 100 examples where

59
00:03:31.300 --> 00:03:35.595
your classifier's output disagrees with the label in your dev set.

60
00:03:35.595 --> 00:03:38.440
And sometimes for a few of those examples,

61
00:03:38.440 --> 00:03:42.175
your classifier disagrees with the label because the label was wrong,

62
00:03:42.175 --> 00:03:44.220
rather than because your classifier was wrong.

63
00:03:44.220 --> 00:03:46.030
So maybe in this example,

64
00:03:46.030 --> 00:03:49.415
you find that the labeler missed a cat in the background.

65
00:03:49.415 --> 00:03:55.810
So put the check mark there to signify that example 98 had an incorrect label.

66
00:03:55.810 --> 00:03:57.600
And maybe for this one,

67
00:03:57.600 --> 00:04:01.845
the picture is actually a picture of a drawing of a cat rather than a real cat.

68
00:04:01.845 --> 00:04:06.865
Maybe you want the labeler to have labeled that Y equals zero rather than Y equals one.

69
00:04:06.865 --> 00:04:09.940
And so put another check mark there.

70
00:04:09.940 --> 00:04:12.585
And just as you count up the percent of

71
00:04:12.585 --> 00:04:15.670
errors due to other categories like we saw in the previous video,

72
00:04:15.670 --> 00:04:20.800
you'd also count up the fraction of percentage of errors due to incorrect labels.

73
00:04:20.800 --> 00:04:23.320
Where the Y value in your dev set was

74
00:04:23.320 --> 00:04:25.940
wrong and that accounted for why your learning algorithm

75
00:04:25.940 --> 00:04:32.140
made a prediction that differed from what the label on your data says.

76
00:04:32.140 --> 00:04:33.835
So the question now is,

77
00:04:33.835 --> 00:04:41.045
is it worthwhile going in to try to fix up this 6% of incorrectly labeled examples.

78
00:04:41.045 --> 00:04:43.060
My advice is, if it makes

79
00:04:43.060 --> 00:04:47.560
a significant difference to your ability to evaluate algorithms on your dev set,

80
00:04:47.560 --> 00:04:50.740
then go ahead and spend the time to fix incorrect labels.

81
00:04:50.740 --> 00:04:52.900
But if it doesn't make a significant difference to

82
00:04:52.900 --> 00:04:56.125
your ability to use the dev set to evaluate cost buyers,

83
00:04:56.125 --> 00:04:58.810
then it might not be the best use of your time.

84
00:04:58.810 --> 00:05:02.075
Let me show you an example that illustrates what I mean by this.

85
00:05:02.075 --> 00:05:05.620
So, three numbers I recommend you look at to try to decide if

86
00:05:05.620 --> 00:05:09.305
it's worth going in and reducing the number of mislabeled examples are the following.

87
00:05:09.305 --> 00:05:12.755
I recommend you look at the overall dev set error.

88
00:05:12.755 --> 00:05:16.570
And so in the example we had from the previous video,

89
00:05:16.570 --> 00:05:20.680
we said that maybe our system has 90% overall accuracy.

90
00:05:20.680 --> 00:05:22.995
So 10% error.

91
00:05:22.995 --> 00:05:26.560
Then you should look at the number of errors or

92
00:05:26.560 --> 00:05:30.495
the percentage of errors that are due to incorrect labels.

93
00:05:30.495 --> 00:05:32.695
So it looks like in this case,

94
00:05:32.695 --> 00:05:35.730
6% of the errors are due to incorrect labels.

95
00:05:35.730 --> 00:05:40.990
So 6% of 10% is 0.6%.

96
00:05:40.990 --> 00:05:45.600
And then you should look at errors due to all other causes.

97
00:05:45.600 --> 00:05:48.280
So if you made 10% error on your dev set

98
00:05:48.280 --> 00:05:51.580
and 0.6% of those are because the labels is wrong,

99
00:05:51.580 --> 00:05:54.430
then the remainder, 9.4% of them,

100
00:05:54.430 --> 00:05:58.231
are due to other causes such as misrecognizing dogs being cats,

101
00:05:58.231 --> 00:06:01.420
great cats and their images.

102
00:06:01.420 --> 00:06:08.380
So in this case, I would say there's 9.4% worth of error that you could focus on fixing,

103
00:06:08.380 --> 00:06:12.370
whereas the errors due to incorrect labels is

104
00:06:12.370 --> 00:06:16.360
a relatively small fraction of the overall set of errors.

105
00:06:16.360 --> 00:06:17.860
So by all means,

106
00:06:17.860 --> 00:06:20.605
go in and fix these incorrect labels if you want

107
00:06:20.605 --> 00:06:24.455
but it's maybe not the most important thing to do right now.

108
00:06:24.455 --> 00:06:26.830
Now, let's take another example.

109
00:06:26.830 --> 00:06:30.040
Suppose you've made a lot more progress on your learning problem.

110
00:06:30.040 --> 00:06:31.896
So instead of 10% error,

111
00:06:31.896 --> 00:06:35.065
let's say you brought the errors down to 2%,

112
00:06:35.065 --> 00:06:43.300
but still 0.6% of your overall errors are due to incorrect labels.

113
00:06:43.300 --> 00:06:47.755
So now, if you want to examine a set of mislabeled dev set images,

114
00:06:47.755 --> 00:06:52.600
set that comes from just 2% of dev set data you're mislabeling,

115
00:06:52.600 --> 00:06:56.065
then a very large fraction of them,

116
00:06:56.065 --> 00:06:59.315
0.6 divided by 2%,

117
00:06:59.315 --> 00:07:05.235
so that is actually 30% rather than 6% of your labels.

118
00:07:05.235 --> 00:07:09.145
Your incorrect examples are actually due to incorrectly label examples.

119
00:07:09.145 --> 00:07:12.445
And so errors due to other causes are now 1.4%.

120
00:07:12.445 --> 00:07:16.885
When such a high fraction of

121
00:07:16.885 --> 00:07:23.640
your mistakes as measured on your dev set due to incorrect labels,

122
00:07:23.640 --> 00:07:30.825
then it maybe seems much more worthwhile to fix up the incorrect labels in your dev set.

123
00:07:30.825 --> 00:07:32.800
And if you remember the goal of the dev set,

124
00:07:32.800 --> 00:07:34.890
the main purpose of the dev set is,

125
00:07:34.890 --> 00:07:39.520
you want to really use it to help you select between two classifiers A and B.

126
00:07:39.520 --> 00:07:42.030
So you're trying out two classifiers A and B,

127
00:07:42.030 --> 00:07:49.740
and one has 2.1% error and the other has 1.9% error on your dev set.

128
00:07:49.740 --> 00:07:52.370
But you don't trust your dev set anymore to be

129
00:07:52.370 --> 00:07:55.020
correctly telling you whether this classifier is

130
00:07:55.020 --> 00:07:57.440
actually better than this because your

131
00:07:57.440 --> 00:08:02.215
0.6% of these mistakes are due to incorrect labels.

132
00:08:02.215 --> 00:08:06.720
Then there's a good reason to go in and fix the incorrect labels in your dev set.

133
00:08:06.720 --> 00:08:10.770
Because in this example on the right is just having a very large impact

134
00:08:10.770 --> 00:08:14.784
on the overall assessment of the errors of the algorithm,

135
00:08:14.784 --> 00:08:17.385
whereas example on the left, the percentage impact is

136
00:08:17.385 --> 00:08:21.055
having on your algorithm is still smaller.

137
00:08:21.055 --> 00:08:24.040
Now, if you decide to go into a dev set and

138
00:08:24.040 --> 00:08:28.285
manually re-examine the labels and try to fix up some of the labels,

139
00:08:28.285 --> 00:08:33.505
here are a few additional guidelines or principles to consider.

140
00:08:33.505 --> 00:08:36.850
First, I would encourage you to apply

141
00:08:36.850 --> 00:08:41.320
whatever process you apply to both your dev and test sets at the same time.

142
00:08:41.320 --> 00:08:44.110
We've talk previously about why you want

143
00:08:44.110 --> 00:08:47.335
to dev and test sets to come from the same distribution.

144
00:08:47.335 --> 00:08:50.890
The dev set is tagging you into target and when you hit it,

145
00:08:50.890 --> 00:08:53.320
you want that to generalize to the test set.

146
00:08:53.320 --> 00:08:55.540
So your team really works more

147
00:08:55.540 --> 00:08:59.195
efficiently to dev and test sets come from the same distribution.

148
00:08:59.195 --> 00:09:01.620
So if you're going in to fix something on the dev set,

149
00:09:01.620 --> 00:09:04.290
I would apply the same process to the test set to make sure

150
00:09:04.290 --> 00:09:07.170
that they continue to come from the same distribution.

151
00:09:07.170 --> 00:09:10.690
So we hire someone to examine the labels more carefully.

152
00:09:10.690 --> 00:09:13.125
Do that for both your dev and test sets.

153
00:09:13.125 --> 00:09:16.255
Second, I would urge you to consider examining

154
00:09:16.255 --> 00:09:20.920
examples your algorithm got right as well as ones it got wrong.

155
00:09:20.920 --> 00:09:23.400
It is easy to look at the examples your algorithm

156
00:09:23.400 --> 00:09:26.875
got wrong and just see if any of those need to be fixed.

157
00:09:26.875 --> 00:09:30.463
But it's possible that there are some examples that you haven't got right,

158
00:09:30.463 --> 00:09:32.000
that should also be fixed.

159
00:09:32.000 --> 00:09:34.560
And if you only fix ones that your algorithms got wrong,

160
00:09:34.560 --> 00:09:38.905
you end up with more bias estimates of the error of your algorithm.

161
00:09:38.905 --> 00:09:42.450
It gives your algorithm a little bit of an unfair advantage.

162
00:09:42.450 --> 00:09:46.560
We just try to double check what it got wrong but you don't also

163
00:09:46.560 --> 00:09:50.910
double check what it got right because it might have gotten something right,

164
00:09:50.910 --> 00:09:54.645
that it was just lucky on fixing the label would

165
00:09:54.645 --> 00:09:59.160
cause it to go from being right to being wrong, on that example.

166
00:09:59.160 --> 00:10:01.995
The second bullet isn't always easy to do,

167
00:10:01.995 --> 00:10:03.865
so it's not always done.

168
00:10:03.865 --> 00:10:08.055
The reason it's not always done is because if you classifier's very accurate,

169
00:10:08.055 --> 00:10:11.940
then it's getting fewer things wrong than right.

170
00:10:11.940 --> 00:10:15.120
So if your classifier has 98% accuracy,

171
00:10:15.120 --> 00:10:19.660
then it's getting 2% of things wrong and 98% of things right.

172
00:10:19.660 --> 00:10:24.365
So it's much easier to examine and validate the labels on 2% of

173
00:10:24.365 --> 00:10:30.345
the data and it takes much longer to validate labels on 98% of the data,

174
00:10:30.345 --> 00:10:31.840
so this isn't always done.

175
00:10:31.840 --> 00:10:34.365
That's just something to consider.

176
00:10:34.365 --> 00:10:41.275
Finally, if you go into a dev and test data to correct some of the labels there,

177
00:10:41.275 --> 00:10:46.410
you may or may not decide to go and apply the same process for the training set.

178
00:10:46.410 --> 00:10:48.600
Remember we said that at this other video that it's actually

179
00:10:48.600 --> 00:10:51.485
less important to correct the labels in your training set.

180
00:10:51.485 --> 00:10:54.750
And it's quite possible you decide to just correct the labels in your dev and

181
00:10:54.750 --> 00:10:58.170
test set which are also often smaller

182
00:10:58.170 --> 00:11:01.710
than a training set and you might not invest all that extra effort

183
00:11:01.710 --> 00:11:06.025
needed to correct the labels in a much larger training set.

184
00:11:06.025 --> 00:11:07.365
This is actually okay.

185
00:11:07.365 --> 00:11:11.775
We'll talk later this week about some processes

186
00:11:11.775 --> 00:11:14.070
for handling when your training data is

187
00:11:14.070 --> 00:11:17.435
different in distribution than you dev and test data.

188
00:11:17.435 --> 00:11:20.190
Learning algorithms are quite robust to that.

189
00:11:20.190 --> 00:11:25.175
It's super important that your dev and test sets come from the same distribution.

190
00:11:25.175 --> 00:11:28.530
But if your training set comes from a slightly different distribution,

191
00:11:28.530 --> 00:11:31.170
often that's a pretty reasonable thing to do.

192
00:11:31.170 --> 00:11:34.060
I will talk more about how to handle this later this week.

193
00:11:34.060 --> 00:11:37.705
So I'd like to wrap up with just a couple of pieces of advice.

194
00:11:37.705 --> 00:11:41.265
First, deep learning researchers sometimes like to say things like,

195
00:11:41.265 --> 00:11:42.920
"I just fed the data to the algorithm.

196
00:11:42.920 --> 00:11:44.897
I trained in and it worked."

197
00:11:44.897 --> 00:11:48.035
There is a lot of truth to that in the deep learning error.

198
00:11:48.035 --> 00:11:51.000
There is more of feeding data in algorithm and just training

199
00:11:51.000 --> 00:11:54.685
it and doing less hand engineering and using less human insight.

200
00:11:54.685 --> 00:11:57.780
But I think that in building practical systems,

201
00:11:57.780 --> 00:12:01.965
often there's also more manual error analysis and more human insight

202
00:12:01.965 --> 00:12:07.270
that goes into the systems than sometimes deep learning researchers like to acknowledge.

203
00:12:07.270 --> 00:12:10.580
Second is that somehow I've seen some engineers and

204
00:12:10.580 --> 00:12:14.415
researchers be reluctant to manually look at the examples.

205
00:12:14.415 --> 00:12:16.510
Maybe it's not the most interesting thing to do,

206
00:12:16.510 --> 00:12:17.745
to sit down and look at

207
00:12:17.745 --> 00:12:21.465
a 100 or a couple hundred examples to counter the number of errors.

208
00:12:21.465 --> 00:12:23.865
But this is something that I so do myself.

209
00:12:23.865 --> 00:12:25.485
When I'm leading a machine learning team and I

210
00:12:25.485 --> 00:12:27.240
want to understand what mistakes it is making,

211
00:12:27.240 --> 00:12:29.430
I would actually go in and look at the data myself and

212
00:12:29.430 --> 00:12:31.820
try to counter the fraction of errors.

213
00:12:31.820 --> 00:12:35.535
And I think that because these minutes or maybe a small number of

214
00:12:35.535 --> 00:12:40.005
hours of counting data can really help you prioritize where to go next.

215
00:12:40.005 --> 00:12:42.720
I find this a very good use of your time and I urge

216
00:12:42.720 --> 00:12:45.480
you to consider doing it if those machines are in

217
00:12:45.480 --> 00:12:47.385
your system and you're trying to decide

218
00:12:47.385 --> 00:12:51.585
what ideas or what directions to prioritize things.

219
00:12:51.585 --> 00:12:55.620
So that's it for the error analysis process.

220
00:12:55.620 --> 00:13:00.060
In the next video, I want to share a view of some thoughts on how error analysis

221
00:13:00.060 --> 00:13:05.000
fits in to how you might go about starting out on the new machine learning project.