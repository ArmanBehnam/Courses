WEBVTT

1
00:00:00.000 --> 00:00:02.335
Other than convolutional layers,

2
00:00:02.335 --> 00:00:07.130
ConvNets often also use pooling layers to reduce the size of the representation,

3
00:00:07.130 --> 00:00:08.510
to speed the computation,

4
00:00:08.510 --> 00:00:12.020
as well as make some of the features that detects a bit more robust.

5
00:00:12.020 --> 00:00:16.390
Let's take a look. Let's go through an example of pooling,

6
00:00:16.390 --> 00:00:20.205
and then we'll talk about why you might want to do this.

7
00:00:20.205 --> 00:00:24.300
Suppose you have a four by four input,

8
00:00:24.300 --> 00:00:28.675
and you want to apply a type of pooling called max pooling.

9
00:00:28.675 --> 00:00:30.320
And the output of

10
00:00:30.320 --> 00:00:34.375
this particular implementation of max pooling will be a two by two output.

11
00:00:34.375 --> 00:00:37.270
And the way you do that is quite simple.

12
00:00:37.270 --> 00:00:40.310
Take your four by four input and break it into

13
00:00:40.310 --> 00:00:44.280
different regions and I'm going to color the four regions as follows.

14
00:00:44.280 --> 00:00:46.130
And then, in the output,

15
00:00:46.130 --> 00:00:47.480
which is two by two,

16
00:00:47.480 --> 00:00:53.240
each of the outputs will just be the max from the corresponding reshaded region.

17
00:00:53.240 --> 00:00:54.680
So the upper left, I guess,

18
00:00:54.680 --> 00:00:57.900
the max of these four numbers is nine.

19
00:00:57.900 --> 00:01:01.760
On upper right, the max of the blue numbers is two.

20
00:01:01.760 --> 00:01:04.273
Lower left, the biggest number is six,

21
00:01:04.273 --> 00:01:08.050
and lower right, the biggest number is three.

22
00:01:08.050 --> 00:01:10.737
So to compute each of the numbers on the right,

23
00:01:10.737 --> 00:01:13.400
we took the max over a two by two regions.

24
00:01:13.400 --> 00:01:18.740
So, this is as if you apply a filter size of two

25
00:01:18.740 --> 00:01:25.290
because you're taking a two by two regions and you're taking a stride of two.

26
00:01:25.290 --> 00:01:30.825
So, these are actually the hyperparameters of

27
00:01:30.825 --> 00:01:36.540
max pooling because we start from this filter size.

28
00:01:36.540 --> 00:01:39.650
It's like a two by two region that gives you the nine.

29
00:01:39.650 --> 00:01:45.580
And then, you step all over two steps to look at this region, to give you the two,

30
00:01:45.580 --> 00:01:46.880
and then for the next row,

31
00:01:46.880 --> 00:01:49.580
you step it down two steps to give you the six,

32
00:01:49.580 --> 00:01:52.570
and then step to the right by two steps to give you three.

33
00:01:52.570 --> 00:01:54.620
So because the squares are two by two, f is equal to two,

34
00:01:54.620 --> 00:01:58.070
and because you stride by two,

35
00:01:58.070 --> 00:02:00.210
s is equal to two.

36
00:02:00.210 --> 00:02:09.526
So here's the intuition behind what max pooling is doing.

37
00:02:09.526 --> 00:02:15.050
If you think of this four by four region as some set of features,

38
00:02:15.050 --> 00:02:19.204
the activations in some layer of the neural network,

39
00:02:19.204 --> 00:02:20.490
then a large number,

40
00:02:20.490 --> 00:02:23.670
it means that it's maybe detected a particular feature.

41
00:02:23.670 --> 00:02:26.495
So, the upper left-hand quadrant has this particular feature.

42
00:02:26.495 --> 00:02:32.470
It maybe a vertical edge or maybe a higher or whisker if you trying to detect a [inaudible].

43
00:02:32.470 --> 00:02:34.820
Clearly, that feature exists in the upper left-hand quadrant.

44
00:02:34.820 --> 00:02:40.055
Whereas this feature, maybe it isn't cat eye detector.

45
00:02:40.055 --> 00:02:43.975
Whereas this feature, it doesn't really exist in the upper right-hand quadrant.

46
00:02:43.975 --> 00:02:47.764
So what the max operation does is a lots of features detected anywhere,

47
00:02:47.764 --> 00:02:53.504
and one of these quadrants , it then remains preserved in the output of max pooling.

48
00:02:53.504 --> 00:02:56.265
So, what the max operates to does is really to say,

49
00:02:56.265 --> 00:02:59.780
if these features detected anywhere in this filter,

50
00:02:59.780 --> 00:03:01.348
then keep a high number.

51
00:03:01.348 --> 00:03:03.510
But if this feature is not detected,

52
00:03:03.510 --> 00:03:07.690
so maybe this feature doesn't exist in the upper right-hand quadrant.

53
00:03:07.690 --> 00:03:11.090
Then the max of all those numbers is still itself quite small.

54
00:03:11.090 --> 00:03:15.252
So maybe that's the intuition behind max pooling.

55
00:03:15.252 --> 00:03:16.535
But I have to admit,

56
00:03:16.535 --> 00:03:19.550
I think the main reason people use max pooling is

57
00:03:19.550 --> 00:03:23.627
because it's been found in a lot of experiments to work well,

58
00:03:23.627 --> 00:03:25.646
and the intuition I just described,

59
00:03:25.646 --> 00:03:27.375
despite it being often cited,

60
00:03:27.375 --> 00:03:33.020
I don't know of anyone fully knows if that is the real underlying reason.

61
00:03:33.020 --> 00:03:34.655
I don't have anyone knows if that's

62
00:03:34.655 --> 00:03:39.930
the real underlying reason that max pooling works well in ConvNets.

63
00:03:39.930 --> 00:03:43.490
One interesting property of max pooling is that it has

64
00:03:43.490 --> 00:03:47.770
a set of hyperparameters but it has no parameters to learn.

65
00:03:47.770 --> 00:03:50.293
There's actually nothing for gradient descent to learn.

66
00:03:50.293 --> 00:03:51.780
Once you fix f and s,

67
00:03:51.780 --> 00:03:56.876
it's just a fixed computation and gradient descent doesn't change anything.

68
00:03:56.876 --> 00:04:00.810
Let's go through an example with some different hyperparameters.

69
00:04:00.810 --> 00:04:04.675
Here, I am going to use, sure you have a five by five input

70
00:04:04.675 --> 00:04:10.290
and we're going to apply max pooling with a filter size that's three by three.

71
00:04:10.290 --> 00:04:13.815
So f is equal to three and let's use a stride of one.

72
00:04:13.815 --> 00:04:18.190
So in this case, the output size is going to be three by three.

73
00:04:18.190 --> 00:04:20.570
And the formulas we had developed in

74
00:04:20.570 --> 00:04:23.945
the previous videos for figuring out the output size for conv layer,

75
00:04:23.945 --> 00:04:27.345
those formulas also work for max pooling.

76
00:04:27.345 --> 00:04:34.345
So, that's n plus 2p minus f over s plus 1.

77
00:04:34.345 --> 00:04:38.458
That formula also works for figuring out the output size of max pooling.

78
00:04:38.458 --> 00:04:41.820
But in this example, let's compute each of the elements of this three by three output.

79
00:04:41.820 --> 00:04:45.080
The upper left-hand elements,

80
00:04:45.080 --> 00:04:46.670
we're going to look over that region.

81
00:04:46.670 --> 00:04:48.735
So notice this is a three by three region

82
00:04:48.735 --> 00:04:51.695
because the filter size is three and to the max there.

83
00:04:51.695 --> 00:04:53.715
So, that will be nine,

84
00:04:53.715 --> 00:04:57.920
and then we shifted over by one because which you can stride at one.

85
00:04:57.920 --> 00:05:00.960
So, that max in the blue box is nine.

86
00:05:00.960 --> 00:05:03.695
Let's shift that over again.

87
00:05:03.695 --> 00:05:06.235
The max of the blue box is five.

88
00:05:06.235 --> 00:05:09.710
And then let's go on to the next row, a stride of one.

89
00:05:09.710 --> 00:05:12.465
So we're just stepping down by one step.

90
00:05:12.465 --> 00:05:16.520
So max in that region is nine, max in that region is nine,

91
00:05:16.520 --> 00:05:19.970
max in that region,

92
00:05:19.970 --> 00:05:22.516
it's now with a two fives, we have maxes of five.

93
00:05:22.516 --> 00:05:26.130
And then finally, max in that is eight.

94
00:05:26.130 --> 00:05:28.965
Max in that is six,

95
00:05:28.965 --> 00:05:31.350
and max in that, this is not [inaudible].

96
00:05:31.350 --> 00:05:35.810
Okay, so this, with this set of hyperparameters f equals three,

97
00:05:35.810 --> 00:05:40.007
s equals one gives that output shown [inaudible].

98
00:05:40.007 --> 00:05:44.975
Now, so far, I've shown max pooling on a 2D inputs.

99
00:05:44.975 --> 00:05:47.370
If you have a 3D input,

100
00:05:47.370 --> 00:05:53.245
then the outputs will have the same dimension.

101
00:05:53.245 --> 00:05:56.765
So for example, if you have five by five by two,

102
00:05:56.765 --> 00:06:02.360
then the output will be three by three by two and the way you compute

103
00:06:02.360 --> 00:06:05.045
max pooling is you perform the computation

104
00:06:05.045 --> 00:06:08.368
we just described on each of the channels independently.

105
00:06:08.368 --> 00:06:11.960
So the first channel which is shown here on top is still the same,

106
00:06:11.960 --> 00:06:13.790
and then for the second channel, I guess,

107
00:06:13.790 --> 00:06:15.790
this one that I just drew at the bottom,

108
00:06:15.790 --> 00:06:19.250
you would do the same computation on that slice of

109
00:06:19.250 --> 00:06:24.365
this value and that gives you the second slice.

110
00:06:24.365 --> 00:06:29.300
And more generally, if this was five by five by some number of channels,

111
00:06:29.300 --> 00:06:34.395
the output would be three by three by that same number of channels.

112
00:06:34.395 --> 00:06:44.541
And the max pooling computation is done independently on each of these N_C channels.

113
00:06:44.541 --> 00:06:46.520
So, that's max pooling.

114
00:06:46.520 --> 00:06:49.815
This one is the type of pooling that isn't used very often,

115
00:06:49.815 --> 00:06:52.870
but I'll mention briefly which is average pooling.

116
00:06:52.870 --> 00:06:56.395
So it does pretty much what you'd expect which is,

117
00:06:56.395 --> 00:06:59.080
instead of taking the maxes within each filter,

118
00:06:59.080 --> 00:07:02.040
you take the average.

119
00:07:02.040 --> 00:07:03.250
So in this example,

120
00:07:03.250 --> 00:07:07.540
the average of the numbers in purple is 3.75,

121
00:07:07.540 --> 00:07:09.940
then there is 1.25,

122
00:07:09.940 --> 00:07:12.930
and four and two.

123
00:07:12.930 --> 00:07:17.020
And so, this is average pooling with hyperparameters f equals two,

124
00:07:17.020 --> 00:07:21.795
s equals two, we can choose other hyperparameters as well.

125
00:07:21.795 --> 00:07:24.640
So these days, max pooling is used much more

126
00:07:24.640 --> 00:07:28.340
often than average pooling with one exception,

127
00:07:28.340 --> 00:07:32.125
which is sometimes very deep in a neural network.

128
00:07:32.125 --> 00:07:36.670
You might use average pooling to collapse your representation from say,

129
00:07:36.670 --> 00:07:40.290
7 by 7 by 1,000.

130
00:07:40.290 --> 00:07:42.755
An average over all the [inaudible] ,

131
00:07:42.755 --> 00:07:45.625
you get 1 by 1 by 1,000.

132
00:07:45.625 --> 00:07:47.475
We'll see an example of this later.

133
00:07:47.475 --> 00:07:54.085
But you see, max pooling used much more in the neural network than average pooling.

134
00:07:54.085 --> 00:07:56.305
So just to summarize,

135
00:07:56.305 --> 00:08:00.100
the hyperparameters for pooling are f,

136
00:08:00.100 --> 00:08:02.840
the filter size and s, the stride,

137
00:08:02.840 --> 00:08:07.360
and maybe common choices of parameters might be f equals two, s equals two.

138
00:08:07.360 --> 00:08:11.045
This is used quite often and this has the effect

139
00:08:11.045 --> 00:08:15.925
of roughly shrinking the height and width by a factor of above two,

140
00:08:15.925 --> 00:08:21.150
and a common chosen hyperparameters might be f equals two, s equals two,

141
00:08:21.150 --> 00:08:23.530
and this has the effect of shrinking

142
00:08:23.530 --> 00:08:28.440
the height and width of the representation by a factor of two.

143
00:08:28.440 --> 00:08:32.094
I've also seen f equals three, s equals two used,

144
00:08:32.094 --> 00:08:37.150
and then the other hyperparameter is just like a binary bit that says,

145
00:08:37.150 --> 00:08:40.120
are you using max pooling or are you using average pooling.

146
00:08:40.120 --> 00:08:43.380
If you want, you can add an extra hyperparameter

147
00:08:43.380 --> 00:08:48.140
for the padding although this is very, very rarely used.

148
00:08:48.140 --> 00:08:50.080
When you do max pooling, usually,

149
00:08:50.080 --> 00:08:51.685
you do not use any padding,

150
00:08:51.685 --> 00:08:55.025
although there is one exception that we'll see next week as well.

151
00:08:55.025 --> 00:08:57.160
But for the most parts of max pooling,

152
00:08:57.160 --> 00:08:59.710
usually, it does not use any padding.

153
00:08:59.710 --> 00:09:05.345
So, the most common value of p by far is p equals zero.

154
00:09:05.345 --> 00:09:13.215
And the input of max pooling is that you input a volume of size that,

155
00:09:13.215 --> 00:09:14.945
N_H by N_W by N_C,

156
00:09:14.945 --> 00:09:21.265
and it would output a volume of size given by this.

157
00:09:21.265 --> 00:09:29.465
So assuming there's no padding by N_W minus f over s,

158
00:09:29.465 --> 00:09:32.015
this one for by N_C.

159
00:09:32.015 --> 00:09:35.295
So the number of input channels is equal to the number of output channels

160
00:09:35.295 --> 00:09:40.555
because pooling applies to each of your channels independently.

161
00:09:40.555 --> 00:09:47.205
One thing to note about pooling is that there are no parameters to learn.

162
00:09:47.205 --> 00:09:50.470
So, when we implement that crop,

163
00:09:50.470 --> 00:09:55.645
you find that there are no parameters that backdrop will adapt through max pooling.

164
00:09:55.645 --> 00:09:58.400
Instead, there are just these hyperparameters that you set once,

165
00:09:58.400 --> 00:10:01.485
maybe set ones by hand or set using cross-validation.

166
00:10:01.485 --> 00:10:03.710
And then beyond that, you are done.

167
00:10:03.710 --> 00:10:07.140
Its just a fixed function that the neural network computes in one of the layers,

168
00:10:07.140 --> 00:10:09.829
and there is actually nothing to learn.

169
00:10:09.829 --> 00:10:11.999
It's just a fixed function.

170
00:10:11.999 --> 00:10:13.350
So, that's it for pooling.

171
00:10:13.350 --> 00:10:15.460
You now know how to build convolutional layers and pooling layers.

172
00:10:15.460 --> 00:10:18.095
In the next video,

173
00:10:18.095 --> 00:10:20.830
let's see a more complex example of a ConvNet.

174
00:10:20.830 --> 00:10:25.000
One that will also allow us to introduce fully connected layers.