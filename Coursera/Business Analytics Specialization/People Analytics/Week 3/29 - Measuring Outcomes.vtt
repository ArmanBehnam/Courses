WEBVTT

1
00:00:00.460 --> 00:00:04.570
So I really wanna just build on that
last piece a little bit more with

2
00:00:04.570 --> 00:00:06.390
something that's a bit of an aside.

3
00:00:06.390 --> 00:00:07.740
But it's only an aside because it's so

4
00:00:07.740 --> 00:00:11.380
important that it doesn't just
apply to this particular segment,

5
00:00:11.380 --> 00:00:16.425
it applies throughout the people
analytics piece of your course, right.

6
00:00:16.425 --> 00:00:21.615
And that is about measuring outcomes
because in any one of the modules

7
00:00:21.615 --> 00:00:26.085
that you've been listening to and learning
from in terms of people analytics,

8
00:00:26.085 --> 00:00:30.925
you will have heard a lot about different
kinds of outcomes that we care about.

9
00:00:30.925 --> 00:00:33.825
Now we were just talking about outcomes,
which is why I'm gonna connect it here,

10
00:00:33.825 --> 00:00:35.939
but again, it applies across the modules.

11
00:00:37.230 --> 00:00:40.590
So the main message here can
be summarized in a nutshell,

12
00:00:40.590 --> 00:00:43.680
which is basically garbage in,
garbage out.

13
00:00:45.130 --> 00:00:50.330
So the idea here is that if
you don't have good measures,

14
00:00:50.330 --> 00:00:53.840
you can plug whatever you like
into your statistical program, and

15
00:00:53.840 --> 00:00:55.970
it can generate lots and
lots of lovely results.

16
00:00:55.970 --> 00:00:59.690
But your results will mean nothing
if those original measures were bad.

17
00:00:59.690 --> 00:01:00.390
Right?

18
00:01:00.390 --> 00:01:02.814
So if you put garbage in,
in the form of poor measures,

19
00:01:02.814 --> 00:01:05.400
whatever results you get are really
gonna be meaningless and

20
00:01:05.400 --> 00:01:08.120
may actually be harmful if
you try to act on them.

21
00:01:08.120 --> 00:01:12.120
So it becomes very, very important
in any form of analytics and

22
00:01:12.120 --> 00:01:15.750
particularly people analytics
what we are talking about today,

23
00:01:15.750 --> 00:01:19.640
be able to measure your
variables really well.

24
00:01:19.640 --> 00:01:23.022
And we're focusing here on outcome
variables, although I've already talked

25
00:01:23.022 --> 00:01:25.494
about those network variables
that we might care about and

26
00:01:25.494 --> 00:01:26.920
how to measure them really well.

27
00:01:26.920 --> 00:01:30.150
But let's think about outcome variables
here because that's gonna apply again

28
00:01:30.150 --> 00:01:32.760
across the field of people analytics.

29
00:01:32.760 --> 00:01:36.698
So if we look at how do collaboration
patterns matter for important outcomes?

30
00:01:36.698 --> 00:01:38.947
That question we were
just talking about and

31
00:01:38.947 --> 00:01:43.017
we have well our five building blocks in
terms of networks, we discussed a lot in

32
00:01:43.017 --> 00:01:46.577
segment two how to actually map
those building blocks effectively.

33
00:01:46.577 --> 00:01:51.343
Actually we talked a lot in segment three
on how to actually map those building

34
00:01:51.343 --> 00:01:52.745
blocks effectively.

35
00:01:52.745 --> 00:01:56.720
Now we're gonna focus on how to connect
those to individual outcomes and as we

36
00:01:56.720 --> 00:02:01.540
were pretty just talking about we're going
to focus particularly on performance.

37
00:02:01.540 --> 00:02:05.570
So we've already discussed a little bit,
we need to map network attributes,

38
00:02:05.570 --> 00:02:08.880
network features to performance but
what I just want to emphasize is

39
00:02:08.880 --> 00:02:12.960
the importance of trying to measure
performance well and what that takes.

40
00:02:15.030 --> 00:02:16.770
So let's think about
measuring performance.

41
00:02:18.380 --> 00:02:21.130
The question is,
what is a strong measure of performance?

42
00:02:21.130 --> 00:02:24.890
I have just picked performance, but
this could be many other things.

43
00:02:24.890 --> 00:02:25.920
It could be satisfaction.

44
00:02:25.920 --> 00:02:27.000
It could be commitment.

45
00:02:27.000 --> 00:02:28.750
It could be intention turnover.

46
00:02:28.750 --> 00:02:30.560
What's a strong measure of anything?

47
00:02:30.560 --> 00:02:31.500
Here we're gonna use performance.

48
00:02:32.890 --> 00:02:36.480
There are several key criteria
whenever you're trying to

49
00:02:36.480 --> 00:02:40.060
measure anything in order to get away from
the garbage in, garbage out scenario and

50
00:02:40.060 --> 00:02:42.360
have really good measures that
can give you really good results.

51
00:02:43.390 --> 00:02:46.770
The first is you have to get a measure
that's at the right level of analysis,

52
00:02:46.770 --> 00:02:49.015
so if I'm trying to measure
individual performance,

53
00:02:49.015 --> 00:02:51.985
I have to make sure that what I'm
measuring in terms of performance

54
00:02:51.985 --> 00:02:53.835
is actually an individual
performance measure.

55
00:02:53.835 --> 00:02:56.395
It's not something that was generated by
the whole unit or the whole group and

56
00:02:56.395 --> 00:02:58.785
the individual didn't really
have that much control over it.

57
00:02:58.785 --> 00:03:00.655
It needs to be at the right
level of analysis if I'm

58
00:03:00.655 --> 00:03:01.995
trying to measure individual performance.

59
00:03:03.965 --> 00:03:07.790
The second and third criteria
are reliability and validity.

60
00:03:07.790 --> 00:03:09.780
And if you've taken any
measurement type courses or

61
00:03:09.780 --> 00:03:14.000
been involved in this field at all, you'll
have heard of reliability and validity.

62
00:03:14.000 --> 00:03:18.640
Reliability means are your assessments,
are your measures, consistent?

63
00:03:18.640 --> 00:03:22.090
And that might mean over time and
it might mean across raters or

64
00:03:22.090 --> 00:03:23.869
across people that
are taking the measures.

65
00:03:25.550 --> 00:03:30.960
The third criteria here is validity,
which means are the assessments accurate?

66
00:03:30.960 --> 00:03:32.280
Are your measures accurate?

67
00:03:32.280 --> 00:03:35.300
Are they measuring what they're
actually supposed to measure?

68
00:03:35.300 --> 00:03:37.700
So if you're measuring performance
in terms of reliability,

69
00:03:37.700 --> 00:03:41.060
you need to say well if I measure this
person's performance in the morning,

70
00:03:41.060 --> 00:03:44.600
and I happen to look at how well they're
performing in the morning, am I gonna get

71
00:03:44.600 --> 00:03:47.660
the same result if I measure their
performance in the late afternoon?

72
00:03:47.660 --> 00:03:50.790
Because if not, it's not a very reliable
measure of their performance, right?

73
00:03:50.790 --> 00:03:52.120
Maybe it's their typing speed, right?

74
00:03:52.120 --> 00:03:54.170
So if their typing speed in the morning
is different from their typing

75
00:03:54.170 --> 00:03:55.630
speed in the afternoon,
it's not a very reliable measure.

76
00:03:55.630 --> 00:04:00.190
So we need measures that are consistent
over time and across raters.

77
00:04:00.190 --> 00:04:02.410
In terms of performance,
if you're looking at validity,

78
00:04:02.410 --> 00:04:04.450
is it actually capturing
their performance?

79
00:04:04.450 --> 00:04:07.390
So if you're measuring typing speed,
is that an aspect

80
00:04:07.390 --> 00:04:10.000
of the performance that's actually
important and relevant to the task?

81
00:04:10.000 --> 00:04:13.050
Or are you actually trying to measure
something about how good they are at

82
00:04:13.050 --> 00:04:13.850
writing?

83
00:04:13.850 --> 00:04:16.590
And typing speed is really
not a good measure of that.

84
00:04:16.590 --> 00:04:18.650
So, when we talk about reliability and
validity,

85
00:04:18.650 --> 00:04:21.589
we often use this kind of bullseye motif.

86
00:04:23.050 --> 00:04:25.850
And it's helpful just for
understanding the difference between them.

87
00:04:25.850 --> 00:04:30.900
So something that's reliable again has to
be consistent over time and across raters.

88
00:04:30.900 --> 00:04:35.110
In other words you're hitting the same
spot on the bullseye over and over again.

89
00:04:35.110 --> 00:04:37.810
That makes it reliable, but
that doesn't necessarily make it valid.

90
00:04:39.290 --> 00:04:42.410
For something to be valid, you need
to be hitting the bullseye, right, or

91
00:04:42.410 --> 00:04:45.580
you need to be sort of equally
distributed around the bullseye, so

92
00:04:45.580 --> 00:04:49.210
there's no sort of bias in the way
you're hitting the target.

93
00:04:49.210 --> 00:04:52.960
So a measure that's valid but
not reliable will be sort of hitting,

94
00:04:52.960 --> 00:04:54.830
it's equally distributed
around the bullseye, but

95
00:04:54.830 --> 00:04:57.770
it's not very reliable, you're
hitting different points all the time.

96
00:04:57.770 --> 00:05:01.520
Neither reliable nor valid, you're kind
of biased to one part of the target, and

97
00:05:01.520 --> 00:05:04.600
both reliable and valid you're
consistently hitting the bullseye.

98
00:05:04.600 --> 00:05:07.780
This is just a little way that
we sometimes use to help us

99
00:05:07.780 --> 00:05:12.780
keep the difference between reliability
and validity separate in our minds.

100
00:05:12.780 --> 00:05:15.500
But going back to these criteria for
measuring performance, so there

101
00:05:15.500 --> 00:05:18.440
has to be the right level of analysis,
has to be reliable, has to be valid.

102
00:05:19.690 --> 00:05:23.070
The next three criteria,
it has to be comparable.

103
00:05:23.070 --> 00:05:27.030
So, if you're measuring performance across
lots of people or across lots of units or

104
00:05:27.030 --> 00:05:30.320
lots of teams say, you need a measure
that's comparable for all those people.

105
00:05:30.320 --> 00:05:32.520
You can't measure everybody
on different criteria, right?

106
00:05:32.520 --> 00:05:34.370
It needs to be comparable.

107
00:05:34.370 --> 00:05:37.190
It needs to be comprehensive, in other
words you need to have that measure for

108
00:05:37.190 --> 00:05:39.220
everybody whose performance
you're trying to measure.

109
00:05:40.490 --> 00:05:42.430
And it needs to be cost-effective usually.

110
00:05:42.430 --> 00:05:45.050
If it's very expensive to collect
performance measures you could have

111
00:05:45.050 --> 00:05:47.760
the best performance measure in
the world that involves, you know,

112
00:05:47.760 --> 00:05:51.180
really following people's performance over
time, and monitoring it, and observing it,

113
00:05:51.180 --> 00:05:52.000
and all sorts of great stuff.

114
00:05:52.000 --> 00:05:55.590
And it can be a really great, reliable,
valid measure of performance, but

115
00:05:55.590 --> 00:05:58.620
it's just not cost-effective, so
we also have to kind of come up with

116
00:05:58.620 --> 00:06:00.064
measures that are relatively
cost-effective.

117
00:06:01.800 --> 00:06:04.660
And then finally, causality.

118
00:06:04.660 --> 00:06:09.320
So in terms of causality, the measure
of performance, if you want it to be

119
00:06:09.320 --> 00:06:12.460
an outcome variable, you have to
be able to make a plausible claim,

120
00:06:12.460 --> 00:06:16.360
a plausible argument for
why it is actually an outcome variable.

121
00:06:16.360 --> 00:06:21.420
In other words, for why something
else causes that outcome variable.

122
00:06:21.420 --> 00:06:23.660
Now I'm not going to go into
that in more detail here.

123
00:06:23.660 --> 00:06:26.530
It's a very big and
a very important topic.

124
00:06:26.530 --> 00:06:29.570
If you want to claim that this
kind of network configuration or

125
00:06:29.570 --> 00:06:33.040
this kind of career path for

126
00:06:33.040 --> 00:06:37.790
example affects your performance,
you need to be able to make causal claims.

127
00:06:37.790 --> 00:06:42.330
The topic of causal claims is
gonna be covered much more depth

128
00:06:42.330 --> 00:06:46.750
by Matthew Bidwell in his module where
he'll talk much more about causality.

129
00:06:46.750 --> 00:06:49.350
So that's all I'm gonna say about it here,
other than that, it's a really big and

130
00:06:49.350 --> 00:06:50.790
important issue.

131
00:06:50.790 --> 00:06:55.090
Okay, so when we have these criteria for
what's a strong measure of performance and

132
00:06:55.090 --> 00:06:59.520
we're thinking about what is the sets
of network attributes that predict

133
00:06:59.520 --> 00:07:01.470
an individual outcome like performance,

134
00:07:01.470 --> 00:07:05.200
again, lots of different performance
measures we could be choosing between, but

135
00:07:05.200 --> 00:07:07.860
we want to use these criteria to
help us decide which are good.

136
00:07:08.860 --> 00:07:11.620
Is sales per quarter a good
measure of performance?

137
00:07:11.620 --> 00:07:14.750
Well, you know sales per quarter may not
be at the right level of analysis if it's

138
00:07:14.750 --> 00:07:16.390
the whole unit or the whole team and

139
00:07:16.390 --> 00:07:18.320
you're trying to measure
individual outcomes.

140
00:07:18.320 --> 00:07:20.820
If it's a whole team that produces sales,
it's not so good.

141
00:07:20.820 --> 00:07:22.190
Cost savings, same thing.

142
00:07:22.190 --> 00:07:25.110
Maybe cost savings are a result of
decisions are taken somewhere else in

143
00:07:25.110 --> 00:07:28.600
the organization and not really a good
measure of individual performance.

144
00:07:28.600 --> 00:07:29.830
Self reported ratings.

145
00:07:29.830 --> 00:07:33.310
We sometimes have to rely on, in surveys,
people measuring their own performance.

146
00:07:33.310 --> 00:07:36.380
We know there's a lot of problems
with self reported ratings.

147
00:07:36.380 --> 00:07:38.820
They don't tend to be very reliable or
valid.

148
00:07:38.820 --> 00:07:41.250
Manager reported ratings might be better,
but

149
00:07:41.250 --> 00:07:43.650
there can be problems with bias there too.

150
00:07:43.650 --> 00:07:46.730
May not be very cost effective
to collect that kind of data.

151
00:07:46.730 --> 00:07:49.100
May not be very comparable
across managers.

152
00:07:49.100 --> 00:07:53.950
Measures like bonuses often capture
something quite different in terms of

153
00:07:53.950 --> 00:07:56.430
the bonus pool, for example, and
don't really capture performance.

154
00:07:56.430 --> 00:07:58.670
So again,
there's a measure of validity there.

155
00:07:58.670 --> 00:08:03.132
So, any measure of performance that you
can get is gonna have some problems, but

156
00:08:03.132 --> 00:08:07.730
understanding what those are and trying to
apply these criteria for what's a strong

157
00:08:07.730 --> 00:08:11.860
measure of performance or a strong measure
of any outcome that you care about

158
00:08:11.860 --> 00:08:16.300
is gonna be critical to avoiding that
garbage in garbage out kind of trap.

159
00:08:16.300 --> 00:08:20.250
Okay if the measure is poor in the first
place, the results are gonna be very,

160
00:08:20.250 --> 00:08:21.150
you can't trust them.

161
00:08:21.150 --> 00:08:21.830
It's not worth it.

162
00:08:21.830 --> 00:08:24.360
You gotta have good measures going in.

163
00:08:24.360 --> 00:08:26.320
So the role of people analytics again,

164
00:08:26.320 --> 00:08:29.620
people analytics is a data driven
approach to managing people at work.

165
00:08:29.620 --> 00:08:33.000
But, and again this applies across
all the modules here, collecting and

166
00:08:33.000 --> 00:08:35.930
analyzing high quality data
is absolutely critical.

167
00:08:35.930 --> 00:08:39.540
If you don't have good data, none of
the rest of it's going to matter at all,

168
00:08:39.540 --> 00:08:40.384
gotta have good data.

169
00:08:40.384 --> 00:08:46.900
So we have now had a chance to think about
how to evaluate collaboration networks in

170
00:08:46.900 --> 00:08:51.590
terms of how can we compare collaboration
attributes and how collaboration

171
00:08:51.590 --> 00:08:55.400
features of the network across people and
how can we map that and connect that

172
00:08:55.400 --> 00:08:59.180
to outcomes that we might care about
like their individual performance.

173
00:08:59.180 --> 00:09:00.010
In our last segment,

174
00:09:00.010 --> 00:09:02.910
we're gonna think about,
how can we intervene to make changes in

175
00:09:02.910 --> 00:09:06.680
collaboration networks based on the kind
of data that we've collected and analyzed?