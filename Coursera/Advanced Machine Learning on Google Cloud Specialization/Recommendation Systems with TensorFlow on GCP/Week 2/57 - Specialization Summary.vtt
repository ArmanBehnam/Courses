WEBVTT

1
00:00:00.110 --> 00:00:06.510
In this specialization, you learned how to do machine learning at scale,

2
00:00:06.510 --> 00:00:09.705
and how to build specialized machine learning models

3
00:00:09.705 --> 00:00:13.725
for images, sequences, and recommendations.

4
00:00:13.725 --> 00:00:16.980
We started with the course that recap

5
00:00:16.980 --> 00:00:21.165
the first specialization by building an end-to-end model.

6
00:00:21.165 --> 00:00:29.130
We started by exploring and visualizing the dataset of natality births in BigQuery.

7
00:00:29.130 --> 00:00:32.750
We finished by deploying an application that was

8
00:00:32.750 --> 00:00:37.265
capable of predicting the weights of newborn babies.

9
00:00:37.265 --> 00:00:42.530
We worked with a sub-sample of the dataset to develop a TensorFlow model.

10
00:00:42.530 --> 00:00:46.235
Once we had prototyped our model locally we used

11
00:00:46.235 --> 00:00:52.655
Cloud Dataflow to create a training and evaluation sets using the entire dataset.

12
00:00:52.655 --> 00:00:57.875
We then trained our model using Cloud ML Engine.

13
00:00:57.875 --> 00:01:00.350
Using this trained model,

14
00:01:00.350 --> 00:01:04.550
we served out a prediction service that an end user was able to

15
00:01:04.550 --> 00:01:11.275
consume via a Python Flask application that we deployed using App Engine.

16
00:01:11.275 --> 00:01:16.870
The second course was on building production machine learning models.

17
00:01:16.870 --> 00:01:21.140
We discussed many of the things that the system should be able to

18
00:01:21.140 --> 00:01:26.850
do and the components that take responsibility for doing these things.

19
00:01:26.850 --> 00:01:30.400
We discussed how to deal with change,

20
00:01:30.400 --> 00:01:33.560
and how they affect machine learning systems,

21
00:01:33.560 --> 00:01:37.285
and what we can do to mitigate those effects.

22
00:01:37.285 --> 00:01:42.965
We talked about how to squeeze the most performance artifact ML system

23
00:01:42.965 --> 00:01:49.100
by choosing the right hardware such as TPUs and removing bottlenecks.

24
00:01:49.100 --> 00:01:53.630
We talked at a high level about the technology behind hybrid systems,

25
00:01:53.630 --> 00:01:57.845
which may run on the Cloud or on the Edge or on Prem.

26
00:01:57.845 --> 00:02:02.425
The third course was on building image models.

27
00:02:02.425 --> 00:02:05.690
We looked at linear and deep neural networks for

28
00:02:05.690 --> 00:02:09.430
image classification but quickly ran into trouble.

29
00:02:09.430 --> 00:02:12.800
Then we looked at convolutional and pooling

30
00:02:12.800 --> 00:02:17.675
layers and how they can operate as feature extractors.

31
00:02:17.675 --> 00:02:22.300
After building the image classifiers from scratch,

32
00:02:22.300 --> 00:02:25.045
we experimented with AutoML,

33
00:02:25.045 --> 00:02:30.395
which uses a combination of transfer learning and neural architecture search.

34
00:02:30.395 --> 00:02:33.260
All you have to do is upload your images into

35
00:02:33.260 --> 00:02:36.535
Cloud storage and the product does the rest.

36
00:02:36.535 --> 00:02:40.775
The fourth course was on building sequence models.

37
00:02:40.775 --> 00:02:42.710
These are the types of models used in

38
00:02:42.710 --> 00:02:47.560
natural language problems like text classification and translation.

39
00:02:47.560 --> 00:02:50.945
We started out by looking at the similarities between

40
00:02:50.945 --> 00:02:55.295
image data and sequence data when it comes to locality.

41
00:02:55.295 --> 00:03:01.645
We were able to apply many of the image techniques to sequences.

42
00:03:01.645 --> 00:03:04.975
We then looked at LSTM models,

43
00:03:04.975 --> 00:03:10.325
which allow the model to maintain state across successive inputs,

44
00:03:10.325 --> 00:03:15.100
something that is very important for sequence problems.

45
00:03:15.100 --> 00:03:22.490
We also discussed different approaches to build embeddings for natural language.

46
00:03:22.490 --> 00:03:28.330
Then we learn how to reuse machine learning components from TensorFlow hub.

47
00:03:28.330 --> 00:03:31.120
Finally, as with images,

48
00:03:31.120 --> 00:03:33.815
we looked at higher level abstractions.

49
00:03:33.815 --> 00:03:36.035
We looked at AutoML,

50
00:03:36.035 --> 00:03:44.175
but this time for text translation and a building conversation systems using Dialogflow.

51
00:03:44.175 --> 00:03:50.600
We ended the specialization with building real world recommendation systems.

52
00:03:50.600 --> 00:03:54.410
This brought together all the concepts that we

53
00:03:54.410 --> 00:03:58.660
learned in both the previous specialization and in this one.

54
00:03:58.660 --> 00:04:00.170
We looked at different types of

55
00:04:00.170 --> 00:04:04.100
recommendation systems, content-based, collaborative filtering,

56
00:04:04.100 --> 00:04:07.130
and knowledge-based, and how to implement each of

57
00:04:07.130 --> 00:04:10.480
them in TensorFlow to build end-to-end systems.

58
00:04:10.480 --> 00:04:13.610
We ended with a lab on how to orchestrate

59
00:04:13.610 --> 00:04:15.680
the continuous retraining of

60
00:04:15.680 --> 00:04:20.075
the recommendation system as new ratings data comes in from users.

61
00:04:20.075 --> 00:04:23.030
With this, we've come to the end of

62
00:04:23.030 --> 00:04:28.190
this specialization on advanced machine learning with TensorFlow on GCP.

63
00:04:28.190 --> 00:04:33.020
Thanks to all my colleagues for their great work and their attention to detail.

64
00:04:33.020 --> 00:04:40.310
Creating a set of 10 courses and keeping it fresh and up-to-date is a huge undertaking.

65
00:04:40.310 --> 00:04:43.520
I wouldn't have been able to do it without the contributions of

66
00:04:43.520 --> 00:04:47.645
so many people both in front of and behind the camera.

67
00:04:47.645 --> 00:04:53.185
We hope that you found this set of courses clear, practical, and useful.

68
00:04:53.185 --> 00:04:54.900
Do tell your friends,

69
00:04:54.900 --> 00:04:58.790
and remember that all the code that accompanies

70
00:04:58.790 --> 00:05:04.370
all the labs in this specialization is open source and available on GitHub.

71
00:05:04.370 --> 00:05:07.580
We hope that you're able to use the code to build

72
00:05:07.580 --> 00:05:11.540
your own end-to-end machine learning systems.

73
00:05:11.540 --> 00:05:14.490
Thank you, and good luck.