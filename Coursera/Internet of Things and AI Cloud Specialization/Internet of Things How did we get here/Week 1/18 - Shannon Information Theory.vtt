WEBVTT

1
00:00:03.270 --> 00:00:12.360
Claude Shannon at Bell Labs in 1948
came up with the information theory,

2
00:00:12.360 --> 00:00:17.170
and the mathematical underpinnings
of all digital communications.

3
00:00:19.850 --> 00:00:27.170
Here is a textbook blog diagram of
the communication model, plot diagram.

4
00:00:29.330 --> 00:00:35.030
This is an information source that is
transmitted through your transmitter.

5
00:00:35.030 --> 00:00:37.410
You have a noisy channel, and

6
00:00:37.410 --> 00:00:43.570
the decoder does reverse of
what the channel encoder does.

7
00:00:43.570 --> 00:00:47.700
And we talked about speech and
audio codecs,

8
00:00:47.700 --> 00:00:52.370
that the reconstruction is
done by the destination.

9
00:00:53.970 --> 00:00:58.850
So, some of the concepts here
are entropy of the signal.

10
00:00:58.850 --> 00:01:03.480
This is a measure of how
much unpredictability or

11
00:01:03.480 --> 00:01:09.260
uncertainty is there in
the information a source generates.

12
00:01:09.260 --> 00:01:16.150
Think of your all SMS messages
over the last one year, and think

13
00:01:16.150 --> 00:01:21.930
of all the projects that you submitted
in the final year of your college.

14
00:01:21.930 --> 00:01:26.070
Clearly, the college reports

15
00:01:26.070 --> 00:01:31.260
have a lot more words
of different meanings.

16
00:01:31.260 --> 00:01:37.570
The size of the vocabulary is much bigger,
compared to the archive of your SMS.

17
00:01:38.590 --> 00:01:44.327
So, the SMS archive has
much smaller entropy,

18
00:01:44.327 --> 00:01:50.066
so you need very few bits
to send that data across

19
00:01:50.066 --> 00:01:57.109
the channel compared to a more involved,
detailed text.

20
00:01:57.109 --> 00:02:02.527
We looked at speech signals
that you're sampling at 8 kHz,

21
00:02:02.527 --> 00:02:09.869
say maybe, 12 bits, to cover a 70 dB
dynamic range, you need 96 kHz/sec.

22
00:02:09.869 --> 00:02:14.720
But, there is a lot of
redundancy in the source.

23
00:02:16.430 --> 00:02:20.370
That the microphone generates and

24
00:02:20.370 --> 00:02:25.720
sampled by the A to D converter, following
the latest Nyquist Sampling Theorem.

25
00:02:27.540 --> 00:02:33.596
And the techniques that I explained

26
00:02:33.596 --> 00:02:39.230
that you could compress it down
to eight kilobits per second,

27
00:02:39.230 --> 00:02:41.965
and still maintain tall quality.

28
00:02:44.310 --> 00:02:48.850
The side effect of this is,
once you have compressed this signal,

29
00:02:48.850 --> 00:02:52.680
you have taken away all
the redundancy in the signal.

30
00:02:52.680 --> 00:02:56.110
Each and every bit that you're
sending over the channel

31
00:02:56.110 --> 00:02:59.302
is that much more important.

32
00:02:59.302 --> 00:03:05.990
So, if you have a noisy channel, you
lose one bit, you fall that much harder.

33
00:03:05.990 --> 00:03:11.145
The impact it has on the quality
of the receiver is so much more,

34
00:03:11.145 --> 00:03:16.594
compared to if you were sending
a signal that is highly redundant,

35
00:03:16.594 --> 00:03:19.929
let's say at 64 kilobits per second.

36
00:03:19.929 --> 00:03:25.149
So what the encoder does is, of course,
if you have a clean channel,

37
00:03:25.149 --> 00:03:28.570
a noiseless channel, then the source rate,

38
00:03:28.570 --> 00:03:32.892
the rate at which the information
source is generating,

39
00:03:32.892 --> 00:03:37.682
let's say the A to D converter
following the Nyquist Theorem.

40
00:03:37.682 --> 00:03:44.650
It can be said that you send over
the channel, but there is always noise.

41
00:03:45.920 --> 00:03:51.250
So in order to combat noise,
what the transmitter encoder does is

42
00:03:51.250 --> 00:03:58.080
it acts in more redundancy in a known
manner, back to the data stream.

43
00:03:58.080 --> 00:04:02.260
And at the receiver,
knowing the properties of

44
00:04:02.260 --> 00:04:07.720
the way in which the transmitter acts,

45
00:04:07.720 --> 00:04:12.780
you can remove that, and you can correct
some of the errors that were there

46
00:04:12.780 --> 00:04:19.040
in the information that you
receive over a noisy channel.

47
00:04:19.040 --> 00:04:22.060
This relates to the channel capacity.

48
00:04:22.060 --> 00:04:26.976
So what Shannon's information theory,
the information

49
00:04:26.976 --> 00:04:31.990
theoretical output is,
it explained all of these things so

50
00:04:31.990 --> 00:04:38.128
that you could build practical
systems using these advanced concepts.