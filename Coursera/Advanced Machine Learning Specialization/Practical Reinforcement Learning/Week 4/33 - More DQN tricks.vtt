WEBVTT

1
00:00:00.000 --> 00:00:06.670
Now, another huge problem would be

2
00:00:06.670 --> 00:00:14.540
DQN is that it actually tries to approximate a set of values that are very interrelated.

3
00:00:14.540 --> 00:00:16.080
This is this point. Let's watch this video,

4
00:00:16.080 --> 00:00:17.180
and in this video,

5
00:00:17.180 --> 00:00:19.670
I want you to take a closer look in how

6
00:00:19.670 --> 00:00:23.320
the Q values change on the bottom-left part of the screen.

7
00:00:23.320 --> 00:00:26.810
I want you to find the segments where they are

8
00:00:26.810 --> 00:00:30.455
more or less equal and the same as they differ as much as possible.

9
00:00:30.455 --> 00:00:36.090
Now, there will be a quiz here to check out that you've made it through the video.

10
00:00:36.230 --> 00:00:40.855
So, what you might have noticed is the fact that most of the time,

11
00:00:40.855 --> 00:00:46.145
especially if the ball in breakout is on the opposite side of the game field,

12
00:00:46.145 --> 00:00:51.695
you'll have Q values for actions more or less on the same value.

13
00:00:51.695 --> 00:00:54.085
This is because in this state,

14
00:00:54.085 --> 00:00:57.205
one action, even a stupid action won't change anything.

15
00:00:57.205 --> 00:01:00.900
Even if you make one bad move while the ball is on the other side of the field,

16
00:01:00.900 --> 00:01:06.100
you'll still have plenty of time to adjust to go the right way and fix the issue.

17
00:01:06.100 --> 00:01:08.405
Therefore, all Q values are more less the same.

18
00:01:08.405 --> 00:01:12.100
There's this common parts which all of those action values have.

19
00:01:12.100 --> 00:01:17.045
This is the state value by the definition we have introduced it previously.

20
00:01:17.045 --> 00:01:20.935
Now, there are also situations where the Q values differ a lot.

21
00:01:20.935 --> 00:01:24.190
These are the cases where one action can make it or break it.

22
00:01:24.190 --> 00:01:29.650
Maybe the ball is approaching your player here, batch or whatever,

23
00:01:29.650 --> 00:01:32.520
your platform, and if you move, say, to the right,

24
00:01:32.520 --> 00:01:36.970
you're going to move at just the right position to catch it and so it bounces off.

25
00:01:36.970 --> 00:01:39.865
If you don't, you'll just miss the ball and lose one life.

26
00:01:39.865 --> 00:01:44.465
So, there are rare cases where those Q values are highly different.

27
00:01:44.465 --> 00:01:48.220
The problem here is that we are considering them as

28
00:01:48.220 --> 00:01:51.955
more or less independent predictions that we would train our network.

29
00:01:51.955 --> 00:01:54.010
So, let's then try to introduce some of

30
00:01:54.010 --> 00:01:57.305
this intuition into how we train the Q network to see if it helps.

31
00:01:57.305 --> 00:01:58.800
This brings us to another architecture.

32
00:01:58.800 --> 00:02:01.050
It's called the dueling deep Q network.

33
00:02:01.050 --> 00:02:03.430
The first thing we have to do is we have to decompose

34
00:02:03.430 --> 00:02:05.860
our Q(s,a), the action value function.

35
00:02:05.860 --> 00:02:09.520
This time we rewrite it as a sum of the state value function,

36
00:02:09.520 --> 00:02:12.295
V(s) and it only depends on the state,

37
00:02:12.295 --> 00:02:15.520
and the neutral which is this capital A(s,a).

38
00:02:15.520 --> 00:02:19.090
The capital A here is the advantage function,

39
00:02:19.090 --> 00:02:21.460
and the intuition here is the advantage is how

40
00:02:21.460 --> 00:02:24.325
much you're action value differs from the state value.

41
00:02:24.325 --> 00:02:27.255
For example, if you have a state in which you have two actions,

42
00:02:27.255 --> 00:02:29.400
the first brings you returns of,

43
00:02:29.400 --> 00:02:32.115
say, plus 100 and the second is plus 1.

44
00:02:32.115 --> 00:02:34.405
Say, you are in a room and you have two doorways.

45
00:02:34.405 --> 00:02:35.860
The first one leads you to a large cake,

46
00:02:35.860 --> 00:02:37.970
and the second, to a small cookie.

47
00:02:38.100 --> 00:02:40.600
After you take each of those actions,

48
00:02:40.600 --> 00:02:42.025
the other opportunity gets lost.

49
00:02:42.025 --> 00:02:45.160
Say, a door closes on the option you have not picked.

50
00:02:45.160 --> 00:02:46.695
Now, in this case,

51
00:02:46.695 --> 00:02:50.540
both action values are positive because you get positive reward.

52
00:02:50.540 --> 00:02:53.965
Say, plus 100 for the cake and plus 1 for the cookie.

53
00:02:53.965 --> 00:02:57.580
The advantages on the contrary are going to be different.

54
00:02:57.580 --> 00:03:01.480
Some of the advantage of the suboptimal action is going to be negative.

55
00:03:01.480 --> 00:03:04.570
This is because if you take this suboptimal action,

56
00:03:04.570 --> 00:03:09.010
then you get the action value of plus 1 minus the state value of plus 100,

57
00:03:09.010 --> 00:03:10.725
which is minus 99.

58
00:03:10.725 --> 00:03:13.090
Basically, it tells you that you have just lost

59
00:03:13.090 --> 00:03:18.145
99 potential units of rewards in this case.

60
00:03:18.145 --> 00:03:25.100
Now, the definition here suggests that the value function that we use is the V star,

61
00:03:25.100 --> 00:03:27.420
the value of our optimal action.

62
00:03:27.420 --> 00:03:31.000
But you can substitute it to any definition of value function so

63
00:03:31.000 --> 00:03:34.770
long as it corresponds with the Q function and you understand what you're doing.

64
00:03:34.770 --> 00:03:37.090
The way we're going to introduce this intuition

65
00:03:37.090 --> 00:03:39.675
into a neural network is basically this way.

66
00:03:39.675 --> 00:03:41.280
We have the usual DQN,

67
00:03:41.280 --> 00:03:44.040
which simply tries to [inaudible] all Q functions,

68
00:03:44.040 --> 00:03:46.890
Q values independently via the insulator,

69
00:03:46.890 --> 00:03:51.080
and then we are going to modify it using our new decomposition.

70
00:03:51.080 --> 00:03:53.220
Now, one unit will have one hand,

71
00:03:53.220 --> 00:03:57.075
which only predicts the state value function which is just one number per state,

72
00:03:57.075 --> 00:03:59.560
and then a set of all the advantages.

73
00:03:59.560 --> 00:04:02.670
To predict those advantages, we actually have to constrain them

74
00:04:02.670 --> 00:04:05.835
in a way that satisfies the common sense of reinforcement learning.

75
00:04:05.835 --> 00:04:08.175
In case of V star,

76
00:04:08.175 --> 00:04:11.910
the maximum possible advantage value is zero because you can never

77
00:04:11.910 --> 00:04:14.005
get action value which is larger than

78
00:04:14.005 --> 00:04:16.575
maximum over all possible action values from the state.

79
00:04:16.575 --> 00:04:21.170
This is basically the definition of the state value of the optimal policy.

80
00:04:21.170 --> 00:04:24.940
Now, what you do is you train those two halves

81
00:04:24.940 --> 00:04:29.550
separately then you just add them up to get your action values.

82
00:04:29.550 --> 00:04:32.325
In fact, you do the opposite thing.

83
00:04:32.325 --> 00:04:33.720
You train them together.

84
00:04:33.720 --> 00:04:37.410
You basically add them up and you minimize the same temporal difference error,

85
00:04:37.410 --> 00:04:39.750
which we used in the usual DQN,

86
00:04:39.750 --> 00:04:43.770
the mean squared error between Q value and improved Q value.

87
00:04:43.770 --> 00:04:49.855
This basically, it starts neural network to approach this problem the right way.

88
00:04:49.855 --> 00:04:53.720
By right, I mean that it should have some separate neurons that's

89
00:04:53.720 --> 00:04:59.105
only solve the problem of how good is it to be in the state.

90
00:04:59.105 --> 00:05:01.115
Another [inaudible] neuron is that,

91
00:05:01.115 --> 00:05:05.285
or basically say that a particular action is better than another action.

92
00:05:05.285 --> 00:05:10.100
Now, this is basically the whole idea behind dueling DQN.

93
00:05:10.100 --> 00:05:12.075
The only difference is that you may define

94
00:05:12.075 --> 00:05:14.840
those advantages and value functions differently.

95
00:05:14.840 --> 00:05:17.930
The option we just covered is the maximization.

96
00:05:17.930 --> 00:05:21.740
You take the constraint that suggest that the maximum advantage is zero.

97
00:05:21.740 --> 00:05:23.570
We can also say that for example,

98
00:05:23.570 --> 00:05:26.730
the evaluation value should be zero by substituting them in format.

99
00:05:26.730 --> 00:05:34.890
It would roughly correspond to some of policy expected value also based action value.

100
00:05:34.890 --> 00:05:37.620
Well, it has actually state value and advantage.

101
00:05:37.620 --> 00:05:41.010
This technically makes some sense but in most cases,

102
00:05:41.010 --> 00:05:45.085
just he receive that proves to be slightly better on [inaudible] problems.

103
00:05:45.085 --> 00:05:47.090
So, here's how dueling DQM works.

104
00:05:47.090 --> 00:05:51.275
We simply introduce those two intermediate players and then we add them up,

105
00:05:51.275 --> 00:05:52.970
and by introducing them separately,

106
00:05:52.970 --> 00:05:55.535
we hid the network that is there should be and of

107
00:05:55.535 --> 00:05:59.360
two important things that may not be that much interdependent.

108
00:05:59.360 --> 00:06:02.070
So, this is the dueling DQN.

109
00:06:02.070 --> 00:06:04.805
Now, the final trick for this video is going to tackle

110
00:06:04.805 --> 00:06:08.630
another issue that we have not yet improved on since the basic Q learning.

111
00:06:08.630 --> 00:06:10.445
It's the issue of exploration.

112
00:06:10.445 --> 00:06:14.230
The problem that we have with the DQN is that,

113
00:06:14.230 --> 00:06:17.100
while it's so complicated, unilateral goal this,

114
00:06:17.100 --> 00:06:21.110
[inaudible] stuff, it still explores in a rather shallow way.

115
00:06:21.110 --> 00:06:22.710
The problem is that if you use,

116
00:06:22.710 --> 00:06:24.460
for example, Epsilon [inaudible] policy,

117
00:06:24.460 --> 00:06:27.350
then the probability of taking one action

118
00:06:27.350 --> 00:06:30.415
which is sub optimal is Epsilon divided by number of actions,

119
00:06:30.415 --> 00:06:31.980
with the probability of, say,

120
00:06:31.980 --> 00:06:36.890
taking five or 10 suboptimal actions in a row is going to be near zero,

121
00:06:36.890 --> 00:06:40.870
because basically Epsilon to the power of this amount of actions, five or 10.

122
00:06:40.870 --> 00:06:42.780
If Epsilon is 0.1,

123
00:06:42.780 --> 00:06:45.920
then you can do the math and see how small it gets.

124
00:06:45.920 --> 00:06:51.260
The problem is that sometimes it takes to actually make those bold steps,

125
00:06:51.260 --> 00:06:52.625
make a few suboptimal,

126
00:06:52.625 --> 00:06:54.680
similarly suboptimal action to discover something

127
00:06:54.680 --> 00:06:57.530
new which is not near the optimal policy but which is

128
00:06:57.530 --> 00:07:04.045
a completely new policy in a way that it approaches the entire decision process.

129
00:07:04.045 --> 00:07:07.370
The Epsilon [inaudible] strategy is very unlikely to discover this.

130
00:07:07.370 --> 00:07:10.535
It is very prone to local optimal convergence.

131
00:07:10.535 --> 00:07:13.170
There is one possible way can solve it.

132
00:07:13.170 --> 00:07:16.260
The one way which fits well with

133
00:07:16.260 --> 00:07:20.820
the deep Q network architecture is so-called Bootstrap DQN.

134
00:07:20.820 --> 00:07:25.910
The [inaudible] here is that you have to train a set of key,

135
00:07:25.910 --> 00:07:29.420
say five or 10 Q value predictors,

136
00:07:29.420 --> 00:07:33.410
that all share the same main body parts,

137
00:07:33.410 --> 00:07:35.785
so they all have same revolutional layers.

138
00:07:35.785 --> 00:07:40.580
The way they are trained is at the begin of every episode,

139
00:07:40.580 --> 00:07:41.900
you pick on them hard,

140
00:07:41.900 --> 00:07:43.490
so you basically flip a,

141
00:07:43.490 --> 00:07:46.675
you throw a dice and you pick one of those key has.

142
00:07:46.675 --> 00:07:49.545
Then you follow its actions and you

143
00:07:49.545 --> 00:07:55.160
train the weights of the head and the weights of the corresponding body.

144
00:07:55.160 --> 00:08:00.365
Then you basically do this thing for the entire episode.

145
00:08:00.365 --> 00:08:02.630
This way, you want the heads to going to be slightly

146
00:08:02.630 --> 00:08:06.545
better but also your features are going to change as well.

147
00:08:06.545 --> 00:08:09.420
Then at the beginning of the next episode,

148
00:08:09.420 --> 00:08:10.900
you pick another head.

149
00:08:10.900 --> 00:08:14.220
So you re-throw dice again and

150
00:08:14.220 --> 00:08:17.985
see what happens and then you follow the policy of this new head.

151
00:08:17.985 --> 00:08:22.605
Since those heads are not directly trained on one of these experiences,

152
00:08:22.605 --> 00:08:27.320
they're not guaranteed to be the same so far as

153
00:08:27.320 --> 00:08:31.920
the network has some possible ways to find different strategies.

154
00:08:31.920 --> 00:08:36.630
So, you can expect them until [inaudible] when they convert them to a policy,

155
00:08:36.630 --> 00:08:38.400
you can expect them to differ,

156
00:08:38.400 --> 00:08:40.980
and this difference is going to be systematic.

157
00:08:40.980 --> 00:08:44.115
So, they wont just take some optimal actions, say,

158
00:08:44.115 --> 00:08:46.245
one time out of 100,

159
00:08:46.245 --> 00:08:48.535
but they'll be fundamentally different in the way

160
00:08:48.535 --> 00:08:51.340
which we prioritize some actions over the others.

161
00:08:51.340 --> 00:08:54.260
Now, this simply repeat this process over all the episodes.

162
00:08:54.260 --> 00:08:55.835
So, you pick an episode, pick a head,

163
00:08:55.835 --> 00:08:57.050
train this head, train the body,

164
00:08:57.050 --> 00:08:58.850
pick another head, and so on.

165
00:08:58.850 --> 00:09:01.280
This process is, in fact, for DQN,

166
00:09:01.280 --> 00:09:02.959
much cheaper than training,

167
00:09:02.959 --> 00:09:04.790
say, K separate agents from scratch.

168
00:09:04.790 --> 00:09:07.680
Because most of the heavy lifting

169
00:09:07.680 --> 00:09:10.665
is actually done on this common body part, the future learning.

170
00:09:10.665 --> 00:09:15.480
While this future part is getting trained on all arbitrations,

171
00:09:15.480 --> 00:09:16.995
because all the heads are connected to it,

172
00:09:16.995 --> 00:09:20.430
then expect the overall progress to be as fast,

173
00:09:20.430 --> 00:09:22.820
almost as fast as the usual DQN.

174
00:09:22.820 --> 00:09:29.220
Maybe even faster because better exploration usually means better policy and less time.

175
00:09:29.220 --> 00:09:34.830
Now, this whole thing has a nickname of Deep Exploration Policy,

176
00:09:34.830 --> 00:09:37.230
Deep Exploration Strategies because, again,

177
00:09:37.230 --> 00:09:42.745
it is able to take a lot of correlated actions that are different from the other heads.

178
00:09:42.745 --> 00:09:47.100
But otherwise, it's still more or less a heuristic that somehow works.

179
00:09:47.100 --> 00:09:50.535
We'll link to explanations of

180
00:09:50.535 --> 00:09:55.160
this article in a greater level of detail in varying section as usual.

181
00:09:55.160 --> 00:10:01.440
Of course, you may expect to find dozens of [inaudible] architecture's,

182
00:10:01.440 --> 00:10:07.800
you may even come up with your new DQN flavor yourself because as of 2017,

183
00:10:07.800 --> 00:10:09.480
they are still getting published.

184
00:10:09.480 --> 00:10:15.990
The last one I know were from the ICML conference from this very year.

185
00:10:15.990 --> 00:10:20.730
The ideas of those architectures are usually that they spot some problems,

186
00:10:20.730 --> 00:10:22.515
some issues, some way you can improve,

187
00:10:22.515 --> 00:10:24.210
then proves this particular issue,

188
00:10:24.210 --> 00:10:25.560
which proves us two things.

189
00:10:25.560 --> 00:10:28.925
First, that the principles of learning gets developed really rapidly,

190
00:10:28.925 --> 00:10:33.975
and the second one is that the DQN architectures float in all the ways you can imagine.

191
00:10:33.975 --> 00:10:37.710
Of course, we'll find an alternative solution right next week,

192
00:10:37.710 --> 00:10:42.480
but until then, you have to get a little bit more acquainted with the DQN.