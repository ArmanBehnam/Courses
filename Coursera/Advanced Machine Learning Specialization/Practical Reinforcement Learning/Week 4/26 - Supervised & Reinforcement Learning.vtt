WEBVTT

1
00:00:02.610 --> 00:00:05.910
Welcome to Week Four of the Reinforcement Learning Course.

2
00:00:05.910 --> 00:00:09.460
During this week, we will talk about model free approximate methods,

3
00:00:09.460 --> 00:00:12.645
which are in fact used in a wide variety of practical tasks.

4
00:00:12.645 --> 00:00:16.590
However, the most practical widely-known application

5
00:00:16.590 --> 00:00:18.780
of reinforcement learning techniques nowadays,

6
00:00:18.780 --> 00:00:20.660
is playing video games.

7
00:00:20.660 --> 00:00:22.920
This is in part because video games are

8
00:00:22.920 --> 00:00:26.530
perfect small-sized and oversimplified reflections of real life,

9
00:00:26.530 --> 00:00:29.570
but also in part because current methods are not

10
00:00:29.570 --> 00:00:33.220
efficient in nature for applications to real life problems.

11
00:00:33.220 --> 00:00:38.810
There is still much that remains to be done in the field of reinforcement learning.

12
00:00:38.810 --> 00:00:40.955
But our today's topics,

13
00:00:40.955 --> 00:00:44.310
approximation methods are ambiguities in

14
00:00:44.310 --> 00:00:48.390
any reinforcement learning application whatever the target domain is,

15
00:00:48.390 --> 00:00:50.795
whether it is a video game playing,

16
00:00:50.795 --> 00:00:53.635
or rep-ranking, or anything else.

17
00:00:53.635 --> 00:00:58.160
Let me start with a quick reminder of what is data question for SARSA algorithm,

18
00:00:58.160 --> 00:01:00.590
the method you have learned in previous lectures.

19
00:01:00.590 --> 00:01:03.630
If you've probably seen this update in a slightly different form,

20
00:01:03.630 --> 00:01:08.325
but in fact this is precisely the same going average of data of current Q value.

21
00:01:08.325 --> 00:01:11.595
It is the value of a target highlighted in blue in this slide.

22
00:01:11.595 --> 00:01:14.640
That is we move our current estimate a little

23
00:01:14.640 --> 00:01:18.440
bit towards it's slightly more refined approximation.

24
00:01:18.440 --> 00:01:23.695
What is important here that in tabular methods, each Q,

25
00:01:23.695 --> 00:01:26.420
S, A, could be seen as a separate parameter,

26
00:01:26.420 --> 00:01:29.420
which was set up during the learning process.

27
00:01:29.420 --> 00:01:30.960
We update this parameter.

28
00:01:30.960 --> 00:01:33.205
And after we finish this single update,

29
00:01:33.205 --> 00:01:36.605
no other values in our table has changed,

30
00:01:36.605 --> 00:01:38.615
only this Q of S and A.

31
00:01:38.615 --> 00:01:40.160
In fact, in tabular methods,

32
00:01:40.160 --> 00:01:42.860
we have more parameters than we have states.

33
00:01:42.860 --> 00:01:45.155
That is so because for each state we have

34
00:01:45.155 --> 00:01:48.515
as many parameters as there are possible actions is at state.

35
00:01:48.515 --> 00:01:51.530
Such number of parameters should sound scary.

36
00:01:51.530 --> 00:01:54.120
Consider for example a simple Atari game.

37
00:01:54.120 --> 00:01:56.880
Number of states in this game is equal to number of

38
00:01:56.880 --> 00:02:00.715
all possible combinations of pixel colors on the screen of this game.

39
00:02:00.715 --> 00:02:04.555
And this number is all the finite by the numbers.

40
00:02:04.555 --> 00:02:08.735
It is in fact much, much larger than number of items in a universe.

41
00:02:08.735 --> 00:02:11.250
Consider also a very basic environment,

42
00:02:11.250 --> 00:02:17.030
gold cards ball, where one need to balance a ball on the card as its name suggests.

43
00:02:17.030 --> 00:02:20.845
In this environment stage includes continuous components.

44
00:02:20.845 --> 00:02:25.455
That is combination of the position of the card and the ball's angle.

45
00:02:25.455 --> 00:02:29.840
That means we need a table of infinite size just to start applying

46
00:02:29.840 --> 00:02:34.120
table of SARSA or tabular [inaudible] method for this problem.

47
00:02:34.120 --> 00:02:37.305
The problem here is not only about the storage memory.

48
00:02:37.305 --> 00:02:41.040
It is also about data needed to appropriately fill such table.

49
00:02:41.040 --> 00:02:45.250
It is also about the time needed to collect and process this data.

50
00:02:45.250 --> 00:02:51.385
This tabular methods sounds Utopia on the way apply to arbitrary task of interest.

51
00:02:51.385 --> 00:02:53.130
To learn such environments,

52
00:02:53.130 --> 00:02:54.905
we should know how to approximate.

53
00:02:54.905 --> 00:02:58.565
How to generalize, to unseen but close states,

54
00:02:58.565 --> 00:03:01.665
given some finite amount of experience.

55
00:03:01.665 --> 00:03:05.445
And this is precisely what we are going to talk about.

56
00:03:05.445 --> 00:03:10.955
One straightforward idea is to reduce number of parameters as much as possible.

57
00:03:10.955 --> 00:03:14.155
Say much, much less than number of state.

58
00:03:14.155 --> 00:03:19.010
Ideally, we would want to make number of parameters independent of number of states.

59
00:03:19.010 --> 00:03:22.010
That is if we want to approximate value function or action

60
00:03:22.010 --> 00:03:26.270
value function using some parametric model V hat or Q hat,

61
00:03:26.270 --> 00:03:29.955
whose parameters W are highlighted in red on the slide.

62
00:03:29.955 --> 00:03:34.195
This is a straightforward idea that hypothetically,

63
00:03:34.195 --> 00:03:39.585
could allow us to learn in environments with infinite and combinatorial number of states.

64
00:03:39.585 --> 00:03:42.770
Cool but nothing comes without a price.

65
00:03:42.770 --> 00:03:46.745
Now we have an interesting coupling between parameters and state.

66
00:03:46.745 --> 00:03:52.620
Any of the model parameters usually affects estimates in all possible states.

67
00:03:52.620 --> 00:03:54.780
This fact is worth keeping in mind,

68
00:03:54.780 --> 00:03:57.505
because it may be a source of many problems.

69
00:03:57.505 --> 00:04:02.190
For now, let's concentrate on how to learn this value and actual value functions.

70
00:04:02.190 --> 00:04:04.830
So we want to be able to compute value and

71
00:04:04.830 --> 00:04:09.810
actual value functions given raw state or state action pairs.

72
00:04:09.810 --> 00:04:14.075
This formulation sounds very similar to what is done in supervised learning.

73
00:04:14.075 --> 00:04:16.415
In supervised learning, we have an input X,

74
00:04:16.415 --> 00:04:18.700
I want to predict some output Y.

75
00:04:18.700 --> 00:04:23.865
What do we do is we construct a loss function that penalizes for misprediction of Y,

76
00:04:23.865 --> 00:04:30.545
and optimizes its loss over as training set that is over Brickell X payers of X and Y.

77
00:04:30.545 --> 00:04:33.110
However, it is unclear how to transfer

78
00:04:33.110 --> 00:04:36.460
this supervised learning paradigm to reinforcement learning.

79
00:04:36.460 --> 00:04:39.290
Which have not yet specified any loss function.

80
00:04:39.290 --> 00:04:41.020
And in reinforcement learning,

81
00:04:41.020 --> 00:04:44.195
we have no Brickell X payers of states in zero values.

82
00:04:44.195 --> 00:04:49.145
We have no training data because we don't have outcomes Y.

83
00:04:49.145 --> 00:04:52.270
And until an agent steps into this environment,

84
00:04:52.270 --> 00:04:54.870
we even have no inputs.

85
00:04:54.870 --> 00:04:59.025
We don't actually know what states are possible in the environment.

86
00:04:59.025 --> 00:05:01.420
We actually have nothing at all.

87
00:05:01.420 --> 00:05:04.670
So let's dig a little bit deeper into this issue,

88
00:05:04.670 --> 00:05:06.620
and develop two different approaches of

89
00:05:06.620 --> 00:05:09.315
application supervised learning to reinforcement learning.

90
00:05:09.315 --> 00:05:11.985
In fact, in addition to dynamic programming,

91
00:05:11.985 --> 00:05:14.830
there are two core approaches in reinforcement learning,

92
00:05:14.830 --> 00:05:21.280
which we will make use of and which allow our agent to learn model free.

93
00:05:21.280 --> 00:05:24.870
The first and more intuitive or to understand is

94
00:05:24.870 --> 00:05:28.835
Monte Carlo learning and the second is temporal difference learning.

95
00:05:28.835 --> 00:05:31.655
But before we jump into the Monte Carlo learning,

96
00:05:31.655 --> 00:05:35.385
let rides the definitions of failure and actual value functions.

97
00:05:35.385 --> 00:05:38.700
Recall that value function is an expectation of return,

98
00:05:38.700 --> 00:05:42.430
condition it on the current state, and action

99
00:05:42.430 --> 00:05:45.330
value function is an expectation of the return,

100
00:05:45.330 --> 00:05:49.195
condition it on the current state and action.

101
00:05:49.195 --> 00:05:52.995
This is expectations are our goals in a perfect world.

102
00:05:52.995 --> 00:05:56.690
If we know this goal for every to state and action, we're done.

103
00:05:56.690 --> 00:05:59.160
We could use any of the supervised learning methods to

104
00:05:59.160 --> 00:06:02.280
approximate these goals and once we build such approximations,

105
00:06:02.280 --> 00:06:06.185
we could use any of the reinforcement learning algorithms, you already know,

106
00:06:06.185 --> 00:06:12.440
such as SARSA expected SARSA or Q learning that is given perfect approximate functions.

107
00:06:12.440 --> 00:06:15.885
The reinforcement learning task doesn't differ from tabular setting.

108
00:06:15.885 --> 00:06:19.125
But again, we are not given these goals,

109
00:06:19.125 --> 00:06:21.470
and nobody knows them beforehand.

110
00:06:21.470 --> 00:06:24.210
Well, if nobody knows the truth,

111
00:06:24.210 --> 00:06:26.405
an approximation is inevitable.

112
00:06:26.405 --> 00:06:30.375
We could also approximate this stargaze written as expectations.

113
00:06:30.375 --> 00:06:33.285
The most straightforward way to approximate an expectation,

114
00:06:33.285 --> 00:06:36.675
is to replace it with its sample-based estimate.

115
00:06:36.675 --> 00:06:38.650
That is in case of value function,

116
00:06:38.650 --> 00:06:42.545
it could replace our goal with sample return G sub T.

117
00:06:42.545 --> 00:06:47.845
Let us denote goals as G of AS and G of S and A for convenience.

118
00:06:47.845 --> 00:06:54.475
This sample return is just sample of cumulative discounted reward gained by a Policy pi,

119
00:06:54.475 --> 00:06:57.495
from state S. Precisely in the same way,

120
00:06:57.495 --> 00:06:59.440
we can proceed with X value function.

121
00:06:59.440 --> 00:07:02.995
Just to place an expectation with its sample based estimate,

122
00:07:02.995 --> 00:07:06.530
that is cumulative discounted reward obtained by

123
00:07:06.530 --> 00:07:10.417
following policy by after making action A instead S..

124
00:07:10.417 --> 00:07:15.135
Noted both is a symbol based estimates of true goals,

125
00:07:15.135 --> 00:07:19.460
are simple numbers, which are known completely at the very end of the episode.

126
00:07:19.460 --> 00:07:22.425
I see numbers to emphasize that they do

127
00:07:22.425 --> 00:07:25.440
not depend on the parameters of our approximation model,

128
00:07:25.440 --> 00:07:29.815
they don't depend on parameters W. This assertion might seem obvious,

129
00:07:29.815 --> 00:07:33.765
but is very important for further discussion.

130
00:07:33.765 --> 00:07:35.635
So, that was super easy.

131
00:07:35.635 --> 00:07:40.175
We now have determined the training data for Supervised Learning tasks,

132
00:07:40.175 --> 00:07:42.760
we have inputs, state and actions,

133
00:07:42.760 --> 00:07:47.030
we have targets, a sample best estimates of our goals,

134
00:07:47.030 --> 00:07:50.280
so can we start learning process without worry?

135
00:07:50.280 --> 00:07:52.550
Well, both yes and no.

136
00:07:52.550 --> 00:07:56.070
Yes, because Supervised Learning task is set up properly,

137
00:07:56.070 --> 00:07:59.810
and no, because it is not the best solution available.

138
00:07:59.810 --> 00:08:02.545
For Monte Carlo approximations,

139
00:08:02.545 --> 00:08:04.915
we will need too many samples to learn in

140
00:08:04.915 --> 00:08:08.835
an environment which lasts more than several hundreds of time steps.

141
00:08:08.835 --> 00:08:13.750
This is a direct consequence of large variance inherent to Monte Carlo methods.

142
00:08:13.750 --> 00:08:19.810
Intuitively, large variance is due to the sum of very many random variables,

143
00:08:19.810 --> 00:08:21.530
that is state, rewards,

144
00:08:21.530 --> 00:08:24.995
actions, at each time step, in an environment.

145
00:08:24.995 --> 00:08:27.575
All these variables are random,

146
00:08:27.575 --> 00:08:32.655
either because scedasticity is the environment or due to scedasticity as a policy,

147
00:08:32.655 --> 00:08:36.700
such slow learning might be inappropriate for many applications,

148
00:08:36.700 --> 00:08:40.060
can we do better, or can we learn faster?

149
00:08:40.060 --> 00:08:42.305
Well, yes we can,

150
00:08:42.305 --> 00:08:45.970
all we can achieve this with the use of Temporal difference learning.

151
00:08:45.970 --> 00:08:47.670
The theory is a question of,

152
00:08:47.670 --> 00:08:50.020
which methods is better for learning,

153
00:08:50.020 --> 00:08:52.155
Temporal difference or Monte Carlo?

154
00:08:52.155 --> 00:08:54.765
Is an open question. However in practice.

155
00:08:54.765 --> 00:09:00.280
Temporal difference methods are a lot more sample efficient than Monte Carlo methods.

156
00:09:00.280 --> 00:09:05.205
And that, let us now see how to approximate in Temporal difference setting.

157
00:09:05.205 --> 00:09:07.440
First, let's derive the definitions of

158
00:09:07.440 --> 00:09:10.210
value and action value function in a convenient way,

159
00:09:10.210 --> 00:09:14.960
we enroll one iteration and it can prove the return as a sum of reward,

160
00:09:14.960 --> 00:09:17.405
and gamma times value of the next state.

161
00:09:17.405 --> 00:09:19.530
Just as we have done previously,

162
00:09:19.530 --> 00:09:23.905
we want to approximate these expectations with it's sample based estimates.

163
00:09:23.905 --> 00:09:29.060
With less term value of the next state is unknown to us, what can we do?

164
00:09:29.060 --> 00:09:32.370
Again, if we are to approximate,

165
00:09:32.370 --> 00:09:35.465
we can approximate this value too and

166
00:09:35.465 --> 00:09:39.145
use our current parametric estimate of the value of the next state.

167
00:09:39.145 --> 00:09:42.170
Now, that we approximate three times,

168
00:09:42.170 --> 00:09:44.860
we approximate the value function with parameters W,

169
00:09:44.860 --> 00:09:46.455
we approximate our target,

170
00:09:46.455 --> 00:09:49.150
the expectation with a temporal based analysis,

171
00:09:49.150 --> 00:09:52.940
and third, within this sample based estimate,

172
00:09:52.940 --> 00:09:57.110
we are approximate the value of the next state with our model of failure function,

173
00:09:57.110 --> 00:09:59.950
because our model is not a perfect one,

174
00:09:59.950 --> 00:10:03.570
this targets unlike Monte Carlo targets are biased,

175
00:10:03.570 --> 00:10:08.405
that is they are not equal to the two targets in an expectation.

176
00:10:08.405 --> 00:10:10.820
However, given enough amount of experience,

177
00:10:10.820 --> 00:10:15.015
Temporal difference methods will converge to the same estimates as much Carlo methods.

178
00:10:15.015 --> 00:10:19.370
The good news is, a Temporal difference targets otherwise to learn as it plays a game,

179
00:10:19.370 --> 00:10:23.725
we don't need to wait until the very end of the episode to update our model.

180
00:10:23.725 --> 00:10:26.805
They also have a very small variance because they depend

181
00:10:26.805 --> 00:10:30.080
only on to hosticity in reward and the next state,

182
00:10:30.080 --> 00:10:33.715
and not all of the hosticity of all rewards, and states,

183
00:10:33.715 --> 00:10:37.310
till the end of the game.

184
00:10:37.310 --> 00:10:42.830
These methods are also free of hosticitiy effect of exploratory actions made with goals,

185
00:10:42.830 --> 00:10:45.065
after the immediate next state,

186
00:10:45.065 --> 00:10:49.895
all these makes Temporal different methods very attractive.

187
00:10:49.895 --> 00:10:52.190
But again, nothing comes without a price,

188
00:10:52.190 --> 00:10:54.295
we are going to uncover some difficulties

189
00:10:54.295 --> 00:10:57.005
with Temporal different methods in a couple of flights.

190
00:10:57.005 --> 00:11:02.815
But for now, please know the targets G in Interpal different methods are not numbered,

191
00:11:02.815 --> 00:11:05.355
they are functions of parameters W,

192
00:11:05.355 --> 00:11:09.590
and it is better to denote it's dependence explicitly in the notation of goals.

193
00:11:09.590 --> 00:11:12.010
Well, the reason for not doing that,

194
00:11:12.010 --> 00:11:15.085
will become clear in a couple of slides, stay tuned.

195
00:11:15.085 --> 00:11:20.390
Up to now, we have presented a way to approximate both value and action value function,

196
00:11:20.390 --> 00:11:22.630
but you might have noticed that,

197
00:11:22.630 --> 00:11:26.040
an idea is very similar for both of them,

198
00:11:26.040 --> 00:11:28.505
the main thing, simpler in frequent presentation,

199
00:11:28.505 --> 00:11:31.930
I'm going to talk only about the action of other function, that is Q function,

200
00:11:31.930 --> 00:11:34.545
and I hope that you can figure out how to

201
00:11:34.545 --> 00:11:37.640
deal with value function by analogy, if you need to.

202
00:11:37.640 --> 00:11:40.695
To recap, we have talked about

203
00:11:40.695 --> 00:11:44.360
what inputs and outputs in supervised learning problem could be,

204
00:11:44.360 --> 00:11:47.275
but we have not yet discussed what loss is

205
00:11:47.275 --> 00:11:50.400
appropriate for such a problem of approximating value functions.

206
00:11:50.400 --> 00:11:52.135
Let's talk about loss now,

207
00:11:52.135 --> 00:11:55.850
as you may notice rewards are better in real numbers,

208
00:11:55.850 --> 00:11:59.875
and thus, returns and of course expected returns,

209
00:11:59.875 --> 00:12:01.475
are also real numbers.

210
00:12:01.475 --> 00:12:06.035
For these reasons, it is natural to impose a regression loss for learning Q function.

211
00:12:06.035 --> 00:12:09.140
There are plenty losses for regression task available,

212
00:12:09.140 --> 00:12:11.550
among them are mean squirter,

213
00:12:11.550 --> 00:12:13.590
mean absolter, and Huber loss,

214
00:12:13.590 --> 00:12:17.185
all of them could and sometimes should be applied to enforce learning problem.

215
00:12:17.185 --> 00:12:19.535
But for the sake of fixed position clearness,

216
00:12:19.535 --> 00:12:21.475
let's focus now on the loss,

217
00:12:21.475 --> 00:12:23.875
most popular in reinforcement learning literature,

218
00:12:23.875 --> 00:12:26.435
that is mean squared the error.

219
00:12:26.435 --> 00:12:30.235
The means squared discrepancy between targets,

220
00:12:30.235 --> 00:12:32.055
also known as goals,

221
00:12:32.055 --> 00:12:35.875
and our estimates, that is our loss function.

222
00:12:35.875 --> 00:12:41.225
Why should average discrepancies across all possible combinations of state and actions?

223
00:12:41.225 --> 00:12:46.050
However, we may be more interested in accurate estimation of Q function in some state,

224
00:12:46.050 --> 00:12:51.225
and be almost indifferent to the Q values of other states, for example,

225
00:12:51.225 --> 00:12:55.720
we don't care much about rarely what we do our best states,

226
00:12:55.720 --> 00:12:58.705
it is sufficient for us to know, they are bad.

227
00:12:58.705 --> 00:13:04.950
This is Rosier initial idea because estimates for each and every state and action,

228
00:13:04.950 --> 00:13:09.670
could be improved only after experiencing that state and making this action,

229
00:13:09.670 --> 00:13:14.090
but nobody want an agent to experience very bad state and made by their actions,

230
00:13:14.090 --> 00:13:18.410
over and over again just to increase precision of bad action estimates.

231
00:13:18.410 --> 00:13:20.405
The expresses idea, we multiplied

232
00:13:20.405 --> 00:13:24.340
the discrepancies by the corresponding weight of importance role.

233
00:13:24.340 --> 00:13:27.485
This weight can be sort of as the fraction of time,

234
00:13:27.485 --> 00:13:29.125
falls in current state as,

235
00:13:29.125 --> 00:13:31.040
and makes action aims a state.

236
00:13:31.040 --> 00:13:33.645
But can also the thing about these weights,

237
00:13:33.645 --> 00:13:34.940
these fractions of time,

238
00:13:34.940 --> 00:13:38.245
as probabilities of visiting the states as.

239
00:13:38.245 --> 00:13:41.205
Now making an action a by policy pie.

240
00:13:41.205 --> 00:13:45.205
Now that we cannot compute this loss using only finite experience,

241
00:13:45.205 --> 00:13:48.170
we should sum over all possible state and actions,

242
00:13:48.170 --> 00:13:52.305
but we might not know which states are in the environment at all,

243
00:13:52.305 --> 00:13:54.665
and it is also not clear how to compute

244
00:13:54.665 --> 00:13:58.240
this ways of importance for the states actually visited,

245
00:13:58.240 --> 00:14:00.360
and actions done by policy.

246
00:14:00.360 --> 00:14:03.535
Also know that, size loss formulation is in fact

247
00:14:03.535 --> 00:14:06.975
an expectation of this squared discrepancy.

248
00:14:06.975 --> 00:14:10.105
And this is so, if we assume that state and

249
00:14:10.105 --> 00:14:13.260
actions are distributed according to distribution row.

250
00:14:13.260 --> 00:14:15.325
Well, if we cannot compute the loss,

251
00:14:15.325 --> 00:14:18.105
we can approximate this loss in a sample based fashion,

252
00:14:18.105 --> 00:14:20.065
that is sample from row.

253
00:14:20.065 --> 00:14:23.495
You see reinforces learning is full of approximations,

254
00:14:23.495 --> 00:14:25.490
but how to sample from row?

255
00:14:25.490 --> 00:14:28.235
With such definition of face of importance,

256
00:14:28.235 --> 00:14:29.800
this is in fact an easy task.

257
00:14:29.800 --> 00:14:33.785
Samples of states and actions from distribution row can be obtained simply,

258
00:14:33.785 --> 00:14:38.555
by collecting visited states and actions done by the policy pie in the environment.

259
00:14:38.555 --> 00:14:42.370
When we are not allowed to do actions in the environment,

260
00:14:42.370 --> 00:14:44.320
that is if we learn of policy,

261
00:14:44.320 --> 00:14:49.190
we can grab samples S and A from experience generated by behavior distributions.

262
00:14:49.190 --> 00:14:51.500
It tells that these distributions,

263
00:14:51.500 --> 00:14:56.005
that row of behavior distributions and row

264
00:14:56.005 --> 00:15:01.230
of our distribution pie are close enough to provide meaningful approximation.

265
00:15:01.230 --> 00:15:04.730
Let me remind you what is off and on policy learning.

266
00:15:04.730 --> 00:15:08.150
Off policy is a set up where there are two policies,

267
00:15:08.150 --> 00:15:11.415
one that makes actions in an environment and this is called,

268
00:15:11.415 --> 00:15:14.075
Behaviour policy, and one that we are

269
00:15:14.075 --> 00:15:17.775
interested in and over which we have a control, a Target policy.

270
00:15:17.775 --> 00:15:22.385
The Target policy is updated and improved to become closer and closer to optimal policy.

271
00:15:22.385 --> 00:15:26.595
The Behaviour policy is out of our control,

272
00:15:26.595 --> 00:15:28.195
it could change over time,

273
00:15:28.195 --> 00:15:29.670
but it could not.

274
00:15:29.670 --> 00:15:35.920
We only require Behaviour policy to have non-zero probabilities of making actions,

275
00:15:35.920 --> 00:15:39.270
we have another probabilities under our Target policy.

276
00:15:39.270 --> 00:15:41.830
This assumption is required to learn,

277
00:15:41.830 --> 00:15:43.815
well in off policy scenario,

278
00:15:43.815 --> 00:15:46.835
otherwise there will be white spots in our data

279
00:15:46.835 --> 00:15:50.555
and we can't guarantee anything about Target policy at all.

280
00:15:50.555 --> 00:15:55.950
Off policy learning is a slightly easier task because in this set up,

281
00:15:55.950 --> 00:15:59.590
Target policy is the same as Behaviour policy,

282
00:15:59.590 --> 00:16:02.480
so we are in control of acting in that environment,

283
00:16:02.480 --> 00:16:05.065
and in control of changing the policy.

284
00:16:05.065 --> 00:16:09.040
But these changes, we could influence the data which we are collecting,

285
00:16:09.040 --> 00:16:11.085
and explores areas on the environment,

286
00:16:11.085 --> 00:16:15.305
which are of our interest not of somebody's interest.

287
00:16:15.305 --> 00:16:17.595
If algorithm can learn off policy,

288
00:16:17.595 --> 00:16:19.800
it also can learn on policy,

289
00:16:19.800 --> 00:16:22.160
but not the other way around.

290
00:16:22.160 --> 00:16:26.995
Off policy and On policy may seem similar to online and offline learning,

291
00:16:26.995 --> 00:16:30.240
however, in varying versions learning works online,

292
00:16:30.240 --> 00:16:32.855
and offline are more frequently used as reference

293
00:16:32.855 --> 00:16:36.175
to updating policy doings opposite that is online,

294
00:16:36.175 --> 00:16:40.640
or updating the policy only when the opposite ends, that is offline.

295
00:16:40.640 --> 00:16:44.760
So Temporal difference methods can be described as online methods,

296
00:16:44.760 --> 00:16:46.335
while Monte Carlo methods,

297
00:16:46.335 --> 00:16:49.395
can be described as a offline methods.