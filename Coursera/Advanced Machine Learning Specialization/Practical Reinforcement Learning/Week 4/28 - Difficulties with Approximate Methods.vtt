WEBVTT

1
00:00:03.380 --> 00:00:06.210
In this week, we are going to talk about

2
00:00:06.210 --> 00:00:09.860
the connection of reinforcement learning and supervised learning.

3
00:00:09.860 --> 00:00:14.040
We will also discuss the difficulties which may arise when using supervised learning,

4
00:00:14.040 --> 00:00:16.565
and reinforcement learning tasks.

5
00:00:16.565 --> 00:00:18.760
Some problems that arise in

6
00:00:18.760 --> 00:00:22.810
reinforcement learning are well known in machine learning community.

7
00:00:22.810 --> 00:00:25.570
These problems are not specific to reinforcement learning,

8
00:00:25.570 --> 00:00:28.985
but pose a significant difficulty in practice.

9
00:00:28.985 --> 00:00:32.105
So, let's discuss these problems one by one.

10
00:00:32.105 --> 00:00:36.760
Probably the most obvious problem is the so-called, curse of dimensionality.

11
00:00:36.760 --> 00:00:39.825
That is the number of states and actions grows

12
00:00:39.825 --> 00:00:43.330
exponentially with the number of variables used to describe them.

13
00:00:43.330 --> 00:00:46.120
In fact, even in relatively small problems,

14
00:00:46.120 --> 00:00:49.195
a number of unique combinations of state and action,

15
00:00:49.195 --> 00:00:51.135
may be combinatorial oblique.

16
00:00:51.135 --> 00:00:53.340
And for each of these combinations,

17
00:00:53.340 --> 00:00:55.700
we need to learn a Q function.

18
00:00:55.700 --> 00:00:58.955
In fact for example, in Atari games,

19
00:00:58.955 --> 00:01:01.540
there are general Aidyn actions available,

20
00:01:01.540 --> 00:01:07.215
and the screen of the size 210 times 180.

21
00:01:07.215 --> 00:01:09.200
This doesn't seem scary,

22
00:01:09.200 --> 00:01:13.370
but think of all possible state and action combinations.

23
00:01:13.370 --> 00:01:15.055
They're number is much,

24
00:01:15.055 --> 00:01:18.425
much larger than the number of atoms in a universe.

25
00:01:18.425 --> 00:01:21.880
And of course, for dealing with such state and action space,

26
00:01:21.880 --> 00:01:25.135
we should use methods which are sufficiently sample efficient.

27
00:01:25.135 --> 00:01:30.505
That is they must be able to generalize well to unseen pairs of state and action,

28
00:01:30.505 --> 00:01:35.825
and use all the available information in the data gathered so far.

29
00:01:35.825 --> 00:01:39.750
And of course, we need these methods to be accurate.

30
00:01:39.750 --> 00:01:41.780
That is to make meaningful decisions.

31
00:01:41.780 --> 00:01:47.415
And meaningful decisions in turn require the methods to be very flexible.

32
00:01:47.415 --> 00:01:50.110
However, any model even if it is proven to be

33
00:01:50.110 --> 00:01:52.650
universal approximator is limited in

34
00:01:52.650 --> 00:01:56.265
its modeling power when you limit the number of its parameters.

35
00:01:56.265 --> 00:02:00.595
Thus, even universal approximators in practice can be limited.

36
00:02:00.595 --> 00:02:03.795
Flexibility of the model raises another crucial question

37
00:02:03.795 --> 00:02:07.410
about what a model is capable of.

38
00:02:07.410 --> 00:02:09.450
There is the problem of overfeeding,

39
00:02:09.450 --> 00:02:10.920
and underfeeding of the data,

40
00:02:10.920 --> 00:02:13.440
also known as bias-variance tradeoff.

41
00:02:13.440 --> 00:02:16.510
Ill treatment of this problem can spoil the training,

42
00:02:16.510 --> 00:02:19.155
and make the model close to absolutely useless.

43
00:02:19.155 --> 00:02:22.280
These problems are worth to keep in mind,

44
00:02:22.280 --> 00:02:24.915
but they're not specific to reinforcement learning,

45
00:02:24.915 --> 00:02:28.150
and thus we will not go into details about this issue.

46
00:02:28.150 --> 00:02:30.850
Instead, we are going to talk about difficulties

47
00:02:30.850 --> 00:02:33.980
of supervised learning applications in reinforcement learning.

48
00:02:33.980 --> 00:02:36.315
We are going to touch several different problems.

49
00:02:36.315 --> 00:02:39.995
First, we will discuss the specificity of trading data,

50
00:02:39.995 --> 00:02:44.880
then we will turn our attention to the issue of the non-stationary data.

51
00:02:44.880 --> 00:02:47.820
And finally, we will briefly discuss one of

52
00:02:47.820 --> 00:02:52.115
the most complex and open theoretical problems in reinforcement learning research,

53
00:02:52.115 --> 00:02:55.320
which is called the deadly triad.

54
00:02:55.320 --> 00:02:58.720
Although we separate the problems between each other,

55
00:02:58.720 --> 00:03:00.885
and try to discuss them in a relation,

56
00:03:00.885 --> 00:03:03.905
in practice they often come all at once.

57
00:03:03.905 --> 00:03:07.840
This happens because the problems are deeply interconnected,

58
00:03:07.840 --> 00:03:10.855
and rarely come apart from each other.

59
00:03:10.855 --> 00:03:15.815
So, why do we say that training data in the reinforcement learning is unusual?

60
00:03:15.815 --> 00:03:18.775
Well, there are many reasons for this.

61
00:03:18.775 --> 00:03:22.130
The first reason is that training data in reinforcement learning is

62
00:03:22.130 --> 00:03:25.770
by no means independent and intensively distributed.

63
00:03:25.770 --> 00:03:28.790
Why is it so, and why is it important?

64
00:03:28.790 --> 00:03:32.240
Consider for example, the Space Invaders Atari game.

65
00:03:32.240 --> 00:03:35.155
While an agent has to prevent an invasion of

66
00:03:35.155 --> 00:03:38.855
aliens coming from top to the bottom of the screen,

67
00:03:38.855 --> 00:03:42.630
an agent could hide behind one of these three rocks,

68
00:03:42.630 --> 00:03:45.535
but aliens can break the rocks.

69
00:03:45.535 --> 00:03:49.980
Now, pretend that an agent has managed to find a quiet place just

70
00:03:49.980 --> 00:03:55.230
under the rock where it is invulnerable to the bullets of the aliens.

71
00:03:55.230 --> 00:03:57.330
And they just spend, let's say,

72
00:03:57.330 --> 00:03:59.210
ten thousands of time steps,

73
00:03:59.210 --> 00:04:02.135
until aliens eventually break that rock.

74
00:04:02.135 --> 00:04:04.690
Guess what will happen next?

75
00:04:04.690 --> 00:04:08.685
An agent forgets almost all he's supposed to learn about that environment,

76
00:04:08.685 --> 00:04:10.915
because of online entity updates.

77
00:04:10.915 --> 00:04:13.900
Now, he knows only how to hide behind the rock.

78
00:04:13.900 --> 00:04:16.005
He knows that this is a good thing to do.

79
00:04:16.005 --> 00:04:18.015
Nothing more, nothing less.

80
00:04:18.015 --> 00:04:22.380
And after the rock was broken,

81
00:04:22.380 --> 00:04:26.920
an agent must start learning almost from scratch.

82
00:04:26.920 --> 00:04:30.020
This sequential correlated fraying data could,

83
00:04:30.020 --> 00:04:32.880
and sometimes is a problem in practice.

84
00:04:32.880 --> 00:04:35.460
This correlation between symbols and training data,

85
00:04:35.460 --> 00:04:38.505
may be harmful not only because of forgetting useful features,

86
00:04:38.505 --> 00:04:40.290
and already accumulated knowledge.

87
00:04:40.290 --> 00:04:43.300
Basically, very many methods in supervised learning rely

88
00:04:43.300 --> 00:04:47.295
heavily on the training data being iid.

89
00:04:47.295 --> 00:04:49.310
When this is not the case,

90
00:04:49.310 --> 00:04:51.430
learning can be either less efficient,

91
00:04:51.430 --> 00:04:52.870
or break down completely.

92
00:04:52.870 --> 00:04:58.355
To begin an intuition why learning can be slowed down by strong correlation.

93
00:04:58.355 --> 00:05:02.585
Think of in what case there is more information?

94
00:05:02.585 --> 00:05:04.695
In case of two independent samples,

95
00:05:04.695 --> 00:05:07.960
or in case of two perfectly correlated samples?

96
00:05:07.960 --> 00:05:11.885
Obviously, the former is more useful for learning from.

97
00:05:11.885 --> 00:05:18.295
Another interesting fact is that SGD loses its convergence properties.

98
00:05:18.295 --> 00:05:21.780
In case of non-independent,

99
00:05:21.780 --> 00:05:23.650
and identical distributed data.

100
00:05:23.650 --> 00:05:27.820
So, we are no more guaranteed to converge to the optimum.

101
00:05:27.820 --> 00:05:31.930
The second reason why we say that the training data is unusual,

102
00:05:31.930 --> 00:05:35.280
is the fact of data dependence on the current policy.

103
00:05:35.280 --> 00:05:40.385
In reinforcement learning, we not only observe the actions chosen by the policy,

104
00:05:40.385 --> 00:05:43.520
but we also observe states and rewards,

105
00:05:43.520 --> 00:05:45.850
that are influenced by the policy.

106
00:05:45.850 --> 00:05:49.305
More to say, when we update the parameters of policy,

107
00:05:49.305 --> 00:05:52.105
we also change not only the distribution of factions,

108
00:05:52.105 --> 00:05:54.815
but we also change the states and rewards,

109
00:05:54.815 --> 00:05:57.660
on which we will further update our agent.

110
00:05:57.660 --> 00:06:00.765
These reasons seem rather obvious,

111
00:06:00.765 --> 00:06:02.370
and to some extent they are.

112
00:06:02.370 --> 00:06:08.205
However in practice, there are environments where an agent sometimes learns the behavior,

113
00:06:08.205 --> 00:06:12.220
that changes the incoming data stream in a way that yields

114
00:06:12.220 --> 00:06:16.780
an agent almost incapable of unlearning such vital behavior.

115
00:06:16.780 --> 00:06:19.190
Literally when this happens,

116
00:06:19.190 --> 00:06:22.220
the learning process starts almost from scratch.

117
00:06:22.220 --> 00:06:27.975
Actually, a policy changes the data stream not only due to policy updates,

118
00:06:27.975 --> 00:06:30.040
but also due to exploration.

119
00:06:30.040 --> 00:06:34.095
This agent constantly experiment with the data stream,

120
00:06:34.095 --> 00:06:38.590
and this also adds it's contribution to the learning stability.

121
00:06:38.590 --> 00:06:41.670
The third reason of why the data in reinforcement learning is a

122
00:06:41.670 --> 00:06:44.680
bit unusual is non-differentiability.

123
00:06:44.680 --> 00:06:48.330
What I mean by this is the behavior of action very function,

124
00:06:48.330 --> 00:06:52.065
when we wiggle a little bit state S, or action A.

125
00:06:52.065 --> 00:06:54.990
And then, if two states are close,

126
00:06:54.990 --> 00:06:56.825
for example, in the L2 Norm,

127
00:06:56.825 --> 00:06:59.955
it doesn't mean that the values of that states,

128
00:06:59.955 --> 00:07:03.235
either state values, or faction values are close.

129
00:07:03.235 --> 00:07:05.560
In fact, that is often not the case.

130
00:07:05.560 --> 00:07:08.190
Consider riding the car in a highway.

131
00:07:08.190 --> 00:07:11.540
When the car is arbitrary close to another car,

132
00:07:11.540 --> 00:07:14.515
everything works great, no collision happens.

133
00:07:14.515 --> 00:07:19.015
And the value of that state doesn't experience any negative effects.

134
00:07:19.015 --> 00:07:22.705
But as soon as a car touches another car,

135
00:07:22.705 --> 00:07:29.030
an accident happens and the consequences of such even light touch may be arbitrary bad.

136
00:07:29.030 --> 00:07:32.625
The same is true for the state success of in time,

137
00:07:32.625 --> 00:07:36.655
they may, but also may not have close values.

138
00:07:36.655 --> 00:07:38.875
Moreover, the same is true for action.

139
00:07:38.875 --> 00:07:40.945
Even if action is continuous,

140
00:07:40.945 --> 00:07:43.470
slight change in action may change the value of

141
00:07:43.470 --> 00:07:46.735
state action function by arbitrary amount.

142
00:07:46.735 --> 00:07:51.360
All these lead to unstable gradients and data inefficiency.

143
00:07:51.360 --> 00:07:54.105
Besides that, if you use temporal difference targets,

144
00:07:54.105 --> 00:07:56.925
you additionally become susceptible to error propagation.

145
00:07:56.925 --> 00:08:01.575
That is if you make an error in estimating the value of particular state in action,

146
00:08:01.575 --> 00:08:07.055
this error will propagate to estimates in states proceeding in time to current state.

147
00:08:07.055 --> 00:08:10.155
That is so because for these preceding states,

148
00:08:10.155 --> 00:08:13.300
we include the error in those estimate in the target.

149
00:08:13.300 --> 00:08:16.060
And our learning procedure is designed to make

150
00:08:16.060 --> 00:08:19.800
our estimate in the preceding state closer to our target.

151
00:08:19.800 --> 00:08:24.962
We will see some tricks against this error propagation in future videos.

152
00:08:24.962 --> 00:08:27.285
There is a handful of examples.

153
00:08:27.285 --> 00:08:30.195
The first example is mount and car environment.

154
00:08:30.195 --> 00:08:33.875
In this environment, one controls the car and needs to reach

155
00:08:33.875 --> 00:08:38.355
the goal which is on the high hill on the right.

156
00:08:38.355 --> 00:08:43.800
The problem is that the car is not powerful enough to reach that goal directly,

157
00:08:43.800 --> 00:08:49.145
instead it should oscillate in a forward backward fission to reach the goal.

158
00:08:49.145 --> 00:08:53.255
On the rise, there is a plot of minus the value of state,

159
00:08:53.255 --> 00:08:55.740
which is described by two float numbers,

160
00:08:55.740 --> 00:08:57.835
that is position and velocity.

161
00:08:57.835 --> 00:09:01.155
Notice that almost for each position there is

162
00:09:01.155 --> 00:09:05.515
an abrupt change in the value corresponding to some velocity.

163
00:09:05.515 --> 00:09:09.345
This mean that in a relatively high velocity may have

164
00:09:09.345 --> 00:09:15.455
a low value because the car with its velocity can come only very close to the goal,

165
00:09:15.455 --> 00:09:17.210
but not reach it.

166
00:09:17.210 --> 00:09:22.900
And a very small increment in the velocity can cause the car to reach its goal,

167
00:09:22.900 --> 00:09:25.190
which in turn corresponds to high value,

168
00:09:25.190 --> 00:09:29.805
because it receives a higher reward in that moment.

169
00:09:29.805 --> 00:09:32.990
The second example is about the helicopter.

170
00:09:32.990 --> 00:09:35.455
Just pretend an agent learning how to control

171
00:09:35.455 --> 00:09:38.335
a helicopter flying between the trees in a forest.

172
00:09:38.335 --> 00:09:42.910
Almost in all positions some of trees may be very close to the helicopter,

173
00:09:42.910 --> 00:09:44.515
and it is a key,

174
00:09:44.515 --> 00:09:46.075
it is a forest there.

175
00:09:46.075 --> 00:09:50.870
But if the helicopter change its position a bit it

176
00:09:50.870 --> 00:09:55.850
could touch the closest trunk, the closest tree.

177
00:09:55.850 --> 00:10:00.655
And that thresh will cause an instantly negative reward,

178
00:10:00.655 --> 00:10:06.530
which in turn will make barely function abrupt in the vicinity of trees trunks.

179
00:10:06.530 --> 00:10:12.105
So we have discussed why the reading data in reinforcement learning is unusual.

180
00:10:12.105 --> 00:10:16.800
But there are also some interesting problems caused by the learning task itself.

181
00:10:16.800 --> 00:10:19.355
One of the most important specificity of a learning task

182
00:10:19.355 --> 00:10:21.950
in reinforcement learning is not stationarily.

183
00:10:21.950 --> 00:10:24.935
As you might remember, almost all algorithms in

184
00:10:24.935 --> 00:10:28.285
reinforcement learning can be viewed as generalized policy thracian.

185
00:10:28.285 --> 00:10:32.080
That means that we constantly update our estimates,

186
00:10:32.080 --> 00:10:37.610
that are defined with respect to some policy which generate a data.

187
00:10:37.610 --> 00:10:41.625
However, when we like generalize policy direction change to policy,

188
00:10:41.625 --> 00:10:44.635
we also invalidates estimates of Q-values,

189
00:10:44.635 --> 00:10:46.835
and this can cause problems,

190
00:10:46.835 --> 00:10:50.820
because the change make all of our targets,

191
00:10:50.820 --> 00:10:55.565
all of the goals, that is G of S and A to become invalid.

192
00:10:55.565 --> 00:11:01.155
This is true not only for temporal difference targets but also for Monte Carlo targets.

193
00:11:01.155 --> 00:11:03.335
The allies are no longer apply because

194
00:11:03.335 --> 00:11:07.250
the changed policy updates different cumulative rewards,

195
00:11:07.250 --> 00:11:12.290
and this is really the problem in practice.

196
00:11:12.290 --> 00:11:15.275
This in turn causes nonstationary can in turn

197
00:11:15.275 --> 00:11:18.280
cause inducing problems such as oscillating behavior.

198
00:11:18.280 --> 00:11:25.125
For example, a little change in Q-values can cause drastic changes in grible disappear.

199
00:11:25.125 --> 00:11:28.855
This in turn can cause drastic changes in training data,

200
00:11:28.855 --> 00:11:33.490
which make the estimates of Q function variant precise.

201
00:11:33.490 --> 00:11:39.005
And that is so just because the all values were learned from a completely different data.

202
00:11:39.005 --> 00:11:44.450
This imprecision causes large gradients and large update in Q-values,

203
00:11:44.450 --> 00:11:48.515
but large updating Q-values causes large changes in policy.

204
00:11:48.515 --> 00:11:54.520
I hope you have noticed the feedback loop which may break learning completely.

205
00:11:54.520 --> 00:11:57.750
To make things a little bit more complex,

206
00:11:57.750 --> 00:12:03.805
note that environment in which agent performs his decisions can be nonstationary too,

207
00:12:03.805 --> 00:12:07.015
like the real world the state's transition and

208
00:12:07.015 --> 00:12:11.075
even rewards can change while time passing.

209
00:12:11.375 --> 00:12:15.355
Learning in a nonstationary environment is probably

210
00:12:15.355 --> 00:12:18.830
one of the most complex tasks in reinforcement learning.

211
00:12:18.830 --> 00:12:22.705
The last but surely not the least concern I want to touch,

212
00:12:22.705 --> 00:12:25.300
is the so-called the deadly triad.

213
00:12:25.300 --> 00:12:28.645
It seem partial explains the essence of the problem.

214
00:12:28.645 --> 00:12:35.100
There are three ingredients which if put together may lead to model divergence.

215
00:12:35.100 --> 00:12:38.135
That is divergence of pareto to infinity.

216
00:12:38.135 --> 00:12:41.295
So what are the deadly ingredients?

217
00:12:41.295 --> 00:12:43.200
The first is enough policy learning,

218
00:12:43.200 --> 00:12:45.685
that is learning parameters as one policy,

219
00:12:45.685 --> 00:12:47.930
while following another policy.

220
00:12:47.930 --> 00:12:49.930
The second is bootstrapping.

221
00:12:49.930 --> 00:12:52.510
Process of updating guess towards a guess.

222
00:12:52.510 --> 00:12:55.565
Dynamic programming and learning on

223
00:12:55.565 --> 00:12:58.900
temporal different targets are all examples of bootstrapping.

224
00:12:58.900 --> 00:13:01.665
The third ingredient is function approximation.

225
00:13:01.665 --> 00:13:06.410
That is using a model with number of parameters smaller than number of states.

226
00:13:06.410 --> 00:13:08.295
And one more time,

227
00:13:08.295 --> 00:13:11.185
if we add all these ingredients together,

228
00:13:11.185 --> 00:13:14.265
we obtain an algorithm which may diverge.

229
00:13:14.265 --> 00:13:17.345
What do I mean by may diverge?

230
00:13:17.345 --> 00:13:23.555
In fact in scientific literature there is a set of toy counter examples which

231
00:13:23.555 --> 00:13:27.605
show how in particular maker decision process

232
00:13:27.605 --> 00:13:31.525
particular algorithm diverges no matter what.

233
00:13:31.525 --> 00:13:35.415
By the way there is a counter example for Q learning too.

234
00:13:35.415 --> 00:13:41.150
However, if we remove any of the three ingredients no divergence is possible.

235
00:13:41.150 --> 00:13:42.820
Think about it for a moment.

236
00:13:42.820 --> 00:13:47.090
There exist examples of almost unconditional divergence.

237
00:13:47.090 --> 00:13:48.995
This divergence is not connected with

238
00:13:48.995 --> 00:13:53.080
whether an algorithm samples or performs full with backup.

239
00:13:53.080 --> 00:13:57.634
It is not connected with degree of brilliance software policy,

240
00:13:57.634 --> 00:14:00.935
whether a task is a control problem or a regression problem.

241
00:14:00.935 --> 00:14:03.320
It is even not connected to complexity as a model,

242
00:14:03.320 --> 00:14:07.815
because in simple linear models are shown to be susceptible to divergence.

243
00:14:07.815 --> 00:14:11.155
This counter examples that can be found in

244
00:14:11.155 --> 00:14:16.375
the scientific literature are sort of artificial and they are a little bit involved.

245
00:14:16.375 --> 00:14:19.810
So we are not going to spend there more time.

246
00:14:19.810 --> 00:14:23.865
This divergence to infinity almost never happens in practice and there are

247
00:14:23.865 --> 00:14:28.825
also a bunch of practical tricks which we believe my stabilize things up.

248
00:14:28.825 --> 00:14:31.115
But note that community of

249
00:14:31.115 --> 00:14:33.615
reinforcement learning researchers still

250
00:14:33.615 --> 00:14:36.705
miss the deeper understanding of this divergence problem.

251
00:14:36.705 --> 00:14:41.495
Its origin is it's vertical principles and how to develop methods against it.

252
00:14:41.495 --> 00:14:44.900
So, be aware of this problem existence.

253
00:14:44.900 --> 00:14:49.455
If despite we are not going to integrate deal with this problem,

254
00:14:49.455 --> 00:14:52.045
you are highly encouraged to look through

255
00:14:52.045 --> 00:14:56.860
the suggested reading to obtain more information about the deadly triad.

256
00:14:56.860 --> 00:15:04.190
So, all these problems do not seem like the ones which are possible to solve.

257
00:15:04.190 --> 00:15:08.300
Well they are really hard, but not fiddle.

258
00:15:08.300 --> 00:15:12.460
We can overcome most of them with practical tips and tricks,

259
00:15:12.460 --> 00:15:16.885
and that will be the next topic to discuss. Stay tuned.