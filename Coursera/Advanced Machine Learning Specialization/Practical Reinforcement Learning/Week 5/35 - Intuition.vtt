WEBVTT

1
00:00:02.450 --> 00:00:05.590
Welcome back to the course. So by this time,

2
00:00:05.590 --> 00:00:09.130
you've probably done some great things in the value-based method.

3
00:00:09.130 --> 00:00:11.300
You've studied how to apply Q-learning,

4
00:00:11.300 --> 00:00:13.020
maybe value iteration before that,

5
00:00:13.020 --> 00:00:16.225
and an arm checker has some bonus algorithms as usual.

6
00:00:16.225 --> 00:00:21.375
Now, the idea behind value-based methods that is easy to explain intuitively,

7
00:00:21.375 --> 00:00:23.510
is that to find an optimal action,

8
00:00:23.510 --> 00:00:26.090
you want your algorithm to learn how much reward,

9
00:00:26.090 --> 00:00:28.575
how much as a discounted do you think you will get.

10
00:00:28.575 --> 00:00:31.010
If it starts from this state,

11
00:00:31.010 --> 00:00:32.230
maybe takes this action,

12
00:00:32.230 --> 00:00:35.690
and then follows this policy or maybe optimal policy for Q-learning.

13
00:00:35.690 --> 00:00:37.940
Now, this approach is very useful,

14
00:00:37.940 --> 00:00:39.550
provided that you know this function.

15
00:00:39.550 --> 00:00:43.230
But today, we're going to study another family of methods that try to

16
00:00:43.230 --> 00:00:47.895
explicitly avoid learning the Q or V or any other value-related functions.

17
00:00:47.895 --> 00:00:51.190
To understand why, let's just use this simple example.

18
00:00:51.190 --> 00:00:54.170
Now, on the next slide will contain a simple question.

19
00:00:54.170 --> 00:00:55.970
It doesn't have any math to it,

20
00:00:55.970 --> 00:00:58.310
it's a question that a toddler can answer,

21
00:00:58.310 --> 00:01:01.405
so I want you to answer as quick as possible.

22
00:01:01.405 --> 00:01:04.270
Now, you have this breakout game here,

23
00:01:04.270 --> 00:01:06.325
and the question is, "What action do you take? Left or right?"

24
00:01:06.325 --> 00:01:13.685
Quick. Now, obviously, right in this case because it comes intuitively.

25
00:01:13.685 --> 00:01:15.420
But now, let's answer another question.

26
00:01:15.420 --> 00:01:18.815
There will be another question on the next slide,

27
00:01:18.815 --> 00:01:20.960
and this one will be slightly more mathematical,

28
00:01:20.960 --> 00:01:23.915
but it's what your reinforcement learning algorithm has to learn.

29
00:01:23.915 --> 00:01:26.345
Quickly, answer now, what's the Q function of

30
00:01:26.345 --> 00:01:29.990
this state and going to right? You have 10 seconds.

31
00:01:32.500 --> 00:01:36.510
Well, I'm still waiting.

32
00:01:38.590 --> 00:01:42.150
It's a bit harder than the previous one, isn't it?

33
00:01:42.150 --> 00:01:43.800
So the idea here is that,

34
00:01:43.800 --> 00:01:46.105
it's really easy for you to understand what to do,

35
00:01:46.105 --> 00:01:50.470
but the Q-learning doesn't explicitly learn what to do.

36
00:01:50.470 --> 00:01:53.295
It instead tries to learn what kind of a value,

37
00:01:53.295 --> 00:01:56.810
what Q function you'll get if you do this,

38
00:01:56.810 --> 00:02:01.725
and it's kind of hard, especially if you consider this applied to everyday problems.

39
00:02:01.725 --> 00:02:05.310
Let's say that you have a very simple problem of,

40
00:02:05.310 --> 00:02:07.385
whether or not do you go for coffee,

41
00:02:07.385 --> 00:02:10.070
so you can make yourself a coffee in another room.

42
00:02:10.070 --> 00:02:12.695
You can either go there and drink a coffee and then proceed,

43
00:02:12.695 --> 00:02:16.150
or you can stay here and avoid drinking coffee.

44
00:02:16.150 --> 00:02:18.360
Now, what you actually do is,

45
00:02:18.360 --> 00:02:20.370
you either feel like you want to do this,

46
00:02:20.370 --> 00:02:24.310
or you feel like you don't, and this is very simple.

47
00:02:24.310 --> 00:02:26.045
What Q-learning has to do,

48
00:02:26.045 --> 00:02:30.705
is it tries to learn the value of your entire life from this moment to the end,

49
00:02:30.705 --> 00:02:35.990
and it tries to add up all the rewards you're going to get in this day, next day,

50
00:02:35.990 --> 00:02:39.250
the day after that with some Gamma-related coefficients,

51
00:02:39.250 --> 00:02:42.195
and this is kind of impractical,

52
00:02:42.195 --> 00:02:46.490
especially because it takes to predict what's going to happen in the future.

53
00:02:46.490 --> 00:02:48.665
So when I say difficult,

54
00:02:48.665 --> 00:02:52.830
I actually mean that it's not only difficult for you to add up rewards with Gammas,

55
00:02:52.830 --> 00:02:54.960
it's also difficult for a new level to approximate.

56
00:02:54.960 --> 00:02:57.350
Or for that matter, any other algorithm.

57
00:02:57.350 --> 00:03:00.990
You have your DQN or something related to DQN,

58
00:03:00.990 --> 00:03:02.865
trying to learn the game of breakouts,

59
00:03:02.865 --> 00:03:05.835
or whether it wants to drink a cup of coffee.

60
00:03:05.835 --> 00:03:09.435
You actually have a squared error minimization problem under the hood.

61
00:03:09.435 --> 00:03:12.340
So what it tries to do, it tries to minimize the squared error between

62
00:03:12.340 --> 00:03:17.390
the predicted Q function and the temporal difference kind of corrected Q Function,

63
00:03:17.390 --> 00:03:18.630
which is on the right here.

64
00:03:18.630 --> 00:03:21.600
So basically, it's reward plus Gamma times whatever.

65
00:03:21.600 --> 00:03:23.330
And if you remember the last week,

66
00:03:23.330 --> 00:03:24.840
we actually considered this last term,

67
00:03:24.840 --> 00:03:26.950
Reward plus Gamma whatever to be constant,

68
00:03:26.950 --> 00:03:29.725
not dependent on the parameters of a Q function.

69
00:03:29.725 --> 00:03:32.765
When it comes to real world applications,

70
00:03:32.765 --> 00:03:34.950
your neuro network size is usually insufficient

71
00:03:34.950 --> 00:03:37.350
because otherwise, it will train for ages.

72
00:03:37.350 --> 00:03:40.260
What it actually means that, your neural network would never be able to

73
00:03:40.260 --> 00:03:43.250
approximate old Q values in a way that it has no error,

74
00:03:43.250 --> 00:03:47.475
so it would have some approximation error there,

75
00:03:47.475 --> 00:03:49.020
and what it actually tries to do,

76
00:03:49.020 --> 00:03:51.750
it tries to make an approximation which

77
00:03:51.750 --> 00:03:55.425
minimizes the lost function or a mean squared error in this case.

78
00:03:55.425 --> 00:04:00.170
So now, imagine that you have two possible Q functions.

79
00:04:00.170 --> 00:04:02.820
You have two possible outcomes of your learning,

80
00:04:02.820 --> 00:04:08.830
and you are considering them on two different states,

81
00:04:08.830 --> 00:04:10.700
the S zero and the S one.

82
00:04:10.700 --> 00:04:12.210
Now in those two states,

83
00:04:12.210 --> 00:04:15.020
you have two actions, A zero and A one.

84
00:04:15.020 --> 00:04:17.300
Let's imagine that you only have two actions,

85
00:04:17.300 --> 00:04:18.900
and that on all other states,

86
00:04:18.900 --> 00:04:20.370
your neuro network's identical.

87
00:04:20.370 --> 00:04:22.255
This is just for simplification.

88
00:04:22.255 --> 00:04:24.715
The first clone here,

89
00:04:24.715 --> 00:04:27.460
is the kind of true Q values,

90
00:04:27.460 --> 00:04:30.180
the Q values that your Q and A neural network would actually going to

91
00:04:30.180 --> 00:04:33.490
get if it does this particular action,

92
00:04:33.490 --> 00:04:36.095
and then follows its policy or the optimal one.

93
00:04:36.095 --> 00:04:40.995
Now, you have the first two rows corresponding to S zero.

94
00:04:40.995 --> 00:04:44.800
In this case, the first action brings you the Q function,

95
00:04:44.800 --> 00:04:46.210
the true Q function of one.

96
00:04:46.210 --> 00:04:47.890
The second one brings you two,

97
00:04:47.890 --> 00:04:50.395
and you have the second state, the S1,

98
00:04:50.395 --> 00:04:52.930
and in this case, the first action brings you three,

99
00:04:52.930 --> 00:04:56.070
and the second one brings you 100.

100
00:04:56.070 --> 00:04:59.370
This is not very practical,

101
00:04:59.370 --> 00:05:03.860
but it kind of serves the point of explaining the stuff,

102
00:05:03.860 --> 00:05:06.740
so you have two networks, two possible approximations.

103
00:05:06.740 --> 00:05:09.875
The first approximation is exact about the first state,

104
00:05:09.875 --> 00:05:14.890
is the A:, so it captures one and two Q values exactly.

105
00:05:14.890 --> 00:05:16.865
But on the second state,

106
00:05:16.865 --> 00:05:19.210
it captures first action right,

107
00:05:19.210 --> 00:05:22.945
but it fails to grasp the actual Q value of the second action.

108
00:05:22.945 --> 00:05:26.215
Then, you have the second possible option of what you can learn.

109
00:05:26.215 --> 00:05:30.880
In this case, the second state is approximately ideally, but the first state,

110
00:05:30.880 --> 00:05:33.300
the S zero has its Q values freed,

111
00:05:33.300 --> 00:05:36.540
so it has an error of plus one and minus one there.

112
00:05:36.540 --> 00:05:40.820
The question is, which of those two Q functions would you prefer?

113
00:05:40.820 --> 00:05:44.130
Which of them would get a better average of work per session,

114
00:05:44.130 --> 00:05:48.610
or which of them will take the optimal action more frequently?

115
00:05:50.470 --> 00:05:55.430
So, you've probably noticed that the option A,

116
00:05:55.430 --> 00:05:58.300
the detrian under the letter A,

117
00:05:58.300 --> 00:06:00.140
it has this innate property,

118
00:06:00.140 --> 00:06:02.175
that it doesn't give you some error.

119
00:06:02.175 --> 00:06:04.790
But if you take the optimal action, the arg maximum,

120
00:06:04.790 --> 00:06:08.920
the arg maximum is seen for this Q network and for the optimal Q function.

121
00:06:08.920 --> 00:06:13.485
Despite being slightly off in s1a1,

122
00:06:13.485 --> 00:06:15.560
it will in fact find the optimal policy here,

123
00:06:15.560 --> 00:06:18.505
unless of course, there are some other states that we have not considered.

124
00:06:18.505 --> 00:06:21.980
The second network, the network B, doesn't have this property,

125
00:06:21.980 --> 00:06:24.620
although its error is very small,

126
00:06:24.620 --> 00:06:27.630
and I think A it will take the sub-optimal action.

127
00:06:27.630 --> 00:06:30.170
So, it's ever sure to be slightly worse.

128
00:06:30.170 --> 00:06:35.295
Well, to conclude the network A, should be better.

129
00:06:35.295 --> 00:06:38.090
Let's pose another question,

130
00:06:38.090 --> 00:06:40.480
you have this square root error minimization problem.

131
00:06:40.480 --> 00:06:42.800
Which of those two options,

132
00:06:42.800 --> 00:06:46.515
A or B, will your network prefer when minimizing the square root error.

133
00:06:46.515 --> 00:06:49.060
What if the square root error is smaller?

134
00:06:49.910 --> 00:06:54.335
Yes, right. It's kind of the other way around.

135
00:06:54.335 --> 00:06:59.530
Basically, option A has the square root error of 50 squared,

136
00:06:59.530 --> 00:07:02.835
which is somewhere around too much,

137
00:07:02.835 --> 00:07:06.230
and option B only has a square root error of 2.

138
00:07:06.230 --> 00:07:11.600
So, it's like plus one squared and minus one squared added up. Now, this is the problem.

139
00:07:11.600 --> 00:07:14.050
The problem is, in that your deep Q in,

140
00:07:14.050 --> 00:07:15.920
while trying to minimize square root error,

141
00:07:15.920 --> 00:07:20.150
will avoid optimal policy and try to converge to something different,

142
00:07:20.150 --> 00:07:22.885
which is although more accurate than sense of a slow function,

143
00:07:22.885 --> 00:07:24.160
it's not what you actually want,

144
00:07:24.160 --> 00:07:25.975
you want to play optimally.

145
00:07:25.975 --> 00:07:30.120
And doing so, approximating Q function is really hard because again,

146
00:07:30.120 --> 00:07:31.380
you remember this Korff example,

147
00:07:31.380 --> 00:07:33.735
you'll have to predict what happens next.

148
00:07:33.735 --> 00:07:38.440
What if you could avoid learning this Q function or V function or ignore it?

149
00:07:38.440 --> 00:07:41.230
If you could try to approximate something else.

150
00:07:41.230 --> 00:07:44.400
In this case, why don't we just learn the policy here?

151
00:07:44.400 --> 00:07:45.680
We want the optimal policy.

152
00:07:45.680 --> 00:07:47.400
We want to take optimal actions,

153
00:07:47.400 --> 00:07:51.195
so why not just directly learn the optimal policy distribution?

154
00:07:51.195 --> 00:07:52.970
During earlier weeks of our course,

155
00:07:52.970 --> 00:07:56.380
we did have one method or a few methods if you listen to the honor check,

156
00:07:56.380 --> 00:08:00.820
we had the method that fits the definition of not learning Q or V function.

157
00:08:00.820 --> 00:08:04.360
Instead, it explicitly tries the probability of taking action in a state,

158
00:08:04.360 --> 00:08:06.230
either by the table or some approximation.

159
00:08:06.230 --> 00:08:09.130
Now, what kind of algorithm works this way?

160
00:08:09.380 --> 00:08:11.700
Yes, this is a cross-entropy method,

161
00:08:11.700 --> 00:08:13.010
and what it does,

162
00:08:13.010 --> 00:08:16.590
it simply plays some games with its current policy, then it means,

163
00:08:16.590 --> 00:08:21.030
minus the cross-entropy between the policy and the games that are more like you kind of,

164
00:08:21.030 --> 00:08:24.235
that reward better than average or better than some percentile.

165
00:08:24.235 --> 00:08:26.615
We'll cover this in more detail in just a few slides.

166
00:08:26.615 --> 00:08:29.225
Now, this algorithm had a lot of drawbacks.

167
00:08:29.225 --> 00:08:30.420
It required a lot of samples,

168
00:08:30.420 --> 00:08:31.690
it discarded a lot of samples,

169
00:08:31.690 --> 00:08:34.520
and it had things that you would

170
00:08:34.520 --> 00:08:38.130
solve with temporal difference rewards and with Q-learning,

171
00:08:38.130 --> 00:08:40.760
but it also had this neat efficiency in it.

172
00:08:40.760 --> 00:08:43.610
It learned kind of complicated environments really simple,

173
00:08:43.610 --> 00:08:46.270
in case of course, you can sample a lot of sessions in there.

174
00:08:46.270 --> 00:08:49.780
The reason behind cross-entropy method being so efficient, provided of course,

175
00:08:49.780 --> 00:08:51.460
it has a lot of samples, is because,

176
00:08:51.460 --> 00:08:52.530
it solves a simpler problem,

177
00:08:52.530 --> 00:08:55.230
it doesn't try to approximate Q function.

178
00:08:55.230 --> 00:08:57.115
It simply tries to learn what to do.

179
00:08:57.115 --> 00:09:00.950
So imagine, you're in front of a tiger and you have to decide,

180
00:09:00.950 --> 00:09:02.430
whether or not you want to pat the tiger,

181
00:09:02.430 --> 00:09:05.110
run from the tiger, provoke the tiger, or ignore the tiger.

182
00:09:05.110 --> 00:09:09.340
What Q-learning does, is it tries to estimate the quality of your life

183
00:09:09.340 --> 00:09:13.760
based on each of those choices and distinct some calculations.

184
00:09:13.760 --> 00:09:18.655
Now then, it simply picks an action which maximizes the expected quality of life.

185
00:09:18.655 --> 00:09:22.300
But in reality, before you do that, the tiger is going to eat you.

186
00:09:22.300 --> 00:09:24.250
What cross-entropy method did is,

187
00:09:24.250 --> 00:09:26.000
it simply tried to learn the probability,

188
00:09:26.000 --> 00:09:30.085
and this is kind of the thing you should do if you don't want to get eaten.

189
00:09:30.085 --> 00:09:35.610
Hopefully, you humans don't ever need Q function every time you want to make a decision.