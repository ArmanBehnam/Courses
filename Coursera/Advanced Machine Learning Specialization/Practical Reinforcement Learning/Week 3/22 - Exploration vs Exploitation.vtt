WEBVTT

1
00:00:03.060 --> 00:00:06.130
By this point now, you're probably asking yourself,

2
00:00:06.130 --> 00:00:09.010
what the hell are we still wasting your time and talking about some weird stuff.

3
00:00:09.010 --> 00:00:11.215
Of course you don't know about all those cool algorithms.

4
00:00:11.215 --> 00:00:14.420
So indeed you have learned about how do you train with Monte Carlo?

5
00:00:14.420 --> 00:00:18.610
You've learned about the Q learning as a particular keys of temporal defense learning.

6
00:00:18.610 --> 00:00:21.130
Those two algorithms are awesome but there's

7
00:00:21.130 --> 00:00:24.120
one thing into them that would prevent them from running efficiently.

8
00:00:24.120 --> 00:00:28.625
This missing link comes from a problem that you have already solved in week one.

9
00:00:28.625 --> 00:00:33.070
But let's recapitulate it on a different example to get as more kind of this to you.

10
00:00:33.070 --> 00:00:36.160
Let's say you're still trying to take a robot for a walk,

11
00:00:36.160 --> 00:00:38.800
so you want to control your robot so that it walks forward

12
00:00:38.800 --> 00:00:41.995
as fast and as harmless as possible.

13
00:00:41.995 --> 00:00:44.140
And you give it reward for based in

14
00:00:44.140 --> 00:00:46.745
the amount of distance it covers at each particular step.

15
00:00:46.745 --> 00:00:49.670
Now imagine that you are training by Q learning.

16
00:00:49.670 --> 00:00:52.205
At the beginning your Q values are zero.

17
00:00:52.205 --> 00:00:55.005
So what is going to happen is, first your robot is going to,

18
00:00:55.005 --> 00:00:59.190
there's probably high chance is going to fall because it's really hard to,

19
00:00:59.190 --> 00:01:05.355
it's really lucky situation if you'll manage to walk by a random permutations of actions.

20
00:01:05.355 --> 00:01:08.660
So the first situation is your robot falls down,

21
00:01:08.660 --> 00:01:10.365
and sneeze and maybe crawls forward.

22
00:01:10.365 --> 00:01:14.540
It still gets the reward. So it has positive Q functions for those actions.

23
00:01:14.540 --> 00:01:16.230
Now isn't it awesome?

24
00:01:16.230 --> 00:01:17.960
So it's already learn something.

25
00:01:17.960 --> 00:01:22.965
The bad part is that it's probably the entire thing it's capable to learn in this way.

26
00:01:22.965 --> 00:01:26.865
Problem here is that, if you use your Q learning in a naive way,

27
00:01:26.865 --> 00:01:30.050
in a way if you always think the action which maximizes

28
00:01:30.050 --> 00:01:33.100
the Q function then you're going to end up in this vicious circle.

29
00:01:33.100 --> 00:01:36.105
You see once your robot has fallen down

30
00:01:36.105 --> 00:01:39.835
and it has observed that falling down gives you a sample is to failure,

31
00:01:39.835 --> 00:01:41.660
the situation is that your,

32
00:01:41.660 --> 00:01:44.000
falling down strategy gives you positive value,

33
00:01:44.000 --> 00:01:46.075
while any other action is zero.

34
00:01:46.075 --> 00:01:51.325
So by definition, you're always falling down because it's the optimal action you know of.

35
00:01:51.325 --> 00:01:55.060
Now, of course this is not right, this is not right because your active,

36
00:01:55.060 --> 00:01:57.155
is that your Q function is not perfect.

37
00:01:57.155 --> 00:02:00.780
So basically the problem here is that your robot is kind of

38
00:02:00.780 --> 00:02:04.700
greedily exploiting whatever the knowledge he's able to obtain,

39
00:02:04.700 --> 00:02:07.520
and he's not trying to get more knowledge even though

40
00:02:07.520 --> 00:02:09.430
some other actions might turn out to be even

41
00:02:09.430 --> 00:02:12.300
more optimal that he thinks of his current optimal action.

42
00:02:12.300 --> 00:02:15.680
So this issue is very kind of ancient,

43
00:02:15.680 --> 00:02:19.420
and is basically comes down to the problem of exploring environment,

44
00:02:19.420 --> 00:02:21.160
versus trying to utilize what it

45
00:02:21.160 --> 00:02:23.480
can launch about the environment to get the highest reward.

46
00:02:23.480 --> 00:02:28.480
You've actually as I've mentioned solved this two weeks ago if the memory tells me right.

47
00:02:28.480 --> 00:02:32.020
And in that case, you've used a very duct tapish method

48
00:02:32.020 --> 00:02:36.890
which is not going to be that far from the optimal one. What did you do?

49
00:02:36.890 --> 00:02:44.055
Yeah, right. In case of the better strategy what you do is,

50
00:02:44.055 --> 00:02:46.390
you simply allowed your algorithm to

51
00:02:46.390 --> 00:02:49.020
sometimes run one of the best action but they aren't one one.

52
00:02:49.020 --> 00:02:52.580
In this case what you have to do is for this environment,

53
00:02:52.580 --> 00:02:56.605
you have to introduce some kind of probability with each to take a random action,

54
00:02:56.605 --> 00:02:59.100
because if you always take optimal actions,

55
00:02:59.100 --> 00:03:01.025
you're going to be stuck in this local optimal.

56
00:03:01.025 --> 00:03:04.950
Now again, the problem of exploration exploitation is of course much

57
00:03:04.950 --> 00:03:08.490
more complicated than the way it's postulated and has much more advanced solutions.

58
00:03:08.490 --> 00:03:11.300
Well this new arrow only going to consider the bare minimum.

59
00:03:11.300 --> 00:03:13.725
The same is research exploration strategies.

60
00:03:13.725 --> 00:03:16.990
They you have some guarantees of conversions but we only introduce them

61
00:03:16.990 --> 00:03:21.040
because that's how we can get the problem of exploration out of the way ASAP.

62
00:03:21.040 --> 00:03:23.480
Now, of course they'll be much more of that stuff but

63
00:03:23.480 --> 00:03:26.060
it's coming to you during the last week of our course.

64
00:03:26.060 --> 00:03:29.280
The simple six exploration strategy that you've probably

65
00:03:29.280 --> 00:03:32.835
already discovered is the so-called espilon greedy exploration.

66
00:03:32.835 --> 00:03:36.035
The espilon here is a probability of taking random action.

67
00:03:36.035 --> 00:03:39.070
So whenever an agent is tasked to pick an action,

68
00:03:39.070 --> 00:03:44.240
what he does is it basically flips a coin or well and it roll certain number with

69
00:03:44.240 --> 00:03:46.460
probability epsilon It takes an action

70
00:03:46.460 --> 00:03:49.845
uniformly sampled from the set of all possible actions.

71
00:03:49.845 --> 00:03:52.905
Otherwise, you just takes the that add marks of the Q function.

72
00:03:52.905 --> 00:03:55.710
Typical values of epsilon are again, point one,

73
00:03:55.710 --> 00:03:58.585
point zero five, point even less than the problem demands it.

74
00:03:58.585 --> 00:04:01.455
The exploration algorithm however has a few drawbacks.

75
00:04:01.455 --> 00:04:03.985
They're quite easy to distinguish once you

76
00:04:03.985 --> 00:04:06.755
start to think how this guy is going to take decisions.

77
00:04:06.755 --> 00:04:09.240
In fact, all the possible actions except

78
00:04:09.240 --> 00:04:11.550
of small ones are equally probable for it to explore,

79
00:04:11.550 --> 00:04:14.985
and they're in a way equally attractive. This of course not true.

80
00:04:14.985 --> 00:04:18.090
In a way if you're trying to find a way to work forward,

81
00:04:18.090 --> 00:04:19.490
you can try it another way you say,

82
00:04:19.490 --> 00:04:20.920
a frowning or maybe jumping,

83
00:04:20.920 --> 00:04:25.315
some other strategy of going forward to perhaps get at more reward per second.

84
00:04:25.315 --> 00:04:29.320
But for espilon strategy is just as likely to try sitting

85
00:04:29.320 --> 00:04:33.940
down on your back for like one hundredth time just because the probability is equal.

86
00:04:33.940 --> 00:04:36.890
You can modify this logic by using another strategy,

87
00:04:36.890 --> 00:04:39.330
it's called the Boltzmann exploration strategy.

88
00:04:39.330 --> 00:04:42.935
The need here is that you seem to try to link your Q values,

89
00:04:42.935 --> 00:04:46.360
action values, into some kind of probability distribution and samples from them.

90
00:04:46.360 --> 00:04:49.120
As you probably remember from deep learning course or any of

91
00:04:49.120 --> 00:04:51.920
the advance machine messaging in the course whatsoever whenever you want

92
00:04:51.920 --> 00:04:54.350
to make something into probability distribution what you do

93
00:04:54.350 --> 00:04:57.220
is you apply some linearity in this case softmax.

94
00:04:57.220 --> 00:05:00.535
So we take an exponent of each Q function,

95
00:05:00.535 --> 00:05:03.195
then divided by the sum of exponents.

96
00:05:03.195 --> 00:05:06.360
You'll also multiply Q function by this factor of Tau.

97
00:05:06.360 --> 00:05:08.110
In this case if Tau is very

98
00:05:08.110 --> 00:05:11.050
large then all the Q values are going to be approximately equal,

99
00:05:11.050 --> 00:05:14.195
and the probability is going to be almost linear form.

100
00:05:14.195 --> 00:05:17.300
If the T, if the Tau however approaches zero,

101
00:05:17.300 --> 00:05:18.650
you'll get the strategy which is

102
00:05:18.650 --> 00:05:22.860
more and more resembling of the greedy optimal action picking strategy.

103
00:05:22.860 --> 00:05:26.455
Now this does sound a little bit more sophisticated,

104
00:05:26.455 --> 00:05:29.160
but in fact you can as easily think up a way

105
00:05:29.160 --> 00:05:32.510
where this optimous policy is worse than the epsilon greedy one.

106
00:05:32.510 --> 00:05:36.145
For example, if you have an action which is under explored,

107
00:05:36.145 --> 00:05:38.775
it has zero action value because you've never tried it,

108
00:05:38.775 --> 00:05:41.240
and other two actions which are inferior but they have

109
00:05:41.240 --> 00:05:42.620
positive values because that's

110
00:05:42.620 --> 00:05:45.250
the kind of reward you get, you'll always get positive reward.

111
00:05:45.250 --> 00:05:49.360
Well, the issue here is you're probably better off you've got feelings.

112
00:05:49.360 --> 00:05:52.980
So if you feel that this policy is special, this well,

113
00:05:52.980 --> 00:05:57.340
softmax one or if you're feeling like you can adjust media issue of those to policies,

114
00:05:57.340 --> 00:05:59.520
and the experiment shows that you're right,

115
00:05:59.520 --> 00:06:02.110
then that's the optimal way you can do,

116
00:06:02.110 --> 00:06:03.645
you can solve this problem so far.

117
00:06:03.645 --> 00:06:08.205
Again we have a more advanced topics about this one later on in the course.

118
00:06:08.205 --> 00:06:11.490
Now the huge issue with those coalition policy if it does actually

119
00:06:11.490 --> 00:06:15.400
affect their a sintogic as their algorithm properties in a negative way.

120
00:06:15.400 --> 00:06:17.820
The fact that under I'll say epsilon the strategy here,

121
00:06:17.820 --> 00:06:19.690
you're always going to explore.

122
00:06:19.690 --> 00:06:24.015
So the problem here is that if you have an epsilon of say point one,

123
00:06:24.015 --> 00:06:26.030
then you'll never be able to always take

124
00:06:26.030 --> 00:06:28.200
optimal action and later be able to behave optimally

125
00:06:28.200 --> 00:06:32.945
because you're always forced to flip a coin and see if tails, then explore.

126
00:06:32.945 --> 00:06:36.040
If you know that by design your problem is solvable,

127
00:06:36.040 --> 00:06:39.080
so the solutions will eventually convert an optimal solution.

128
00:06:39.080 --> 00:06:41.020
The much you can do is you can decrease

129
00:06:41.020 --> 00:06:44.610
the epsilon presented at start with a large value of point 5 because the beginning,

130
00:06:44.610 --> 00:06:47.780
the agent knows nothing like don't know and it's no point in trying

131
00:06:47.780 --> 00:06:53.270
to listen to its false Q values any better than the random ones.

132
00:06:53.270 --> 00:06:56.935
Then after the as the training goes by,

133
00:06:56.935 --> 00:06:59.940
you can gradually reduce it for example by multiplying this point five

134
00:06:59.940 --> 00:07:04.010
by point 99 each time you finish a full session for example.

135
00:07:04.010 --> 00:07:07.650
Then by the time you finish say 100 or a few hundred sessions,

136
00:07:07.650 --> 00:07:10.370
the epsilon is going to be really close to zero.

137
00:07:10.370 --> 00:07:13.620
And in the Infinity mathematically speaking,

138
00:07:13.620 --> 00:07:17.865
the algorithm is going to behave greedily because the epsilon coverts it to zero.

139
00:07:17.865 --> 00:07:21.650
Now, this is how you technically reason away with the fact that

140
00:07:21.650 --> 00:07:25.685
you're exploring all the time but also going to exploit eventually.

141
00:07:25.685 --> 00:07:30.010
But this pressure is really dangerous when you apply it hazardly.

142
00:07:30.010 --> 00:07:31.780
For example, imagine that you're

143
00:07:31.780 --> 00:07:34.440
having an environment that changes it's properties over time.

144
00:07:34.440 --> 00:07:37.005
You're having the binner ads or prediction problem,

145
00:07:37.005 --> 00:07:39.825
and you're a key audience,

146
00:07:39.825 --> 00:07:42.860
it changes because there are more people approaching

147
00:07:42.860 --> 00:07:46.105
your new service over time and the distribution changes.

148
00:07:46.105 --> 00:07:48.770
In this case, it's never a good idea to completely

149
00:07:48.770 --> 00:07:51.270
throw away exploration because you have to adapt,

150
00:07:51.270 --> 00:07:53.025
you have to train your network,

151
00:07:53.025 --> 00:07:56.150
and without using epsilon or any other exploration means,

152
00:07:56.150 --> 00:07:58.745
you have no way of getting away from the local optimum.

153
00:07:58.745 --> 00:08:04.880
This is the kind of the definitive guide into exploration in two minutes or less.

154
00:08:04.880 --> 00:08:07.130
And if you want to get to more about exploration

155
00:08:07.130 --> 00:08:09.555
which is in neither super or complicated problem,

156
00:08:09.555 --> 00:08:13.180
I encourage you to survive until the last week of the course.