WEBVTT

1
00:00:02.870 --> 00:00:06.650
This brings us to those two huge problems of algorithms,

2
00:00:06.650 --> 00:00:08.570
the on-policy and off-policy ones.

3
00:00:08.570 --> 00:00:10.300
You might want to remember those terms.

4
00:00:10.300 --> 00:00:12.820
So, let's kind of recap on how they

5
00:00:12.820 --> 00:00:15.495
work because the main tuition is what we just covered.

6
00:00:15.495 --> 00:00:17.260
The on-policy aggression is like SARSA,

7
00:00:17.260 --> 00:00:21.740
they assume that their experience comes from the agents himself,

8
00:00:21.740 --> 00:00:26.500
and they try to improve agents policy right down this online stop.

9
00:00:26.500 --> 00:00:29.440
So, the agent plays and then it improves and plays again.

10
00:00:29.440 --> 00:00:33.920
And what you want to do is you want to get optimal Strategies as quickly as possible.

11
00:00:33.920 --> 00:00:36.730
Off-policy algorithms like Q-learning,

12
00:00:36.730 --> 00:00:40.560
as an example, they have slightly relax situation.

13
00:00:40.560 --> 00:00:44.935
They don't assume that the sessions you're obtained, you're training on,

14
00:00:44.935 --> 00:00:48.290
are the ones that you're going to use when the agent is going to

15
00:00:48.290 --> 00:00:52.605
finally get kind of unchained and applied to the actual problem.

16
00:00:52.605 --> 00:00:56.425
For example, you may train your agents to,

17
00:00:56.425 --> 00:01:00.220
well, to find a policy for a bipedal robot to walk forward.

18
00:01:00.220 --> 00:01:01.820
And this case, for example,

19
00:01:01.820 --> 00:01:04.840
you might use a 3D simulation in which you

20
00:01:04.840 --> 00:01:07.940
train your Q function with any exploration you want.

21
00:01:07.940 --> 00:01:14.800
So, you can use this cheap virtual reality simulation which you train your robots,

22
00:01:14.800 --> 00:01:20.210
and in which you are not suffering any cost from your robot falling down.

23
00:01:20.210 --> 00:01:22.150
So, you can set a latch epsilon,

24
00:01:22.150 --> 00:01:24.725
you can train for a long time without any regret,

25
00:01:24.725 --> 00:01:26.595
except for maybe CPU power.

26
00:01:26.595 --> 00:01:31.150
And then you want your agent to train not how to find,

27
00:01:31.150 --> 00:01:33.080
not how to behave optimally with

28
00:01:33.080 --> 00:01:36.390
this exploration latch Epson but how to find an optimal policy one.

29
00:01:36.390 --> 00:01:39.460
It's always asked to be an optimal action.

30
00:01:39.460 --> 00:01:41.590
So, basically, it first trains for

31
00:01:41.590 --> 00:01:44.835
this Epsilon based or maybe Boltzmann based exploration.

32
00:01:44.835 --> 00:01:48.755
And then the exploration goes away and it gets to pick optimal actions all the time.

33
00:01:48.755 --> 00:01:51.900
Another possible solution with off-policy algorithms in which Q-learning is

34
00:01:51.900 --> 00:01:55.800
also kind of a good way to improve

35
00:01:55.800 --> 00:01:58.885
your enforce learning agent is where you are

36
00:01:58.885 --> 00:02:02.930
training your agent on a policy which is different from his current policy.

37
00:02:02.930 --> 00:02:05.310
For example, you're trying

38
00:02:05.310 --> 00:02:10.480
to teach your self-driving car to drive in a particular situation,

39
00:02:10.480 --> 00:02:12.940
and you pre-train it not on

40
00:02:12.940 --> 00:02:16.995
its own actions but on actions of a driver of an actual human pilot.

41
00:02:16.995 --> 00:02:21.530
In this case, you're probably saving yourself for a lot of money and maybe some miles,

42
00:02:21.530 --> 00:02:23.990
something to use in fact.

43
00:02:23.990 --> 00:02:26.010
The issue here is that at the beginning,

44
00:02:26.010 --> 00:02:29.195
your agent is not that close optimal policies

45
00:02:29.195 --> 00:02:33.915
that you're ready to trust him control of an actual car.

46
00:02:33.915 --> 00:02:37.020
So you have actions that are not always optimal

47
00:02:37.020 --> 00:02:40.480
because humans are not having the perfect reaction time,

48
00:02:40.480 --> 00:02:43.230
but they are sometimes optional and you want your agent

49
00:02:43.230 --> 00:02:46.325
to change how to behave optimally from a human,

50
00:02:46.325 --> 00:02:48.220
kind of in theory or input.

51
00:02:48.220 --> 00:02:52.310
This can also be easily traced from the chief worlds but

52
00:02:52.310 --> 00:02:54.735
this is actually one where you can

53
00:02:54.735 --> 00:02:57.760
just improve through the premise of your algorithms later on.

54
00:02:57.760 --> 00:03:01.990
So, again have a Q-learning SARSA and expect value SARSA.

55
00:03:01.990 --> 00:03:04.300
So we just covered how Q-learning and

56
00:03:04.300 --> 00:03:07.800
SARSA relate to those on-policy, off-policy duologies.

57
00:03:07.800 --> 00:03:11.700
Now, the only thing your remaining is how expected value SARSA works.

58
00:03:11.700 --> 00:03:16.400
The question here is, can you train expected value SARSA in an off-policy scenario.

59
00:03:16.400 --> 00:03:18.700
Can you train on actions that are not,

60
00:03:18.700 --> 00:03:22.270
if not the actions taken under its current policy?

61
00:03:22.270 --> 00:03:24.940
Maybe. Can you adjust it for, maybe,

62
00:03:24.940 --> 00:03:26.210
training on human data,

63
00:03:26.210 --> 00:03:29.170
pre-training on before training on its own sessions.

64
00:03:29.170 --> 00:03:31.140
Well, right. You can.

65
00:03:31.140 --> 00:03:33.850
The issue with expected value SARSA that there is expectation,

66
00:03:33.850 --> 00:03:38.860
and you are free to set the per viewed distribution for the expectation anyway you want.

67
00:03:38.860 --> 00:03:40.890
If you set a, well,

68
00:03:40.890 --> 00:03:45.610
an expectation over what resembles the human per views of taking actions,

69
00:03:45.610 --> 00:03:49.485
if you train up with human policy and if you use its probabilities to pick an action,

70
00:03:49.485 --> 00:03:52.145
I will probably get an on-policy algorithm.

71
00:03:52.145 --> 00:03:55.230
If you take Epsilon grading policy and set

72
00:03:55.230 --> 00:03:56.785
an epsilon to something small that

73
00:03:56.785 --> 00:03:58.875
you're actually going to use after you play the algorithm,

74
00:03:58.875 --> 00:04:02.100
or even zero, you'll get something that's a lot like Q-learning,

75
00:04:02.100 --> 00:04:04.725
but it accounts for a different policy,

76
00:04:04.725 --> 00:04:07.715
one that picks optimal action that said probability.

77
00:04:07.715 --> 00:04:11.740
And basically it's the most universal version.

78
00:04:11.740 --> 00:04:13.725
You set an expectation,

79
00:04:13.725 --> 00:04:16.220
if you set up a beautiful optimal action of one,

80
00:04:16.220 --> 00:04:21.740
you'll get expected value SARSA exactly equal to Q-learning, and therefore, off-policy.

81
00:04:21.740 --> 00:04:26.970
Okay, so, there's also this neat question of whether the Crossentropy method,

82
00:04:26.970 --> 00:04:32.155
the first reinforcement training methods we ever studied relates to honor of policy.

83
00:04:32.155 --> 00:04:38.885
And this is slightly controversial but I want you to take a try at this one.

84
00:04:38.885 --> 00:04:44.670
Well again, it's kind of strange but the issue here is that

85
00:04:44.670 --> 00:04:49.425
the present method technically requires you to sample sessions from your current policy.

86
00:04:49.425 --> 00:04:54.250
You can of course modify it in some way to allow it for some different strategies.

87
00:04:54.250 --> 00:04:57.070
But if you change it on a policy which is clearly sub-optimal,

88
00:04:57.070 --> 00:04:59.550
if you always pick samples from that policy,

89
00:04:59.550 --> 00:05:02.915
then there's no way you're going to improve the,

90
00:05:02.915 --> 00:05:06.020
well, the selected elite sessions based on that.

91
00:05:06.020 --> 00:05:08.125
So technically it's on policy only.

92
00:05:08.125 --> 00:05:11.870
Now let's see how we're going to exploit this issue of on-policy,

93
00:05:11.870 --> 00:05:15.430
off-policy algorithms to get some benefits for our practical problems.

94
00:05:15.430 --> 00:05:17.590
It's a very famous sticking in the first Q-learning,

95
00:05:17.590 --> 00:05:19.190
in fact you might have heard about it.

96
00:05:19.190 --> 00:05:20.945
The name is experience replay,

97
00:05:20.945 --> 00:05:24.000
and most people associated with new latch for

98
00:05:24.000 --> 00:05:27.225
based method but it's kind of generic. It can be applied anywhere.

99
00:05:27.225 --> 00:05:30.530
The idea here is that you can train your agent not

100
00:05:30.530 --> 00:05:34.610
just on immediate state actual rewards next state players.

101
00:05:34.610 --> 00:05:37.640
You can actually record its previous interaction,

102
00:05:37.640 --> 00:05:42.755
and change intuitively on sessions that are samples from this large pool.

103
00:05:42.755 --> 00:05:45.670
So you're playing a game, say again you're trying

104
00:05:45.670 --> 00:05:49.360
to make that can be strong but to go forward without falling,

105
00:05:49.360 --> 00:05:53.610
and instead of making one update on every step,

106
00:05:53.610 --> 00:05:57.155
what you do is you record saying 10,000 previous steps,

107
00:05:57.155 --> 00:06:01.035
and you sample a hundred random transitions there from this pool,

108
00:06:01.035 --> 00:06:03.115
this huge cylinder here.

109
00:06:03.115 --> 00:06:06.455
And you make usual Q value update,

110
00:06:06.455 --> 00:06:13.870
the QSA equals alpha times new Q value plus one month of the times old Q value,

111
00:06:13.870 --> 00:06:15.460
given the states actions,

112
00:06:15.460 --> 00:06:17.545
your worse next state samples from the pool.

113
00:06:17.545 --> 00:06:21.550
Of course those samples if you record your 10,000 iterations,

114
00:06:21.550 --> 00:06:24.660
are going to be probably worse than your current policy.

115
00:06:24.660 --> 00:06:29.490
So, if you just learn to walk upright,

116
00:06:29.490 --> 00:06:33.185
then 10,000 iterations before,

117
00:06:33.185 --> 00:06:35.535
you probably haven't learned that yet.

118
00:06:35.535 --> 00:06:38.090
And you won't get samples where you're walking upright.

119
00:06:38.090 --> 00:06:41.030
But other ways, this allows you to cheat your way

120
00:06:41.030 --> 00:06:44.390
into a hundred times more frequent updates.

121
00:06:44.390 --> 00:06:46.995
So, making hundreds, kind of,

122
00:06:46.995 --> 00:06:50.115
hundreds of virtual updates per one review.

123
00:06:50.115 --> 00:06:53.460
This gives you a lot of profit where sampling

124
00:06:53.460 --> 00:06:56.250
were getting you FA Eros Prime that is actually very expensive.

125
00:06:56.250 --> 00:06:59.970
For example, if you're using actual physical car or robot to get

126
00:06:59.970 --> 00:07:05.120
FA Eros Prime by actually moving it into a physical environment.

127
00:07:05.120 --> 00:07:07.580
This is actually a part of your bonus assignments,

128
00:07:07.580 --> 00:07:09.625
so you'll have a more detailed description of it later.

129
00:07:09.625 --> 00:07:13.480
Finally, this idea of experience replay is going to be very

130
00:07:13.480 --> 00:07:17.715
popular along the neural network based deep reinforcement learning methods.

131
00:07:17.715 --> 00:07:21.480
We'll study those matters in the next week. Until then.