WEBVTT

1
00:00:00.000 --> 00:00:07.695
So, let's now see how those ideas compare to one another in a more strict and formal way.

2
00:00:07.695 --> 00:00:11.585
The Monte-carlo method seems to rely on sampling of complete trajectories.

3
00:00:11.585 --> 00:00:16.185
They have to get at least one full directory to even begin training.

4
00:00:16.185 --> 00:00:20.530
The full trajectory here means that you have to be in the initial state then iterate it,

5
00:00:20.530 --> 00:00:21.950
you'll get to the terminal state.

6
00:00:21.950 --> 00:00:25.010
Sounds easy enough, but when it gives extra challenge

7
00:00:25.010 --> 00:00:27.770
because many complicated trouble control tasks as well

8
00:00:27.770 --> 00:00:30.240
as complicated video games that require you to get

9
00:00:30.240 --> 00:00:33.630
through tens of thousands of time step before you get to the terminal state.

10
00:00:33.630 --> 00:00:37.635
Even Atari games have thousands of iterations before they terminate.

11
00:00:37.635 --> 00:00:42.440
The issue here is that you won't be able to actually

12
00:00:42.440 --> 00:00:44.530
begin from your policy before you spend say

13
00:00:44.530 --> 00:00:47.955
five minutes to get one sample of your experience.

14
00:00:47.955 --> 00:00:52.475
The case is even worse where your process is actually infinite.

15
00:00:52.475 --> 00:00:54.470
So, you have a robot which tries to walk forward,

16
00:00:54.470 --> 00:00:56.025
but there's no key termination condition.

17
00:00:56.025 --> 00:00:59.245
It just gets rewarded for the amount of distance occurs.

18
00:00:59.245 --> 00:01:02.675
The temporal difference method, the Q-learning course similar methods,

19
00:01:02.675 --> 00:01:04.355
they don't have this problem at all.

20
00:01:04.355 --> 00:01:06.360
Instead, they are able to improve a policy in

21
00:01:06.360 --> 00:01:10.095
a meaningful way just based on one time step with this potentially infinite timeline.

22
00:01:10.095 --> 00:01:12.790
So, have your S, A, error,

23
00:01:12.790 --> 00:01:15.720
and next S. And even before you pick the next actions,

24
00:01:15.720 --> 00:01:17.920
what happens, you can already improve your Q-function.

25
00:01:17.920 --> 00:01:21.150
And by say an iteration 100 of this process can

26
00:01:21.150 --> 00:01:25.350
probably expect your algorithm to behave statistically significantly better.

27
00:01:25.350 --> 00:01:27.180
Now, this is not always the case with

28
00:01:27.180 --> 00:01:29.835
this general principle behind the temporal difference method.

29
00:01:29.835 --> 00:01:34.350
In a way, you can consider them as closer to how your humans learn.

30
00:01:34.350 --> 00:01:36.410
You can say that for example,

31
00:01:36.410 --> 00:01:39.830
your humans are capable of learning all kinds of wholesome stuff like writing, reading,

32
00:01:39.830 --> 00:01:43.340
walking upright and while getting the course here and rank all this kind of

33
00:01:43.340 --> 00:01:48.070
all unstoppable advance machine learning without having access to one full session.

34
00:01:48.070 --> 00:01:49.945
So, before you begin learning,

35
00:01:49.945 --> 00:01:54.325
no one gave you the recording of you from birth to death because you cannot die at once.

36
00:01:54.325 --> 00:01:56.160
And this is awesome in a way.

37
00:01:56.160 --> 00:01:58.820
And this is what makes you closer to

38
00:01:58.820 --> 00:02:00.710
the temporal difference method or rather what

39
00:02:00.710 --> 00:02:02.955
makes temporal difference method closer to how you learn.

40
00:02:02.955 --> 00:02:06.035
There's also a lot of things that can say in favor of Monte-carlo method.

41
00:02:06.035 --> 00:02:09.710
In this case, for example if you're having an imperfect table or function,

42
00:02:09.710 --> 00:02:13.400
say you've accidentally discretize

43
00:02:13.400 --> 00:02:16.375
your state poorly when trying to apply it to a particular problem.

44
00:02:16.375 --> 00:02:18.300
Then your Monte-carloa and temporal difference methods,

45
00:02:18.300 --> 00:02:19.795
they are going to run different,

46
00:02:19.795 --> 00:02:22.275
kind of different approximations of your Q-function.

47
00:02:22.275 --> 00:02:25.140
They are going to be wrong in different ways and the way Monte-carlo is wrong,

48
00:02:25.140 --> 00:02:28.515
it's actually kind of less biased than the temporal difference is going to be.

49
00:02:28.515 --> 00:02:32.110
If you're in ideal case this is not that important,

50
00:02:32.110 --> 00:02:34.135
but remember if you're trying to apply for

51
00:02:34.135 --> 00:02:37.065
learning practical problem whenever in ideal case.

52
00:02:37.065 --> 00:02:40.030
We'll include a lot of information about how those two algorithms

53
00:02:40.030 --> 00:02:43.480
compare to one another in the reading section as usual.