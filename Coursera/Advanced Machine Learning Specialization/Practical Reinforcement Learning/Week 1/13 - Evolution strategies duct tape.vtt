WEBVTT

1
00:00:02.890 --> 00:00:07.025
This time, deducting going to be of a different blue color

2
00:00:07.025 --> 00:00:10.990
because it's not technically heuristic.

3
00:00:10.990 --> 00:00:13.660
It's rather a better way to

4
00:00:13.660 --> 00:00:17.335
define the reward function adaptively as your agent progresses.

5
00:00:17.335 --> 00:00:23.500
Imagine you are trying to train a bipedal robot to walk forward as fast as possible.

6
00:00:23.500 --> 00:00:28.880
So, you have a robot with a ton of motors in each unit that they can move independently

7
00:00:28.880 --> 00:00:31.990
and you give it a reward for the amount of distance it

8
00:00:31.990 --> 00:00:35.290
managed to cover in say, 10 seconds.

9
00:00:35.290 --> 00:00:36.830
The distance is measured in meters,

10
00:00:36.830 --> 00:00:40.335
so you get a plus say, 10 for 10 meters.

11
00:00:40.335 --> 00:00:41.900
So, in this case,

12
00:00:41.900 --> 00:00:44.350
if you start training from a random guess,

13
00:00:44.350 --> 00:00:46.560
you probably find out that for first several iterations,

14
00:00:46.560 --> 00:00:49.550
the reward therefore is going to be somewhere between say,

15
00:00:49.550 --> 00:00:50.920
zero and one because

16
00:00:50.920 --> 00:00:54.960
the initial session is going to consist of robot falling down his face,

17
00:00:54.960 --> 00:00:57.550
and maybe crawling forward by sporadic,

18
00:00:57.550 --> 00:00:59.465
chaotic movements of his limbs.

19
00:00:59.465 --> 00:01:02.090
So, this is how you're going to begin.

20
00:01:02.090 --> 00:01:06.010
But eventually, as you hopefully converge to some kind of optimal way,

21
00:01:06.010 --> 00:01:09.140
robot looks upright, the theory was of the plus 100,

22
00:01:09.140 --> 00:01:12.030
maybe even more if your robot find a way to run fast.

23
00:01:12.030 --> 00:01:15.600
This basically, it doesn't technically break anything.

24
00:01:15.600 --> 00:01:21.440
But, it does make you adjust the learning parameters of your algorithm.

25
00:01:21.440 --> 00:01:25.225
For example, if you find an optimal learning range for the beginning of your algorithm,

26
00:01:25.225 --> 00:01:31.975
the [inaudible] will probably be an overshoot at the or interference where the rewards are latched.

27
00:01:31.975 --> 00:01:35.650
So, instead of trying to adjust learning based on the fly, you can try a different thing.

28
00:01:35.650 --> 00:01:40.630
You can try to use, redefine your rewards based on this simple equation.

29
00:01:40.630 --> 00:01:44.465
So, if there is something that you subtract from each reward,

30
00:01:44.465 --> 00:01:47.920
this some kind of constant value which doesn't depend on a particular session,

31
00:01:47.920 --> 00:01:51.155
or multiplied by something core, or subtract something,

32
00:01:51.155 --> 00:01:55.340
then maximizing the initial reward is exactly,

33
00:01:55.340 --> 00:01:58.390
it's going to give you exactly the same solution as maximizing this new reward.

34
00:01:58.390 --> 00:02:00.920
If your algorithm converters turn often, of course.

35
00:02:00.920 --> 00:02:04.610
This is the same sequence that happens if we use the,

36
00:02:04.610 --> 00:02:08.455
if we modify the last function for supervise learning.

37
00:02:08.455 --> 00:02:10.955
This time we just normalize the rewards.

38
00:02:10.955 --> 00:02:14.800
And to normalize, we can compute the median and standard deviation

39
00:02:14.800 --> 00:02:18.655
based on either several sessions within one iteration.

40
00:02:18.655 --> 00:02:21.170
So, if sampled, say a hundred sessions,

41
00:02:21.170 --> 00:02:22.250
we get the mean reward,

42
00:02:22.250 --> 00:02:26.130
the standard deviation via maybe your favorite numpile-like package,

43
00:02:26.130 --> 00:02:28.555
and then you subtract one and divide by the second.

44
00:02:28.555 --> 00:02:32.930
But, the session which is the best of

45
00:02:32.930 --> 00:02:35.130
those 100-session batch will still get

46
00:02:35.130 --> 00:02:38.615
the highest award because that's how arithmetics work.

47
00:02:38.615 --> 00:02:41.570
So, this new reward,

48
00:02:41.570 --> 00:02:44.770
which is the reward minus mean divided by standard deviation,

49
00:02:44.770 --> 00:02:48.135
those rewards is usually referred to as the advantage.

50
00:02:48.135 --> 00:02:51.070
How your algorithm performs compared to

51
00:02:51.070 --> 00:02:55.805
the mean performance of other attempts within this iteration,

52
00:02:55.805 --> 00:02:59.695
or well, other algorithms depending on how you define the,

53
00:02:59.695 --> 00:03:03.025
involved the events function here, how you calculate the mean.

54
00:03:03.025 --> 00:03:07.085
And basically, it has the same properties as,

55
00:03:07.085 --> 00:03:10.270
it gives the same benefits as when you train a linear model when

56
00:03:10.270 --> 00:03:14.295
you normalize the feature there as an input.

57
00:03:14.295 --> 00:03:17.860
So, here it goes. You have a modified version.

58
00:03:17.860 --> 00:03:21.035
Just block in this advantage instead of

59
00:03:21.035 --> 00:03:25.265
every occurrence of rewards in the previous slide like this,

60
00:03:25.265 --> 00:03:29.900
and then you'll get an algorithm which hopefully converts slightly faster.

61
00:03:29.900 --> 00:03:33.690
We'll get a much closer look at this advantage function,

62
00:03:33.690 --> 00:03:35.660
and its benefits and drawbacks in

63
00:03:35.660 --> 00:03:38.990
the later weeks for our course when we cover the policy based methods.

64
00:03:38.990 --> 00:03:40.865
So, here it goes.

65
00:03:40.865 --> 00:03:43.235
Now, we'll see how the evolution strategies compares to

66
00:03:43.235 --> 00:03:46.845
other algorithms we've already covered and the ones we are yet to cover.

67
00:03:46.845 --> 00:03:50.385
It's again, yet another black box characterization trick.

68
00:03:50.385 --> 00:03:53.960
So, the huge upside

69
00:03:53.960 --> 00:03:57.505
of the evolution strategies algorithm that it's super easy to implement.

70
00:03:57.505 --> 00:04:02.510
And since a single iteration of the algorithm is very simple to implement,

71
00:04:02.510 --> 00:04:05.410
it's also kind of trivial to parallelize.

72
00:04:05.410 --> 00:04:07.580
Imagine you have, imagine you live in

73
00:04:07.580 --> 00:04:10.760
the modern era and for any research you have several CPUs,

74
00:04:10.760 --> 00:04:14.865
say 1,000 CPUs that can compute things independently,

75
00:04:14.865 --> 00:04:17.540
and you want to use all,

76
00:04:17.540 --> 00:04:21.225
the whole bunch of CPUs to estimate this formulae.

77
00:04:21.225 --> 00:04:24.135
Let's say that we take well,

78
00:04:24.135 --> 00:04:25.820
a hundred samples for theta,

79
00:04:25.820 --> 00:04:33.010
and for each sample we play 10 games with this particular theta to cover for the,

80
00:04:33.010 --> 00:04:36.375
in say like a CCC of this second sum in the formulation.

81
00:04:36.375 --> 00:04:40.650
How can we compute this formula explicitly without losing a lot of

82
00:04:40.650 --> 00:04:45.175
time because of the sequential nature? Probably, right?

83
00:04:45.175 --> 00:04:48.120
You can sample a thousands,

84
00:04:48.120 --> 00:04:49.950
say a hundreds values of theta,

85
00:04:49.950 --> 00:04:54.560
and then send each theta to 10 cores in your

86
00:04:54.560 --> 00:04:59.580
1,000 core cluster so that this core would be able to compute one trajectory.

87
00:04:59.580 --> 00:05:02.915
Then for attempt computing just one trajectory you would get

88
00:05:02.915 --> 00:05:07.420
a number of trajectories equal to your number, of course.

89
00:05:07.420 --> 00:05:11.870
And this gives you a linear improvements of scale of 1,000.

90
00:05:11.870 --> 00:05:16.280
This is super great because more complete algorithms usually don't scale that well

91
00:05:16.280 --> 00:05:21.005
because they have a lot of sequential paths that cannot be easily paralyzed.

92
00:05:21.005 --> 00:05:24.205
So, but here's an upside.

93
00:05:24.205 --> 00:05:26.070
Now, we see some of the results of how

94
00:05:26.070 --> 00:05:29.335
this algorithm works in the practical environments.

95
00:05:29.335 --> 00:05:31.230
This is a report by OpenAI.

96
00:05:31.230 --> 00:05:34.920
There's also a very well written blog post that you can read,

97
00:05:34.920 --> 00:05:37.000
which we recommend you to do right now.

98
00:05:37.000 --> 00:05:40.590
This probably be a pop-up with a text right here.

99
00:05:40.590 --> 00:05:43.170
So, here are the training costs,

100
00:05:43.170 --> 00:05:46.975
the rewards per the number of iterations the algorithm for

101
00:05:46.975 --> 00:05:51.270
each of the three games they have tried the algorithm on.

102
00:05:51.270 --> 00:05:53.370
There are of course much more experimentations in the black box,

103
00:05:53.370 --> 00:05:58.590
they can find that yields slightly different although generally similar results.

104
00:05:58.590 --> 00:06:01.360
And what you're going to find here is that the evolution strategies,

105
00:06:01.360 --> 00:06:05.045
the one that's the orange line here,

106
00:06:05.045 --> 00:06:09.370
is usually almost as efficient as this other method TRPO.

107
00:06:09.370 --> 00:06:13.370
Now, TRPO takes almost an entire lecture to explain,

108
00:06:13.370 --> 00:06:18.705
and in some cases it's even less efficient as we can see in the very first plot here.

109
00:06:18.705 --> 00:06:21.460
But, if we can parallelize our method,

110
00:06:21.460 --> 00:06:23.200
we can scale it to multiple cores,

111
00:06:23.200 --> 00:06:27.715
it'll be much more efficient than TRPO because of the all well,

112
00:06:27.715 --> 00:06:29.080
computation power you haven't had.

113
00:06:29.080 --> 00:06:33.110
And you cannot do the same parallelization trick with TRPO as efficiently.