WEBVTT

1
00:00:02.330 --> 00:00:06.715
And this is as to this more general decision process formulation

2
00:00:06.715 --> 00:00:10.500
ever which consists of ancient environment in this closed loop.

3
00:00:10.500 --> 00:00:13.050
This process consists of two steps.

4
00:00:13.050 --> 00:00:17.350
First, you have agents observe some properties of the environment,

5
00:00:17.350 --> 00:00:22.845
some kind of sensory input or while user features in the banner as Westmont case,

6
00:00:22.845 --> 00:00:25.580
and then your agent can pick an action,

7
00:00:25.580 --> 00:00:30.035
given those observations and send this section back into the environment.

8
00:00:30.035 --> 00:00:34.485
After that, he not only receives the feedback for his particular action,

9
00:00:34.485 --> 00:00:38.190
he also changes the properties of the environment in some way.

10
00:00:38.190 --> 00:00:41.880
So not only you get click or no click

11
00:00:41.880 --> 00:00:46.250
you also change how your user base perceives your service.

12
00:00:46.250 --> 00:00:53.050
Now, this thing of course works not only for the banner ads for a case study,

13
00:00:53.050 --> 00:00:55.250
but also for great many other programs.

14
00:00:55.250 --> 00:00:58.230
In fact it encompasses all the supervised,

15
00:00:58.230 --> 00:01:01.745
unsupervised and other sub-domains of machine learning.

16
00:01:01.745 --> 00:01:06.080
The intuition here, is that you have something that you don't control,

17
00:01:06.080 --> 00:01:09.150
the environment, the guy below here,

18
00:01:09.150 --> 00:01:11.060
and this is the source of your data,

19
00:01:11.060 --> 00:01:12.335
the source of scary horse,

20
00:01:12.335 --> 00:01:16.895
but in the worst case this is a black box so we have no access to the inner workings of

21
00:01:16.895 --> 00:01:22.065
how it delivers observations and how it assigns feedback.

22
00:01:22.065 --> 00:01:26.230
You can however try to reverse engineer this from the data.

23
00:01:26.230 --> 00:01:29.650
The thing about here, the agent is whatever you can

24
00:01:29.650 --> 00:01:33.320
control and in this case you have all the power you want,

25
00:01:33.320 --> 00:01:36.610
all the mathematical formulas

26
00:01:36.610 --> 00:01:40.060
you want and anything here you can use any supervised learning technique,

27
00:01:40.060 --> 00:01:43.970
any heuristics basically, you can do anything above the line.

28
00:01:43.970 --> 00:01:48.750
And later we'll see a lot of cool machine learning methods being applied here,

29
00:01:48.750 --> 00:01:52.160
so we'll have neural networks and samples of trees,

30
00:01:52.160 --> 00:01:55.675
some kind of blending algorithms and so on.

31
00:01:55.675 --> 00:02:01.310
Now see how this formalism applies to problems outside the one we've just covered.

32
00:02:01.310 --> 00:02:05.195
Now, obviously we have this news case in the web,

33
00:02:05.195 --> 00:02:08.320
like recommending movies, recommending books,

34
00:02:08.320 --> 00:02:10.350
placing advertisements and so on,

35
00:02:10.350 --> 00:02:16.115
and we've just covered this in a high enough level of detail.

36
00:02:16.115 --> 00:02:19.690
But this can also apply to things on

37
00:02:19.690 --> 00:02:23.730
the other spectrum of technical problems, say robotics.

38
00:02:23.730 --> 00:02:28.150
And robotics are where enforcement learning has a lot of applications.

39
00:02:28.150 --> 00:02:32.500
You can do all kinds of stuff from teaching your robots to play

40
00:02:32.500 --> 00:02:37.365
football to teaching your industrial robot to flip pancakes if that's what you want.

41
00:02:37.365 --> 00:02:41.570
There's also this hot topic of self-driving cars which

42
00:02:41.570 --> 00:02:45.740
is yet another special case of dynamics system,

43
00:02:45.740 --> 00:02:51.945
it's another kind of robot which you can also improve by teaching it to,

44
00:02:51.945 --> 00:02:54.820
well not bump into things and drive more efficiently in

45
00:02:54.820 --> 00:02:58.205
terms of fuel efficiency for example or time,

46
00:02:58.205 --> 00:03:00.090
so let's get slightly more specific.

47
00:03:00.090 --> 00:03:01.645
Let's say we have a bipedal robot,

48
00:03:01.645 --> 00:03:03.135
we want to teach it to walk,

49
00:03:03.135 --> 00:03:05.340
preferably walk forward without falling down.

50
00:03:05.340 --> 00:03:08.580
And in this case the reinforcement learning problem is defined thusly,

51
00:03:08.580 --> 00:03:12.260
we have observations which are whatever sensory the robot has,

52
00:03:12.260 --> 00:03:14.425
maybe the angles of the limbs,

53
00:03:14.425 --> 00:03:19.245
are ever limp and maybe the camera feed if the robot has a camera installed on it.

54
00:03:19.245 --> 00:03:24.580
Now the action in this case is whatever signal your robot sends to its motion subsystem.

55
00:03:24.580 --> 00:03:27.770
Having motors attached to every joint and

56
00:03:27.770 --> 00:03:31.165
you can bend them however you want by sending some kind of voltage maybe.

57
00:03:31.165 --> 00:03:33.290
And in this case,

58
00:03:33.290 --> 00:03:37.790
you want to maximize the feedback which can be defined by for example,

59
00:03:37.790 --> 00:03:41.060
the distance you robot managed to move forward without falling.

60
00:03:41.060 --> 00:03:43.430
This is a problem for

61
00:03:43.430 --> 00:03:45.740
a significantly different domain but it can be

62
00:03:45.740 --> 00:03:48.715
formalized into this very same decision process.

63
00:03:48.715 --> 00:03:54.360
Now, although this problem in itself is quite useless,

64
00:03:54.360 --> 00:03:55.810
you've probably noticed that

65
00:03:55.810 --> 00:04:00.390
more than 80 percent of enforcement learning articles published in

66
00:04:00.390 --> 00:04:03.910
the few recent years are trying

67
00:04:03.910 --> 00:04:09.245
to solve this particular problem of teaching agents to play games.

68
00:04:09.245 --> 00:04:13.620
And this is kind of strange actually because there is a lot of other problems that you

69
00:04:13.620 --> 00:04:18.475
can solve effectively that will lead to practically relevant results.

70
00:04:18.475 --> 00:04:21.230
Now I want you to define the observations,

71
00:04:21.230 --> 00:04:22.845
actions and feedback in this process.

72
00:04:22.845 --> 00:04:25.700
So let's try to find out what will our agent get us an input,

73
00:04:25.700 --> 00:04:28.110
what will it be able to do in this environment,

74
00:04:28.110 --> 00:04:30.550
and how do you measure its performance.

75
00:04:30.550 --> 00:04:34.050
Of course it's quite obvious as a game,

76
00:04:34.050 --> 00:04:38.080
and you can seem to denote your observation as whatever

77
00:04:38.080 --> 00:04:42.370
you see and hear in the game maybe a sequence of images and the sound if you want,

78
00:04:42.370 --> 00:04:46.675
and if you have access to the random access memory of the console,

79
00:04:46.675 --> 00:04:48.760
you can use the random access memory instead of

80
00:04:48.760 --> 00:04:52.465
this image and get even better higher level observation.

81
00:04:52.465 --> 00:04:56.960
Your actions are as simple as pressing buttons and moving joysticks so it would be

82
00:04:56.960 --> 00:05:03.415
six to eight possible conditions of joystick manipulations you can do at once.

83
00:05:03.415 --> 00:05:08.495
And the feedback is the score the game provides you in the top of the screenshots.

84
00:05:08.495 --> 00:05:14.060
There's another success story concerning reinforcements turning in board games domain.

85
00:05:14.060 --> 00:05:18.020
Probably the most over-hyped one is the Alpha goal which was able

86
00:05:18.020 --> 00:05:23.000
to play goal at levels that allowed to be top of the line goal players.

87
00:05:23.000 --> 00:05:27.000
There's also a few other successful applications

88
00:05:27.000 --> 00:05:29.880
in backgammon with both TG Gammon and Simel ones,

89
00:05:29.880 --> 00:05:31.220
but these are games,

90
00:05:31.220 --> 00:05:35.845
so let's try to see how it applies to something more closer to production.

91
00:05:35.845 --> 00:05:37.810
There's also a huge sub-domain of

92
00:05:37.810 --> 00:05:40.740
re-enforcement learning at the borderline with deep learning.

93
00:05:40.740 --> 00:05:43.010
If you want the formal description of it,

94
00:05:43.010 --> 00:05:45.325
it would be the methods for Deep Learning to

95
00:05:45.325 --> 00:05:48.385
optimize the non-differentiable loss functions.

96
00:05:48.385 --> 00:05:52.190
This sounds slightly theoretical but in practice,

97
00:05:52.190 --> 00:05:54.060
it has a lot of qualifications.

98
00:05:54.060 --> 00:05:57.260
For example, if you want to build the machine translation system,

99
00:05:57.260 --> 00:05:58.820
the deep learning approach,

100
00:05:58.820 --> 00:06:03.010
the usual approach is to minimize the cross entropy or if you want,

101
00:06:03.010 --> 00:06:05.545
you maximize the logarithm of likelihood.

102
00:06:05.545 --> 00:06:09.190
Reinforcement learning, however I urge you to explicitly maximize the metrics you want,

103
00:06:09.190 --> 00:06:13.525
say the Blair who they measure or other machine translation specific metrics.

104
00:06:13.525 --> 00:06:17.950
This of course has some ramifications and again we'll delve into more detail later on,

105
00:06:17.950 --> 00:06:20.280
but the general idea here is that it allows

106
00:06:20.280 --> 00:06:23.600
you more freedom in terms of what you want to optimize.

107
00:06:23.600 --> 00:06:27.440
Also this needs a new area

108
00:06:27.440 --> 00:06:30.365
that tries to use reinforcement learning to optimize the architecture of

109
00:06:30.365 --> 00:06:33.670
DPL networks automatically which is kind of promising because

110
00:06:33.670 --> 00:06:37.970
there is no other automatic way to building your art or architecture.