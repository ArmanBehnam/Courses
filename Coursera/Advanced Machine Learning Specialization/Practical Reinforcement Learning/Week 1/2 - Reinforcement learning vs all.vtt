WEBVTT

1
00:00:02.780 --> 00:00:05.200
Now, let's go a quick recap on how

2
00:00:05.200 --> 00:00:08.215
reinforcement learning compares to other demands machine learning.

3
00:00:08.215 --> 00:00:12.720
In supervised learning, you generally assume that you have a data set where you record

4
00:00:12.720 --> 00:00:17.495
not only observations but also the optimal answer that some kind of expert gave you.

5
00:00:17.495 --> 00:00:21.960
And your main task is to get as close as possible to the opinion of this expert.

6
00:00:21.960 --> 00:00:24.940
Of course this means that you need this expertise to gather the data in

7
00:00:24.940 --> 00:00:27.830
the first place and if you don't have, you're more or less screwed.

8
00:00:27.830 --> 00:00:30.620
It is also very important to supervise

9
00:00:30.620 --> 00:00:34.160
learning can usually assumed that your training points are independent of one another.

10
00:00:34.160 --> 00:00:36.510
And this basically helps you a great deal if

11
00:00:36.510 --> 00:00:40.575
you apply say a stochastic gradient descent where you want to sample the data.

12
00:00:40.575 --> 00:00:43.815
Reinforcement learning however lacks all those assumptions.

13
00:00:43.815 --> 00:00:46.540
You have this, first you don't have a dataset,

14
00:00:46.540 --> 00:00:48.505
as you have already noticed.

15
00:00:48.505 --> 00:00:51.460
Instead, you have some kind of system from which you can sample data,

16
00:00:51.460 --> 00:00:53.070
but you don't get the reference answers,

17
00:00:53.070 --> 00:00:55.225
so there is no expert telling you what to do.

18
00:00:55.225 --> 00:00:58.375
Instead, you can try actions by yourself and there is some kind of

19
00:00:58.375 --> 00:01:01.465
critique that assigns you positive or negative feedback.

20
00:01:01.465 --> 00:01:02.830
In case of one advertisement,

21
00:01:02.830 --> 00:01:06.470
this feedback was denoted as basically the money you get for the clicks

22
00:01:06.470 --> 00:01:12.705
and basically this implies that whatever algorithm you use for enforcement learning,

23
00:01:12.705 --> 00:01:15.720
you have to take care of exploring all the possible options,

24
00:01:15.720 --> 00:01:20.540
lest your risk to never try the optimal action and never learn it.

25
00:01:20.540 --> 00:01:26.230
The other problem with this reinforcement learning formulation is that your agents,

26
00:01:26.230 --> 00:01:28.350
your well, decision maker,

27
00:01:28.350 --> 00:01:31.470
basically affects his own observations.

28
00:01:31.470 --> 00:01:34.430
So whenever you, for example,

29
00:01:34.430 --> 00:01:36.985
whenever you try to ride the bicycle,

30
00:01:36.985 --> 00:01:40.345
if you always turn left on the first turn,

31
00:01:40.345 --> 00:01:42.430
you will never get data points where

32
00:01:42.430 --> 00:01:45.190
you ride on the part of the road which is on the right turn.

33
00:01:45.190 --> 00:01:49.600
Basically, you have to be very careful to explore the state space as well, otherwise,

34
00:01:49.600 --> 00:01:55.725
you risk misinterpretation there and failing to grasp the entirety of your problem.

35
00:01:55.725 --> 00:01:58.420
The other domains like unsupervised learning,

36
00:01:58.420 --> 00:02:01.925
also differ from reinforcement learning a great deal.

37
00:02:01.925 --> 00:02:04.015
Unsupervised learning tries to,

38
00:02:04.015 --> 00:02:05.680
it doesn't have expert as well,

39
00:02:05.680 --> 00:02:08.405
but it tries to do a different thing.

40
00:02:08.405 --> 00:02:10.600
Instead of trying to be an optimal strategy,

41
00:02:10.600 --> 00:02:12.450
it seemed to try to describe the data,

42
00:02:12.450 --> 00:02:17.780
find some underlying structure and this is very different from trying to find

43
00:02:17.780 --> 00:02:20.120
a strategy because sometimes it's much easier

44
00:02:20.120 --> 00:02:23.290
to ride the bicycle than to understand the structure of it,

45
00:02:23.290 --> 00:02:26.380
especially when it comes to not the bicycle but say a computer.

46
00:02:26.380 --> 00:02:28.625
Now finally, it's important to remember that

47
00:02:28.625 --> 00:02:30.960
although there are those kind of features and bullet points,

48
00:02:30.960 --> 00:02:35.480
there are no hard decision boundaries of what's unsupervised learning,

49
00:02:35.480 --> 00:02:37.525
what's supervised learning, what's reinforcement learning.

50
00:02:37.525 --> 00:02:40.775
Instead, if you're trying to solve any particular practical problem,

51
00:02:40.775 --> 00:02:45.340
you'll probably find yourself using it in some combination of supervised learning,

52
00:02:45.340 --> 00:02:46.815
inside of reinforcement learning,

53
00:02:46.815 --> 00:02:49.210
and unsupervised learning as a helper maybe.

54
00:02:49.210 --> 00:02:53.680
But reinforcement learning is more or less the most general area that can

55
00:02:53.680 --> 00:02:59.210
be treated as kind of the superset of full supervised non-supervised learning here.