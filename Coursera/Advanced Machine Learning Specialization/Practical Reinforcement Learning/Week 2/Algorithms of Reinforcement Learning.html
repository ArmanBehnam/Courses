<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Algorithms of Reinforcement Learning</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="bio.html">Bio</a></div>
<div class="menu-item"><a href="cv.html">CV</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="life.html">Life</a></div>
<div class="menu-item"><a href="calendar.html">Calendar</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="team.html">Team</a></div>
<div class="menu-item"><a href="books.html">Books</a></div>
<div class="menu-item"><a href="pubs.html">Publications</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="grad.html" class="current">Grad</a></div>
<div class="menu-item"><a href="ugrad.html" class="current">Undergrad</a></div>
<div class="menu-category">Other</div>
<div class="menu-item"><a href="https://scholar.google.ca/citations?user=zvC19mQAAAAJ&hl=en">Google&nbsp;Scholar</a></div>
<div class="menu-item"><a href="links.html">Technicalities</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Algorithms of Reinforcement Learning</h1>
<div id="subtitle"></div>
</div>
<h3>Access</h3>
<ul>
<li><p>Download the <a href="http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf" target=&ldquo;blank&rdquo;>pdf</a>, free of charge, courtesy of our wonderful publisher. Last update:
March 12, 2019

</p>
</li>
<li><p>Access the <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009" target=&ldquo;blank&rdquo;>original</a> on the Morgan and Claypool webpage
</p>
</li>
<li><p>Buy a printed copy from</p>
<ul>
<li><p><a href="http://www.amazon.com/Algorithms-Reinforcement-Learning-Csaba-Szepesvari/dp/1608454924" target=&ldquo;blank&rdquo;>Amazon.com</a> ca. USD 35.00
</p>
</li>
<li><p><a href="http://www.amazon.ca/Algorithms-Reinforcement-Learning-Csaba-Szepesvari/dp/1608454924" target=&ldquo;blank&rdquo;>Amazon.ca</a> ca. CDN 42.02
</p>
</li>
<li><p><a href="http://www.amazon.co.uk/Algorithms-Reinforcement-Learning-Czaba-Szepesvari/dp/1608454924/ref=gfix-ews-form" target=&ldquo;blank&rdquo;>Amazon.co.uk</a>, GBP18.99.
</p>
</li></ul>
</li>
<li><p>Faculty can write to <a href="mailto:info@morganclaypool.com"><tt>info@morganclaypool.com</tt></a>
to request a desk copy

</p>
</li>
<li><p>Japanese translation by <a href="https://sotets.uk" target=&ldquo;blank&rdquo;>Sotetsu Koyamada</a>!
The translation has a short supplementary material about the equivalence of the forward and backward views of TD lambda (by <a href="https://masomatics.github.io" target=&ldquo;blank&rdquo;>Dr. Koyama</a>) and also on deep RL (by Sotetsu Koyamada).
Amazon Asia <a href="http://amzn.asia/blc1fbR" target=&ldquo;blank&rdquo;>link</a>,
<a href="http://www.kyoritsu-pub.co.jp/bookdetail/9784320124226" target=&ldquo;blank&rdquo;>Kyoritsu pub</a>,
<a href="https://github.com/rl-tokyo/szepesvari-book" target=&ldquo;blank&rdquo;>errata</a>.
</p>
</li>
</ul>
<h3>Why did I write this book?</h3>
<p>
Good question! There exist a good number of really great books on Reinforcement Learning. So why a new book?

I had selfish reasons: I wanted a short book, which nevertheless contained the major ideas underlying state-of-the-art RL algorithms (back in 2010), a discussion of their relative strengths and weaknesses, with hints on what is known (and not known, but would be good to know) about these algorithms. If I succeeded, time will tell. Or, you can, by sending me an e-mail at <a href="mailto:csaba.szepesvari@gmail.com"><tt>csaba.szepesvari@gmail.com</tt></a>
</p>
<h3>Abstract</h3>
<p>
Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms&rsquo; merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.
</p>
<h3>Table of contents</h3>
<ol>
<li><p>Preface  ix</p>
</li>
<li><p>Acknowledgments   xiii</p>
</li>
<li><p>Markov Decision Processes    1</p>
<ol>
<li><p>Preliminaries   1</p>
</li>
<li><p>Markov Decision Processes    1</p>
</li>
<li><p>Value functions   6</p>
</li>
<li><p>Dynamic programming algorithms for solving MDPs    10</p>
</li></ol>
</li>
<li><p>Value Prediction Problems    11</p>
<ol>
<li><p>Temporal difference learning in finite state spaces   11</p>
<ol>
<li><p>Tabular TD(0)    11</p>
</li>
<li><p>Every-visit Monte-Carlo   14</p>
</li>
<li><p>TD(lambda): Unifying Monte-Carlo and TD(0)   16</p>
</li></ol>
</li>
<li><p>Algorithms for large state spaces    18</p>
<ol>
<li><p>TD(lambda) with function approximation   22</p>
</li>
<li><p>Gradient temporal difference learning    25</p>
</li>
<li><p>Least-squares methods   27</p>
</li>
<li><p>The choice of the function space    33</p>
</li></ol>
</li></ol>
</li>
<li><p>Control   37</p>
<ol>
<li><p>A catalog of learning problems   37</p>
</li>
<li><p>Closed-loop interactive learning    38</p>
<ol>
<li><p>Online learning in bandits   38</p>
</li>
<li><p>Active learning in bandits    40</p>
</li>
<li><p>Active learning in Markov Decision Processes   41</p>
</li>
<li><p>Online learning in Markov Decision Processes    42</p>
</li></ol>
</li>
<li><p>Direct methods   47</p>
<ol>
<li><p>Q-learning in finite MDPs   47</p>
</li>
<li><p>Q-learning with function approximation   49</p>
</li></ol>
</li>
<li><p>Actor-critic methods   52</p>
<ol>
<li><p>Implementing a critic   54</p>
</li>
<li><p>Implementing an actor   56</p>
</li></ol>
</li></ol>
</li>
<li><p>For Further Exploration    63</p>
<ol>
<li><p>Further reading   63</p>
</li>
<li><p>Applications    63</p>
</li>
<li><p>Software   64</p>
</li></ol>
</li>
<li><p>Appendix: The Theory of Discounted Markovian Decision Processes    65</p>
<ol>
<li><p>A.1 Contractions and Banachâ€™s fixed-point theorem   65</p>
</li>
<li><p>A.2 Application to MDPs   69</p>
</li></ol>
</li>
<li><p>Bibliography   73</p>
</li>
<li><p>Author's Biography   89
</p>
</li>
</ol>
<h3>Algorithms</h3>
<p>The book, as the title suggests, describes a number of algorithms. These are the following. For algorithms whose names are boldfaced a pseudocode is also given.
</p>
<ol>
<li><p>Value iteration   p. 10</p>
</li>
<li><p>Policy iteration   p. 10</p>
</li>
<li><p><b>Tabular TD(0)</b>   p. 12</p>
</li>
<li><p><b>Every-visit Monte-Carlo</b>   p. 15</p>
</li>
<li><p>Tabular TD(lambda), accumulating traces   p. 17</p>
</li>
<li><p>First-visit Monte-Carlo   p. 17</p>
</li>
<li><p><b>Tabular TD(lambda), replacing traces</b>   pp. 17-18</p>
</li>
<li><p><b>TD(lambda) with function approximation</b>   p. 22</p>
</li>
<li><p><b>GTD2</b>   p. 26</p>
</li>
<li><p>TDC   p. 27</p>
</li>
<li><p>LSTD   p. 28</p>
</li>
<li><p><b>RLSTD</b>   pp. 28-29</p>
</li>
<li><p>(R)LSTD(lambda)   pp. 29-30</p>
</li>
<li><p><b>lambda-LSPE</b>   pp. 30-31</p>
</li>
<li><p>Boltzmann exploration   p. 38</p>
</li>
<li><p>epsilon-greedy   p. 39</p>
</li>
<li><p><b>UCB1</b>   pp. 39-40</p>
</li>
<li><p>Action elimination   p. 41</p>
</li>
<li><p>E^3   p. 42</p>
</li>
<li><p><b>UCRL2</b>   pp. 42-44</p>
</li>
<li><p>R-max, MBIE, OI, delayed-Q, MorMax   p. 44</p>
</li>
<li><p><b>Tabular Q-learning</b>   p. 47</p>
</li>
<li><p><b>Q-learning with function approximation</b>   pp. 49-50</p>
</li>
<li><p>State-aggregation based Q-learning   p. 50</p>
</li>
<li><p>Soft-state aggregation based Q-learning   p. 50</p>
</li>
<li><p><b>Fitted Q-iteration</b>   pp. 51-52</p>
</li>
<li><p>Generalized policy iteration   pp. 52-53</p>
</li>
<li><p>SARSA   p. 54</p>
</li>
<li><p><b>SARSA(lambda)</b>   pp. 55-55</p>
</li>
<li><p><b>LSTDQ(lambda)</b>   pp. 55-56</p>
</li>
<li><p><b>LSPI(lambda)</b>   p. 57</p>
</li>
<li><p>REINFORCE   p. 59</p>
</li>
<li><p><b>Actor-critic with SARSA(1)</b>   pp. 60-61</p>
</li>
<li><p>Natural actor-critic   p. 57
</p>
</li>
</ol>
<h3>Other unique features of the book</h3>
<p>
The book discusses the following function approximation methods:</p>
<ol>
<li><p>Linear function approximation,</p>
</li>
<li><p>tensor product construction,</p>
</li>
<li><p>kernel smoothing,</p>
</li>
<li><p>averagers,</p>
</li>
<li><p>state aggregation,</p>
</li>
<li><p>tile coding,</p>
</li>
<li><p>nearest-neighbor methods,</p>
</li>
<li><p>nonparametric kernel smoothing,</p>
</li>
<li><p>gaussian processes, and</p>
</li>
<li><p>RKHS regression.
</p>
</li>
</ol>
<p><p>
In addition, it discusses the relative merits of &ldquo;batch&rdquo; (LS-type) and incremental (TD-type) algorithms,
the influence of the choice of the function approximation method (can we overfit in reinforcement learning?),
various theoretically well-founded online learning algorithms (ever wondered about what an efficient exploration method should do?),
actor-critic algorithms,
and more.

Some connections to other parts of the literature (outside of machine learning) are mentioned. This includes connection of LSTD (and related methods) to Z-estimation (from statistics), sample-average approximation methods (from operations research), or the connection of policy gradient algorithms to likelihood ratio methods (from simulation optimization). In general, the book has many pointers to the literature. I think that the books provides a <s>very good</s> reasonable starting point if someone wants to know the status of the theory related to some algorithm or idea.The book cites 207 works, many of which were quite recent in 2010.
</p>
<h3>New results in the book</h3>
<p></p>
<ol>
<li><p>TD(lambda) with linear function approximation solves a model (previously, this was known for lambda=0 only)</p>
</li>
<li><p>A new bound on the complexity of active learning in finite deterministic MDPs, which significantly improves a previous bound by Sebastian Thrun.
</p>
</li>
</ol>
<h3>Tutorial, slides</h3>
<p>
Some people find it much easier to learn from slides.
<a href="http://richsutton.com" target=&ldquo;blank&rdquo;>Rich</a> and I gave a tutorial at AAAI-2010 in July that was based on the book.
The tutorial webpage is <a href="http://www.sztaki.hu/~szcsaba/research/AAAI10_Tutorial" target=&ldquo;blank&rdquo;>here</a>.
We used the following slides:
</p>
<ol>
<li><p><a href="http://www.sztaki.hu/~szcsaba/research/AAAI10_Tutorial/tutorial01-slidesonly.pdf" target=&ldquo;blank&rdquo;>Part 1</a> &ndash; Foundations of computing optimal decisions</p>
</li>
<li><p><a href="http://www.sztaki.hu/~szcsaba/research/AAAI10_Tutorial/tutorial02-slidesonly.pdf" target=&ldquo;blank&rdquo;>Part 2</a> &ndash; Learning to predict values</p>
</li>
<li><p><a href="http://www.sztaki.hu/~szcsaba/research/AAAI10_Tutorial/tutorial03-slidesonly.pdf" target=&ldquo;blank&rdquo;>Part 3</a> &ndash; Learning to control</p>
</li>
<li><p><a href="http://www.sztaki.hu/~szcsaba/research/AAAI10_Tutorial/tutorial04-slidesonly.pdf" target=&ldquo;blank&rdquo;>Part 4</a> &ndash; Summary
</p>
</li>
</ol>
<h3>Errata (last update: June 25, 2018)</h3>
<p>
In an ideal world, we would publish with no mistakes. The world is not ideal. The second best thing then is to keep a list of mistakes (and update the pdf!).
For your convenience, here I give you an errata both as a <a href="http://www.ualberta.ca/~szepesva/RLBook/Errata.pdf" target=&ldquo;blank&rdquo;>pdf</a> file and also in <a href="http://www.ualberta.ca/~szepesva/RLBook/Errata.html" target=&ldquo;blank&rdquo;>html</a>.


The above errata is based mostly on a list provided by <a href="http://www.ualberta.ca/~bartok" target=&ldquo;blank&rdquo;>Gabor</a>, who deserves a big thanks for reading through the text so carefully. I am also indebted to
<a href="https://sotets.uk" target=&ldquo;blank&rdquo;>Sotetsu Koyamada</a>
who more recently have given me another lengthy list of typos.
Earlier (and more recently), several individual read various parts of the draft and have submitted useful suggestions, which I tried
to incorporate.
They include:</p>
<ol>
<li><p><a href="http://web.mit.edu/dimitrib/www/home.html" target=&ldquo;blank&rdquo;>Dimitri Bertsekas</a></p>
</li>
<li><p><a href="http://webdocs.cs.ualberta.ca/~gbalazs" target=&ldquo;blank&rdquo;>Gabor Balazs</a></p>
</li>
<li><p><a href="http://www.facebook.com/obernardo" target=&ldquo;blank&rdquo;>Bernardo Avila Pires</a></p>
</li>
<li><p><a href="http://www.castlelab.princeton.edu/Staff.html" target=&ldquo;blank&rdquo;>Warren Powell</a></p>
</li>
<li><p><a href="http://www.idsia.ch/~tom/" target=&ldquo;blank&rdquo;>Tom Schaul</a></p>
</li>
<li><p><a href="http://webdocs.cs.ualberta.ca/~sutton/index.html" target=&ldquo;blank&rdquo;>Rich Sutton</a></p>
</li>
<li><p><a href="http://www.dpem.tuc.gr/robolab/ENPAGES/EN_PERSONNEL/EN_VLASSIS.htm" target=&ldquo;blank&rdquo;>Nikos Vlassis</a>,</p>
</li>
<li><p><a href="http://webdocs.cs.ualberta.ca/~hengshua/" target=&ldquo;blank&rdquo;>Hengshuai Yao</a></p>
</li>
<li><p><a href="http://staff.science.uva.nl/~whiteson/Shimon_Whiteson/Home.html" target=&ldquo;blank&rdquo;>Shimon Whiteson</a>,</p>
</li>
<li><p><a href="https://github.com/mtomassoli" target=&ldquo;blank&rdquo;>Massimiliano Tomassoli</a> and</p>
</li>
<li><p>Hill Ma.
</p>
</li>
</ol>
<p><p>
Thank You! All the remaining errors are mine.</p>
<div id="footer">
<div id="footer-text">
Page generated 2020-03-17, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-19435627-1";
urchinTracker();
</script>
</body>
</html>
