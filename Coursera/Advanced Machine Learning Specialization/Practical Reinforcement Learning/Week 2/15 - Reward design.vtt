WEBVTT

1
00:00:03.150 --> 00:00:07.965
So, welcome to week two of our reinforcement learning course.

2
00:00:07.965 --> 00:00:09.850
During this week, we will talk about

3
00:00:09.850 --> 00:00:13.090
core concepts lying at the heart reinforcement learning.

4
00:00:13.090 --> 00:00:16.010
How to explain to agent what do we want it to do?

5
00:00:16.010 --> 00:00:19.295
The answer to this question is reward hypothesis,

6
00:00:19.295 --> 00:00:22.380
which states that we could formulate any goal and

7
00:00:22.380 --> 00:00:26.025
any purpose in terms of cumulative sum of scalar signal.

8
00:00:26.025 --> 00:00:28.375
This signal is called reward,

9
00:00:28.375 --> 00:00:31.780
and the sum of the signal over time is called a return.

10
00:00:31.780 --> 00:00:37.685
So, the return G sub t is a sum of immediate rewards from arbitrary time t,

11
00:00:37.685 --> 00:00:39.850
until the very end of the episode.

12
00:00:39.850 --> 00:00:44.685
The end of the episode is denoted by a capital T. This return consists of

13
00:00:44.685 --> 00:00:47.170
all rewards to the end of the episode and

14
00:00:47.170 --> 00:00:50.265
thus is a measure of global optimality of agent policy.

15
00:00:50.265 --> 00:00:54.470
Return is a random variable because each immediate reward depends on

16
00:00:54.470 --> 00:00:58.665
the agent action and also depends on the environment reaction to this action.

17
00:00:58.665 --> 00:01:02.325
For now, consider as example, the game of chess.

18
00:01:02.325 --> 00:01:05.210
Let's assume that we have designed the immediate rewards to be

19
00:01:05.210 --> 00:01:08.310
a value of all opponent's piece taken at a particular timestep,

20
00:01:08.310 --> 00:01:11.050
T. So, the return is equal to the total value of

21
00:01:11.050 --> 00:01:15.860
all opponent's pieces an agent have managed to take till the end of the game.

22
00:01:15.860 --> 00:01:22.535
Also, mathematically convenient, such formulation of our desire could have side effects.

23
00:01:22.535 --> 00:01:27.115
So, to better understand the limitations of cumulative sum of immediate rewards,

24
00:01:27.115 --> 00:01:31.415
consider another example of data center non-stop cooling system.

25
00:01:31.415 --> 00:01:35.330
An agent controls temperature in data center room and could adjust

26
00:01:35.330 --> 00:01:39.590
speed of different fans to enforce required temperature regime.

27
00:01:39.590 --> 00:01:42.510
We reward this agent with plus one for

28
00:01:42.510 --> 00:01:45.000
each second the system's temperature was sufficiently

29
00:01:45.000 --> 00:01:48.500
low and give it reward equal to zero otherwise.

30
00:01:48.500 --> 00:01:52.840
So, can you think of any possible problem in such a design?

31
00:01:52.840 --> 00:01:56.330
Well, the problem here is the length of the episode.

32
00:01:56.330 --> 00:01:59.625
Unlike the chess games this task doesn't have natural ending.

33
00:01:59.625 --> 00:02:02.205
That is, cooling system is meant to operate, well,

34
00:02:02.205 --> 00:02:03.635
in every day of the week,

35
00:02:03.635 --> 00:02:07.080
every single moment and operate in that way forever.

36
00:02:07.080 --> 00:02:13.105
The essence solves the problem with infinite horizon lies in optimization problem,

37
00:02:13.105 --> 00:02:15.395
that is, our objective, our return,

38
00:02:15.395 --> 00:02:18.625
is infinite for a myriad of non optimal behaviors.

39
00:02:18.625 --> 00:02:21.420
For instance, it is indeed infinite for an agent,

40
00:02:21.420 --> 00:02:26.040
behaviors that violates temperature regime at every search timestep.

41
00:02:26.040 --> 00:02:30.955
Tasks who has infinite horizons are often called continuing tasks.

42
00:02:30.955 --> 00:02:35.365
So of course, we could split such infinite horizon in some fixed length chunks.

43
00:02:35.365 --> 00:02:38.280
For example, hours, or days and assess

44
00:02:38.280 --> 00:02:42.005
agent's performance on the basis of performance during these chunks.

45
00:02:42.005 --> 00:02:48.105
However, as this approach requires manual decisions of what chunk length is appropriate,

46
00:02:48.105 --> 00:02:53.285
and this approach does not generalize well to arbitrary problem.

47
00:02:53.285 --> 00:02:59.920
Infinite horizon is not the only problem which may occur with the formulation of return.

48
00:02:59.920 --> 00:03:03.695
To better understand another problem with cumulative sum of rewards,

49
00:03:03.695 --> 00:03:06.430
consider as an example of an agent responsible for

50
00:03:06.430 --> 00:03:09.450
floor cleaning and air conditioning in a building.

51
00:03:09.450 --> 00:03:13.740
So, we encourage our agent to clean the floor by rewarding cleaning action with

52
00:03:13.740 --> 00:03:19.095
large value of 100 but because we also want air in the building to be fresh,

53
00:03:19.095 --> 00:03:23.970
we also encourage an agent for turning air conditioning system on and subsequently off,

54
00:03:23.970 --> 00:03:26.295
with reward equal to one.

55
00:03:26.295 --> 00:03:29.290
For this example, there is no infinite horizon

56
00:03:29.290 --> 00:03:32.005
because the episode ends when the day is over.

57
00:03:32.005 --> 00:03:35.915
So, what potential problem do you see in such a design?

58
00:03:35.915 --> 00:03:40.735
Well, this example illustrates a problem of positive feedback loop.

59
00:03:40.735 --> 00:03:43.500
This positive feedback loop is a cycle which

60
00:03:43.500 --> 00:03:46.980
allows an agent to gain large or possibly infinite reward.

61
00:03:46.980 --> 00:03:50.560
In this example, an agent will completely ignored the task of

62
00:03:50.560 --> 00:03:54.475
floor cleaning because this activity takes the whole day to finish.

63
00:03:54.475 --> 00:03:58.385
While turning air conditioning on off takes one second.

64
00:03:58.385 --> 00:04:00.510
And by doing this sequence of actions many,

65
00:04:00.510 --> 00:04:01.990
many times throughout the day,

66
00:04:01.990 --> 00:04:06.960
and agent is able to obtain cumulative reward much larger than 100.

67
00:04:06.960 --> 00:04:10.980
Well, the feedback loop in this task is an example of a bad reward design.

68
00:04:10.980 --> 00:04:14.810
We will discuss the reward design a little bit later, but for now,

69
00:04:14.810 --> 00:04:19.700
think about the environments where this infinite loop is in fact desire.

70
00:04:19.700 --> 00:04:23.560
For example, giving a positive reward for each second an agent is riding

71
00:04:23.560 --> 00:04:27.725
a bicycle is an example of a desire for positive feedback loop.

72
00:04:27.725 --> 00:04:32.685
We want an agent to never fall off the bike and in this case,

73
00:04:32.685 --> 00:04:36.200
with agent getting better and better in riding a bicycle,

74
00:04:36.200 --> 00:04:40.205
we could also face the problem of unboundedly increasing sum of rewards.

75
00:04:40.205 --> 00:04:44.240
This unboundedness can greatly harm our optimization procedure and thus,

76
00:04:44.240 --> 00:04:46.005
could break that learning process.

77
00:04:46.005 --> 00:04:48.795
But, how could we deal with infinite return?

78
00:04:48.795 --> 00:04:54.305
What could help us against the return being very large due to positive infinite loop?

79
00:04:54.305 --> 00:04:57.435
One of the most common approaches is discounting.

80
00:04:57.435 --> 00:05:00.740
Discounting means that we introduce some multiplier gamma,

81
00:05:00.740 --> 00:05:03.970
which is less than one and greater or equal to zero.

82
00:05:03.970 --> 00:05:09.810
This multiplier present rate with which things lose their value over time.

83
00:05:09.810 --> 00:05:12.385
That is, each reward receive later,

84
00:05:12.385 --> 00:05:13.735
as in current moment t,

85
00:05:13.735 --> 00:05:16.820
is reduced by multiplying the reward with the gamma,

86
00:05:16.820 --> 00:05:20.040
to the power of number of timesteps to this return.

87
00:05:20.040 --> 00:05:23.640
So discounting focusses agents attention

88
00:05:23.640 --> 00:05:27.735
more on close rewards and reduce the value of very distant once.

89
00:05:27.735 --> 00:05:34.220
More informally, the same cake compared to today's one was gamma times less tomorrow,

90
00:05:34.220 --> 00:05:38.110
gamma square times less the day after tomorrow, and so on.

91
00:05:38.110 --> 00:05:41.290
So that rewards received in n timesteps in the future

92
00:05:41.290 --> 00:05:44.930
is worth gamma to the power of N minus one times less,

93
00:05:44.930 --> 00:05:48.180
compared with the same reward acting right now.

94
00:05:48.180 --> 00:05:51.530
Such discounting allows to make infinite sum finite,

95
00:05:51.530 --> 00:05:53.450
providing that each term, is assumed,

96
00:05:53.450 --> 00:05:56.200
is bounded was from above and below.

97
00:05:56.200 --> 00:06:00.425
Well, why does this discounting solves the problem of return being infinite?

98
00:06:00.425 --> 00:06:04.410
This is so because of mathematical probity of geometric progression.

99
00:06:04.410 --> 00:06:07.435
So for example, if each immediate reward is equal to one,

100
00:06:07.435 --> 00:06:10.200
as an example of this data center cooling system,

101
00:06:10.200 --> 00:06:14.750
that infant sum is equal to the value of one over one minus gamma.

102
00:06:14.750 --> 00:06:17.200
Maximal return have nothing to do with

103
00:06:17.200 --> 00:06:20.365
the number of steps after which agent does not care,

104
00:06:20.365 --> 00:06:23.845
and this fact is useful to keep in mind.

105
00:06:23.845 --> 00:06:27.710
On the floors, you can see why rewards of plus one would be equal

106
00:06:27.710 --> 00:06:31.325
two after being discounting N times was different gammas.

107
00:06:31.325 --> 00:06:35.945
Know that slope of any curve decreases while increases N,

108
00:06:35.945 --> 00:06:39.775
this decrease suggests that even in the near future,

109
00:06:39.775 --> 00:06:44.130
are discounted at a higher discounted rate then events in the distant future.

110
00:06:44.130 --> 00:06:48.440
Also know that in case gamma is not exactly equal to zero,

111
00:06:48.440 --> 00:06:53.940
every reward, even infinitely distant one contributes to the return computation.

112
00:06:53.940 --> 00:06:55.390
It may contribute very,

113
00:06:55.390 --> 00:06:58.190
very little for very distant rewards,

114
00:06:58.190 --> 00:07:00.000
but it certainly does so.

115
00:07:00.000 --> 00:07:03.165
That is our agent still cares about distant rewards,

116
00:07:03.165 --> 00:07:06.795
but not as much as in case of undiscounted return.

117
00:07:06.795 --> 00:07:10.180
However, you should understand that agent

118
00:07:10.180 --> 00:07:13.410
will be almost indifferent to the very distant rewards,

119
00:07:13.410 --> 00:07:16.235
and this change of optimization objective

120
00:07:16.235 --> 00:07:21.445
definitely change what optimal policy looks like in each of these cases.

121
00:07:21.445 --> 00:07:26.575
So, why is discounting meaningful thing to do?

122
00:07:26.575 --> 00:07:28.760
Well, the reason of discounting is

123
00:07:28.760 --> 00:07:33.325
partially mathematical convenience and partially inspiration from human behavior.

124
00:07:33.325 --> 00:07:35.320
Humans, just like animals,

125
00:07:35.320 --> 00:07:37.845
given two similar rewards show of preference for

126
00:07:37.845 --> 00:07:41.140
the one that arrives sooner rather than later.

127
00:07:41.140 --> 00:07:44.170
Humans also discount the value of the later their work by

128
00:07:44.170 --> 00:07:47.580
a factor that increases with the length of the delay.

129
00:07:47.580 --> 00:07:51.350
However, a scientific literature suggests that human and animal discounting

130
00:07:51.350 --> 00:07:55.840
function f(t) is different and is more like hyperbolic discounting.

131
00:07:55.840 --> 00:07:57.960
The discounting function f(t) used in

132
00:07:57.960 --> 00:08:01.995
the reinforcement learning is very similar to the so-called quasi-hyperbolic discounting.

133
00:08:01.995 --> 00:08:06.650
Mainly, if we assume beta in quasi-hyperbolic discounting is equal to one,

134
00:08:06.650 --> 00:08:11.045
then we get precisely the same discounting scheme you have been shown on previous slide.

135
00:08:11.045 --> 00:08:13.790
In some sense, a quasi-hyperbolic discounting is

136
00:08:13.790 --> 00:08:16.825
a discrete approximation to a hyperbolic discounting function.

137
00:08:16.825 --> 00:08:19.525
And thus, it's rather close to how human discount.

138
00:08:19.525 --> 00:08:21.850
However, unlike hyperbolic discounting,

139
00:08:21.850 --> 00:08:24.355
it has some nice mathematical properties.

140
00:08:24.355 --> 00:08:29.120
The second reason of this particular kind of discounting is mathematical convenience.

141
00:08:29.120 --> 00:08:32.310
It not only makes infinite sums finite,

142
00:08:32.310 --> 00:08:35.690
it preserve some amount of contribution for each reward,

143
00:08:35.690 --> 00:08:39.570
but it also allows us to express the return in a recurrent function.

144
00:08:39.570 --> 00:08:43.895
We will heavily rely upon this recourse of definition of Gt,

145
00:08:43.895 --> 00:08:49.210
through Rt plus gamma multiplied by Gt plus one in future lectures.

146
00:08:49.210 --> 00:08:52.370
So, make sure you understand and remember it.

147
00:08:52.370 --> 00:08:56.965
Why do we also think about discounting from a different more theoretical perspective?

148
00:08:56.965 --> 00:08:58.305
Under mark of assumption,

149
00:08:58.305 --> 00:09:02.185
any action affects only the immediate reward and the next state.

150
00:09:02.185 --> 00:09:03.890
Well, any action could

151
00:09:03.890 --> 00:09:08.770
affect all or some of the future rewards by affecting the next state.

152
00:09:08.770 --> 00:09:12.500
That is, when you find a pressure in corner of the room,

153
00:09:12.500 --> 00:09:15.020
you definitely should assign them credit related to this

154
00:09:15.020 --> 00:09:17.950
rewards to the action of opening the door to this room.

155
00:09:17.950 --> 00:09:21.480
So, let's assume that the action effect lasts

156
00:09:21.480 --> 00:09:25.860
some subsequent number of steps after the action was committed, and then end.

157
00:09:25.860 --> 00:09:30.625
Let us also treat gamma S probability of effect continuation,

158
00:09:30.625 --> 00:09:34.925
and one minus gamma S probability is that effect ends.

159
00:09:34.925 --> 00:09:37.130
Then, the expected amount of return,

160
00:09:37.130 --> 00:09:42.400
which is due to current action is exactly equal to the discounted return.

161
00:09:42.400 --> 00:09:47.070
That is, with probability one minus gamma effect as immediately after

162
00:09:47.070 --> 00:09:50.030
the action and only the immediate reward error

163
00:09:50.030 --> 00:09:52.605
zero is attributed to the action committed now.

164
00:09:52.605 --> 00:09:56.995
With probability gamma effect less two timesteps and then,

165
00:09:56.995 --> 00:09:59.535
ends with probability one minus gamma.

166
00:09:59.535 --> 00:10:05.405
And in this case, R0 and R1 are attributed to the current action, and so on.

167
00:10:05.405 --> 00:10:07.110
So, you get the idea.

168
00:10:07.110 --> 00:10:10.300
Let us now speak a little bit about reward design.

169
00:10:10.300 --> 00:10:12.730
Two examples given in this lecture are,

170
00:10:12.730 --> 00:10:15.295
in fact, examples of a bad reward design.

171
00:10:15.295 --> 00:10:17.770
Consider for example, a game of chess and

172
00:10:17.770 --> 00:10:20.305
reward equal to the value of taken opponent's piece.

173
00:10:20.305 --> 00:10:22.770
In this case, an agent will not have the desire to

174
00:10:22.770 --> 00:10:26.335
win because it knows for sure that it will not be rewarded.

175
00:10:26.335 --> 00:10:31.085
That is so because we reward an agent only for taking pieces.

176
00:10:31.085 --> 00:10:33.000
And the cleaning robot example,

177
00:10:33.000 --> 00:10:36.855
also you know the reward given for cleaning the floor.

178
00:10:36.855 --> 00:10:39.340
To these examples share the same mistakes.

179
00:10:39.340 --> 00:10:42.430
And both of them, reward is given for how an agent should

180
00:10:42.430 --> 00:10:46.625
perform it is and not for what to do we wanted to do.

181
00:10:46.625 --> 00:10:49.530
In the game of chess, we should reward an agent for winning

182
00:10:49.530 --> 00:10:53.050
the game and not for taking pieces because the

183
00:10:53.050 --> 00:10:55.715
later could simply result in losing

184
00:10:55.715 --> 00:10:59.340
every single game with a lot of opponent's pieces taken.

185
00:10:59.340 --> 00:11:01.585
That is surely not what we want.

186
00:11:01.585 --> 00:11:04.010
The same is valid for the second example.

187
00:11:04.010 --> 00:11:06.405
We should reward an agent for fresh air and

188
00:11:06.405 --> 00:11:10.425
clean floor but not for using the means to achieve this.

189
00:11:10.425 --> 00:11:14.990
However, as such sparse rewards given only in the very end of an event,

190
00:11:14.990 --> 00:11:17.660
say plus one in the chess game for winning,

191
00:11:17.660 --> 00:11:21.225
may introduce additional difficulties in agent's learning.

192
00:11:21.225 --> 00:11:24.090
To some extent, this may be mitigated by your work shaping,

193
00:11:24.090 --> 00:11:26.010
which we'll discuss a bit later.

194
00:11:26.010 --> 00:11:28.630
The next concern is about reward shifting.

195
00:11:28.630 --> 00:11:30.275
In the machine learning courses,

196
00:11:30.275 --> 00:11:32.665
you may get used to standardizing

197
00:11:32.665 --> 00:11:36.275
training and testing the data before doing anything with this data.

198
00:11:36.275 --> 00:11:39.420
Well, for example, means of instruction and division by

199
00:11:39.420 --> 00:11:43.365
standard deviation is often a good idea in supervised and unsupervised learning.

200
00:11:43.365 --> 00:11:44.780
In the reinforcement learning,

201
00:11:44.780 --> 00:11:49.545
this is valid only for state representations but not for reward.

202
00:11:49.545 --> 00:11:52.700
You should not standardize rewards.

203
00:11:52.700 --> 00:11:55.010
So, let us see what will happen with

204
00:11:55.010 --> 00:11:59.490
optimal policy if you shift all the rewards by some mean value.

205
00:11:59.490 --> 00:12:07.345
So consider a world with four states and episode starts in S1 and ends in S4.

206
00:12:07.345 --> 00:12:11.710
Rewards for transitions are reason both the arrows.

207
00:12:11.710 --> 00:12:13.990
In the following example,

208
00:12:13.990 --> 00:12:17.315
mean other words will be negative, say, minus three.

209
00:12:17.315 --> 00:12:20.010
And suppression of this value will turns

210
00:12:20.010 --> 00:12:25.190
a negative feedback loop between states S2 and S3 into a positive feedback loop.

211
00:12:25.190 --> 00:12:27.440
So, optimal action for this world with

212
00:12:27.440 --> 00:12:31.290
modified rewards will definitely make use of this positive feedback loop.

213
00:12:31.290 --> 00:12:34.700
However, in the world with untransparent rewards,

214
00:12:34.700 --> 00:12:38.965
this policy will get possibly unboundedly bad the return.

215
00:12:38.965 --> 00:12:42.440
So the second takeaway is do not subtract anything

216
00:12:42.440 --> 00:12:46.005
from rewards unless you're pretty sure what you're doing.

217
00:12:46.005 --> 00:12:49.370
So, what transformations do not change optimal policy?

218
00:12:49.370 --> 00:12:52.605
One of such transformations is reward scaling.

219
00:12:52.605 --> 00:12:55.415
That means you can divide all the rewards in

220
00:12:55.415 --> 00:12:58.535
market decision process by any non-zero constant.

221
00:12:58.535 --> 00:13:02.330
And be sure that the optimal policy wasn't changed.

222
00:13:02.330 --> 00:13:06.160
This scaling by another constant may be especially helpful if you know

223
00:13:06.160 --> 00:13:10.470
beforehand the greatest immediate reward possible in a market decision process.

224
00:13:10.470 --> 00:13:13.530
And it is also useful for approximate methods,

225
00:13:13.530 --> 00:13:16.195
which we are going to cover in future videos.

226
00:13:16.195 --> 00:13:19.760
In other transformation which doesn't change the optimal policy in

227
00:13:19.760 --> 00:13:24.060
a market decision process is related to the so-called reward shaping.

228
00:13:24.060 --> 00:13:29.210
Reward shaping is an umbrella term for many methods that get an agent in

229
00:13:29.210 --> 00:13:34.410
a learning process by adding an extra values to the immediate rewards coming from an MDP.

230
00:13:34.410 --> 00:13:37.655
In general, under such reward shaping,

231
00:13:37.655 --> 00:13:39.505
the optimal policy changes.

232
00:13:39.505 --> 00:13:42.870
But there exists an important theory specifying how

233
00:13:42.870 --> 00:13:47.005
extra rewards should look like in order to preserve the optimal policy.

234
00:13:47.005 --> 00:13:50.210
In fact, these extra values should be equal to

235
00:13:50.210 --> 00:13:53.670
the difference of the potentials of next and current states.

236
00:13:53.670 --> 00:13:56.950
Such potential function is denoted by pi on this slide.

237
00:13:56.950 --> 00:14:01.280
We can also multiply the potential of the next state by a discount factor,

238
00:14:01.280 --> 00:14:03.090
gamma, if we want.

239
00:14:03.090 --> 00:14:05.690
The resultant potential-based function,

240
00:14:05.690 --> 00:14:07.630
F of SA and its prime,

241
00:14:07.630 --> 00:14:09.255
is presented on the slide.

242
00:14:09.255 --> 00:14:13.880
In particular, if I know nothing about condition probabilities and rewards in an MDP,

243
00:14:13.880 --> 00:14:17.250
the potential-based function F or the form depicted on

244
00:14:17.250 --> 00:14:22.425
this slide is the only one that preserves the optimal policy under reward shaping.

245
00:14:22.425 --> 00:14:25.625
The intuition of why does such addition of F

246
00:14:25.625 --> 00:14:29.125
later and change optimal policies may be as follows.

247
00:14:29.125 --> 00:14:33.005
Pretend for simplicity that discount factor gamma is equal to one,

248
00:14:33.005 --> 00:14:35.860
that is there are no discounting in an MDP.

249
00:14:35.860 --> 00:14:41.875
In such a case, the potential-based function sound or any circular sequence of states,

250
00:14:41.875 --> 00:14:44.445
that is starting and ending in the same state,

251
00:14:44.445 --> 00:14:46.340
is precisely equal to zero.

252
00:14:46.340 --> 00:14:50.060
That is why an agent cannot fall into a trap of

253
00:14:50.060 --> 00:14:54.435
a positive feedback loop caused by the introduction of potential-based rewards.

254
00:14:54.435 --> 00:14:58.820
Also, note that the added values do not depend on action A,

255
00:14:58.820 --> 00:15:02.150
but only on current and next state potentials.

256
00:15:02.150 --> 00:15:07.295
Thus, the potentials pi cannot influence that action selection directly,

257
00:15:07.295 --> 00:15:09.900
only by the means or the next state.

258
00:15:09.900 --> 00:15:15.170
But the influence of the next state potential is also illuminated because it is

259
00:15:15.170 --> 00:15:20.820
obstructed from the total return once we subsequently exit the next state as pi.

260
00:15:20.820 --> 00:15:24.830
This potential-based shaping function trick either other advanced technique.

261
00:15:24.830 --> 00:15:27.245
So, if you are get interested,

262
00:15:27.245 --> 00:15:29.040
you are highly encouraged to read

263
00:15:29.040 --> 00:15:32.650
the relevant paper specified at the bottom of the slide.