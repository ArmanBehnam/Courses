WEBVTT

1
00:00:02.620 --> 00:00:09.315
Most likely, you are already aware of energy and momentum conservation laws.

2
00:00:09.315 --> 00:00:13.680
Energy and momentum of an isolated system should be constant,

3
00:00:13.680 --> 00:00:16.395
or as they say, conserved.

4
00:00:16.395 --> 00:00:20.505
There are plenty of other quantities in physics that are

5
00:00:20.505 --> 00:00:25.125
kept at constant level during an isolated system of illusion.

6
00:00:25.125 --> 00:00:28.920
Thanks to one of the most profound theorems

7
00:00:28.920 --> 00:00:32.680
proven at the first half of 20th century by Emmy Noether

8
00:00:32.680 --> 00:00:40.474
every conservation law is tightly connected to certain kind of symmetry in the universe.

9
00:00:40.474 --> 00:00:45.845
For example, energy consideration is connected to time uniformity.

10
00:00:45.845 --> 00:00:51.530
For example, it doesn't matter if you start the experiment today or tomorrow,

11
00:00:51.530 --> 00:00:57.125
it will develop the same way under assumption of isolated system of course.

12
00:00:57.125 --> 00:01:00.255
Let's recall what Lepton Flavor is.

13
00:01:00.255 --> 00:01:03.430
For every decay, we can compute the number of

14
00:01:03.430 --> 00:01:07.970
the first generation lepton second-generation leptons and third-generation leptons.

15
00:01:07.970 --> 00:01:10.995
Similarly, there is a quark flavor number.

16
00:01:10.995 --> 00:01:14.790
It is known not to be conserved or violated.

17
00:01:14.790 --> 00:01:16.860
Look at the decay of B meson.

18
00:01:16.860 --> 00:01:20.355
It consist of Anti-B and s quark.

19
00:01:20.355 --> 00:01:23.625
After transformation of W boson,

20
00:01:23.625 --> 00:01:25.690
we end up with a charm,

21
00:01:25.690 --> 00:01:29.090
anti-charm pair that is called gepside and a strange,

22
00:01:29.090 --> 00:01:33.850
anti-strange pair that joins into particle that is called Phi.

23
00:01:33.850 --> 00:01:38.155
Neutrinos can also transform into each other.

24
00:01:38.155 --> 00:01:43.300
The Nobel Prize in 2015 was awarded to Takaaki Kajita

25
00:01:43.300 --> 00:01:49.084
and Arthur McDonald's exactly for predictions of this possibility.

26
00:01:49.084 --> 00:01:55.580
However, standard model predictions for charged lepton is negligible.

27
00:01:55.580 --> 00:02:02.790
At the same time, so the expectation for charged Lepton Flavor Violation are much higher,

28
00:02:02.790 --> 00:02:09.390
one of the simplest decay that conserve energy charge lepton number,

29
00:02:09.390 --> 00:02:15.015
but violates the lepton flavor number is presented at the slide bottom.

30
00:02:15.015 --> 00:02:20.510
It is a transformation of muon into an electron and a gamma quant.

31
00:02:20.510 --> 00:02:22.285
So on this slide,

32
00:02:22.285 --> 00:02:28.555
you see example of more complicated decays that violate lepton flavor.

33
00:02:28.555 --> 00:02:31.690
So for example on top right diagram,

34
00:02:31.690 --> 00:02:36.985
you have a Tau particle that decays into three muons.

35
00:02:36.985 --> 00:02:42.920
Probability of this decay in Standard Model is very low,

36
00:02:42.920 --> 00:02:46.080
so it is less than 10 to -40,

37
00:02:46.080 --> 00:02:49.555
so we can not measure process of

38
00:02:49.555 --> 00:02:55.935
such probability using LHC or other existing technologies.

39
00:02:55.935 --> 00:03:03.390
But according to new physics predictions for example in supersymmetry,

40
00:03:03.390 --> 00:03:08.315
such decay can happen thanks to other particles or

41
00:03:08.315 --> 00:03:13.890
bosons that do not exist in regular standard model.

42
00:03:13.890 --> 00:03:19.410
So you can see on the right example and you can see below examples of

43
00:03:19.410 --> 00:03:23.100
such decays and probability of those might

44
00:03:23.100 --> 00:03:28.130
be much higher than predictions from standard model.

45
00:03:28.130 --> 00:03:34.914
The analysis strategy for the decay of Tau to three muons could be the following.

46
00:03:34.914 --> 00:03:39.370
Since we are looking for the decay of something like depicted below,

47
00:03:39.370 --> 00:03:43.960
we want our trigger to catch muons that come from a single vertex.

48
00:03:43.960 --> 00:03:49.735
Having three of those reduces the amount of background quite drastically.

49
00:03:49.735 --> 00:03:52.185
Additional constraint comes from the fact that

50
00:03:52.185 --> 00:03:55.995
Tau flies some distance from the proton collision point,

51
00:03:55.995 --> 00:04:01.290
so the source of those muons should be distant from primary vertex.

52
00:04:01.290 --> 00:04:05.885
There might be other restriction on muon momentum and energy.

53
00:04:05.885 --> 00:04:11.790
Then we have to design an event selection technique based on machine learning.

54
00:04:11.790 --> 00:04:13.050
But not to spoil the fun,

55
00:04:13.050 --> 00:04:19.185
we have to hide particular part of the data before we are happy with the classifier.

56
00:04:19.185 --> 00:04:21.735
This is called blinding.

57
00:04:21.735 --> 00:04:28.120
We train our classifier on the mixture of data and simulated data.

58
00:04:28.120 --> 00:04:30.805
As we're satisfied whether its properties,

59
00:04:30.805 --> 00:04:37.110
we can apply to signal region and estimate number of events passing the selection,

60
00:04:37.110 --> 00:04:40.260
but then we have to convert this number into branching

61
00:04:40.260 --> 00:04:44.915
fractions somehow to compare it with the predictions of standard model.

62
00:04:44.915 --> 00:04:50.320
That's why we might need normalization and calibration channel.

63
00:04:50.320 --> 00:04:52.940
If we apply the same selection to this channel,

64
00:04:52.940 --> 00:04:58.790
we get the number of events that correspond to the well known branching fraction.

65
00:04:58.790 --> 00:05:03.330
Since the topology of the calibration channel is very similar,

66
00:05:03.330 --> 00:05:10.725
it is assumed that the ratio of counted events to branching fraction should be the same.

67
00:05:10.725 --> 00:05:14.570
So why blending is useful.

68
00:05:14.570 --> 00:05:22.380
So let's recap that signal region it is region of mass spectrum with high probability of

69
00:05:22.380 --> 00:05:26.070
a signal and the probability of a signal is very

70
00:05:26.070 --> 00:05:31.260
different from the probability of a background at this region.

71
00:05:31.260 --> 00:05:40.240
So it is due to the Feynman diagram and the nature of the process of the decay.

72
00:05:40.240 --> 00:05:43.455
So in this region is hidden during

73
00:05:43.455 --> 00:05:47.565
analysis to avoid psychological or experimental is bias.

74
00:05:47.565 --> 00:05:52.620
So to avoid having decisions on which

75
00:05:52.620 --> 00:05:58.070
cut should we apply or when should we stop analysis or search for bug,

76
00:05:58.070 --> 00:06:03.740
just looking at the data that we have to analyze and come to the final conclusion.

77
00:06:03.740 --> 00:06:11.630
So this plot shows a hypothetical distributions for example

78
00:06:11.630 --> 00:06:15.860
signal is in blue color and distribution of the background in

79
00:06:15.860 --> 00:06:20.355
the black and the innermost region is the signal region.

80
00:06:20.355 --> 00:06:24.740
The distribution in the outer region is

81
00:06:24.740 --> 00:06:29.800
used to interpolate the background contribution in the signal region,

82
00:06:29.800 --> 00:06:34.810
and the narrow regions are used for analysis optimization.

83
00:06:34.810 --> 00:06:37.220
In rare decays searches,

84
00:06:37.220 --> 00:06:43.770
blinding is done by defining the entire analysis prior to

85
00:06:43.770 --> 00:06:47.490
evaluating the part of the data in which your signal is saught for.

86
00:06:47.490 --> 00:06:52.080
This part also referred to as signal region.

87
00:06:52.080 --> 00:06:54.250
In case of Tau to three muons,

88
00:06:54.250 --> 00:07:00.775
the candidates with invariant mass between M Tau minus 20 MeV.

89
00:07:00.775 --> 00:07:05.975
M Tau plus 20 MeV were removed from the analysis

90
00:07:05.975 --> 00:07:11.609
from the dataset for the development of the strategy and the classifier optimization.

91
00:07:11.609 --> 00:07:13.560
Once the analysis is defined,

92
00:07:13.560 --> 00:07:16.700
the signal region is analyzed.

93
00:07:16.700 --> 00:07:20.400
This means that the number of candidates is

94
00:07:20.400 --> 00:07:24.915
evaluated and can be compared to the expectation.

95
00:07:24.915 --> 00:07:30.100
So, to discriminate between signal and background,

96
00:07:30.100 --> 00:07:36.420
we should include such features as vertex fit quality.

97
00:07:36.420 --> 00:07:39.740
How well muons actually come together,

98
00:07:39.740 --> 00:07:42.890
displacement from the primary vertex to

99
00:07:42.890 --> 00:07:49.525
how distant the trajectory of a particle or a secondary vertex from the primary vertex.

100
00:07:49.525 --> 00:07:52.065
Track quality, track isolation,

101
00:07:52.065 --> 00:07:57.760
and samples that are used for training of classifier are

102
00:07:57.760 --> 00:08:05.290
taken from Monte Carlo simulation for the signal and real data for the background.

103
00:08:05.290 --> 00:08:14.515
Similar channel with similar topology of the s decaying into Phi and pi,

104
00:08:14.515 --> 00:08:18.270
used for calibration and normalization of the classifier,

105
00:08:18.270 --> 00:08:24.570
and as a metric or proxy metrics because we are interested in branching fraction,

106
00:08:24.570 --> 00:08:28.925
which might not be directly related to this metric.

107
00:08:28.925 --> 00:08:33.050
We use a AUC area and ROC curve metric.

108
00:08:33.050 --> 00:08:35.380
So, as we mentioned before,

109
00:08:35.380 --> 00:08:40.650
the signal in the mass region or a single region,

110
00:08:40.650 --> 00:08:44.370
have very different shape from the background.

111
00:08:44.370 --> 00:08:49.270
So, you can probably spot the problem,

112
00:08:49.270 --> 00:08:56.370
that if we give mass or feature that correlates with the mass to the classifier,

113
00:08:56.370 --> 00:09:03.210
we might get a biased estimation of number of background classifier.

114
00:09:03.210 --> 00:09:07.425
Number of background events will come to this point a little bit later.

115
00:09:07.425 --> 00:09:09.750
But let's continue with the strategy.

116
00:09:09.750 --> 00:09:11.545
For example, we got the model,

117
00:09:11.545 --> 00:09:17.705
we got the classifier that gives the best area under ROC curve we can imagine.

118
00:09:17.705 --> 00:09:23.150
Of course, there is a question if two classifiers get the same error in the ROC curve,

119
00:09:23.150 --> 00:09:24.180
which one should we choose?

120
00:09:24.180 --> 00:09:27.930
But let's, for the educational part,

121
00:09:27.930 --> 00:09:30.305
leave this question aside.

122
00:09:30.305 --> 00:09:38.465
Get the best threshold for such a classifier that maximizes this fraction on the slide.

123
00:09:38.465 --> 00:09:43.685
Essentially, it is true positive rate squared or number of signals squared

124
00:09:43.685 --> 00:09:50.435
over number of background that are misclassified as a signal,

125
00:09:50.435 --> 00:09:57.015
which gives us roughly estimation of efficiency of our classifier.

126
00:09:57.015 --> 00:10:03.665
Then, we apply such classifier with such a threshold though our real-data sample,

127
00:10:03.665 --> 00:10:07.430
with signal region still hidden.

128
00:10:07.430 --> 00:10:11.520
So, we estimate amount of background events in

129
00:10:11.520 --> 00:10:17.120
the signal region by extrapolating side bends to the signal region.

130
00:10:17.120 --> 00:10:19.895
We'll show it on the next slide.

131
00:10:19.895 --> 00:10:24.160
Then, we unblind this signal region,

132
00:10:24.160 --> 00:10:31.220
and one, apply classifier to it and count the number of events in this region.

133
00:10:31.220 --> 00:10:35.700
Then, we unblind or apply

134
00:10:35.700 --> 00:10:40.865
this classifier to the same signal region of normalization channel,

135
00:10:40.865 --> 00:10:46.615
and count number of events which is Ncal there.

136
00:10:46.615 --> 00:10:50.960
Then, we check hypotheses p-value.

137
00:10:53.090 --> 00:10:57.460
Depending on it, we estimate a branching fraction,

138
00:10:57.460 --> 00:11:00.180
branching ratio or upper limit.

139
00:11:00.180 --> 00:11:05.850
So, how do we count expected number of background events?

140
00:11:05.850 --> 00:11:11.780
So, first, we apply selection of our classifier to sidebands.

141
00:11:11.780 --> 00:11:19.210
Then, we assume parametric PDF for combinatorial background like exponential form,

142
00:11:19.210 --> 00:11:25.220
and we fit the model like exponential model to real-data in the sideband and

143
00:11:25.220 --> 00:11:31.275
check if probability distribution function performs well using one of quality criteria.

144
00:11:31.275 --> 00:11:33.320
Then we extrapolate the model to

145
00:11:33.320 --> 00:11:37.190
the Blind region and compute area under this extrapolation.

146
00:11:37.190 --> 00:11:42.120
Then, we can estimate expected number of background events in this region.

147
00:11:42.120 --> 00:11:44.155
Look at this slide.

148
00:11:44.155 --> 00:11:47.165
So, here, by blue edge,

149
00:11:47.165 --> 00:11:54.920
you have rough estimation of number of background events expected to observe.

150
00:11:54.920 --> 00:11:57.695
So, after we get those numbers,

151
00:11:57.695 --> 00:12:01.600
we can compute the branching fraction.

152
00:12:01.600 --> 00:12:06.309
So, here is the formula that is used to estimate

153
00:12:06.309 --> 00:12:12.320
the branching fraction out of experimentally computable or countable numbers,

154
00:12:12.320 --> 00:12:15.765
and something that you can get out of literature.

155
00:12:15.765 --> 00:12:22.380
So, here is the numbers that are substitute at this formula.

156
00:12:23.550 --> 00:12:28.045
You essentially connect together two things,

157
00:12:28.045 --> 00:12:33.685
the branching fraction and number of signal that you

158
00:12:33.685 --> 00:12:37.430
estimated by counting events in

159
00:12:37.430 --> 00:12:41.800
the signal region after applying a classifier to this region.

160
00:12:41.800 --> 00:12:44.360
So, in case of this analysis,

161
00:12:44.360 --> 00:12:49.045
no significant evidence for an excess of events had been observed.

162
00:12:49.045 --> 00:12:50.860
So, as we've mentioned before,

163
00:12:50.860 --> 00:12:55.665
the Standard Model is both remarkably simple and very powerful.

164
00:12:55.665 --> 00:12:58.910
Nearly every quantity that has been measured

165
00:12:58.910 --> 00:13:02.845
in particle physics falls right on the predicted value,

166
00:13:02.845 --> 00:13:06.680
and for Tau to free muon decay,

167
00:13:06.680 --> 00:13:10.380
we've got something that still waits for discovery.

168
00:13:10.380 --> 00:13:17.090
There were not enough data statistics to say how many signals samples are there.

169
00:13:17.090 --> 00:13:20.510
In such cases, we can only estimate how

170
00:13:20.510 --> 00:13:29.700
likely that the observed value is not higher than a certain threshold.

171
00:13:29.700 --> 00:13:32.150
A particular technique that is called,

172
00:13:32.150 --> 00:13:34.380
Confidence Level Estimation that puts

173
00:13:34.380 --> 00:13:39.535
an upper measurement estimation with specified confidence level.

174
00:13:39.535 --> 00:13:42.625
You can see the colorful plot on the right

175
00:13:42.625 --> 00:13:45.920
that shows the different confidence levels which

176
00:13:45.920 --> 00:13:52.670
are vertical axis for various values of branching fraction which is horizontal axis.

177
00:13:52.670 --> 00:13:58.775
Usually, the upper limit is set by 90 or 95 percent of confidence.

178
00:13:58.775 --> 00:14:06.000
So, the upper limit set by LHCb by these analysis in 2013 is

179
00:14:06.000 --> 00:14:14.420
that the branching fraction of the decay is not above 8.0 with 90 percent confidence.