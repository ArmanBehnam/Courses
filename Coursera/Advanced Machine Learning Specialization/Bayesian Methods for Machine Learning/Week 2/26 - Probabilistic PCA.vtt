WEBVTT

1
00:00:00.000 --> 00:00:04.403
[MUSIC]

2
00:00:04.403 --> 00:00:06.036
Are ice cream sellers evil?

3
00:00:06.036 --> 00:00:10.287
Probably not, well,
at least not all of them.

4
00:00:10.287 --> 00:00:15.170
But I can totally imagine a situation
where the price of the ice cream goes

5
00:00:15.170 --> 00:00:18.305
up whenever the temperature
outside goes up.

6
00:00:18.305 --> 00:00:21.822
And if it's indeed the case,
we can see a plot like this.

7
00:00:21.822 --> 00:00:25.660
Here on the x-axis we have temperature,
and

8
00:00:25.660 --> 00:00:29.804
on the y-axis we have
the price of the ice cream.

9
00:00:29.804 --> 00:00:35.424
And each data point, on some particular
day, we measured the temperature,

10
00:00:35.424 --> 00:00:39.140
we asked some ice cream
seller about his price, and

11
00:00:39.140 --> 00:00:43.471
we plotted this data point in
the two-dimensional plane.

12
00:00:43.471 --> 00:00:48.706
So we can see that these two variables
are strongly correlated here and

13
00:00:48.706 --> 00:00:50.490
related to each other.

14
00:00:50.490 --> 00:00:55.050
Can we exploit this closeness of
this meaning of these two random

15
00:00:55.050 --> 00:00:56.916
variables to each other?

16
00:00:56.916 --> 00:01:02.368
Well, we may say that
these two variables are so

17
00:01:02.368 --> 00:01:08.107
related that you can use
one to measure the other.

18
00:01:08.107 --> 00:01:12.602
For example, if you want to know
your temperature outside and

19
00:01:12.602 --> 00:01:16.585
you forgot your thermometer and
also the smart phone.

20
00:01:16.585 --> 00:01:20.058
You can ask your closest ice
cream dealer for his price and

21
00:01:20.058 --> 00:01:22.308
compute the temperature from that.

22
00:01:22.308 --> 00:01:25.008
Which basically means that
these two numbers are so

23
00:01:25.008 --> 00:01:27.154
related that you don't have to use two.

24
00:01:27.154 --> 00:01:31.964
You can as well use just one of them, and
compute the other from the first one.

25
00:01:31.964 --> 00:01:36.866
Or if you put it a little bit differently,
you can draw a line which

26
00:01:36.866 --> 00:01:41.253
goes through your data and
kind of aligned with your data.

27
00:01:41.253 --> 00:01:46.470
Then you can project each data
point you have on this line.

28
00:01:46.470 --> 00:01:53.977
And this way, instead of two numbers
to describe each data point,

29
00:01:53.977 --> 00:01:59.349
you now can use one,
the position on this line.

30
00:01:59.349 --> 00:02:02.559
And this way you will not lose much.

31
00:02:02.559 --> 00:02:06.475
So if you look at the lengths
of the projections, so

32
00:02:06.475 --> 00:02:10.949
how much information do you
lose when you project points?

33
00:02:10.949 --> 00:02:15.202
And each blue data point is projected
on the corresponding orange one.

34
00:02:15.202 --> 00:02:20.190
You see the lengths are not high,
so you keep most of

35
00:02:20.190 --> 00:02:26.112
the information in your data
set by projecting on this line.

36
00:02:26.112 --> 00:02:28.961
And now,
instead of this two-dimensional data,

37
00:02:28.961 --> 00:02:31.486
you can use a one-dimensional projection.

38
00:02:31.486 --> 00:02:34.585
So you can use just the position of

39
00:02:34.585 --> 00:02:39.131
this line as your description
of the data point.

40
00:02:39.131 --> 00:02:44.265
And it's just another way to say that
these two random variables are so

41
00:02:44.265 --> 00:02:47.408
connected that you don't have to use two.

42
00:02:47.408 --> 00:02:51.410
You, as well, may use just one
to describe both of them, and

43
00:02:51.410 --> 00:02:55.115
this is exactly the idea of
dimensional introduction.

44
00:02:55.115 --> 00:02:59.721
So you have two-dimensional data and
you project it into one dimension,

45
00:02:59.721 --> 00:03:02.777
trying to keep as much
information as possible.

46
00:03:02.777 --> 00:03:06.874
And one of the most popular
way to do it is called

47
00:03:06.874 --> 00:03:11.616
principal component analysis,
or PCA, for short.

48
00:03:11.616 --> 00:03:16.463
And PCA tries to find the best
possible linear transformation,

49
00:03:16.463 --> 00:03:20.602
which projects your
two-dimensional data into 1D.

50
00:03:20.602 --> 00:03:25.004
Or more generally, your multidimensional
data into lower dimensions,

51
00:03:25.004 --> 00:03:27.917
while keeping as much
information as possible.

52
00:03:27.917 --> 00:03:30.077
So PCA is cool.

53
00:03:30.077 --> 00:03:33.479
It gives you an optimal solution
to this kind of problem.

54
00:03:33.479 --> 00:03:35.693
It has analytical solutions, so

55
00:03:35.693 --> 00:03:40.594
you can just write down the formula of
the solution of the PCA problem, and

56
00:03:40.594 --> 00:03:44.087
this analytical formula
is very faster implement.

57
00:03:44.087 --> 00:03:47.598
So if you give me 10,000
dimensional points,

58
00:03:47.598 --> 00:03:52.304
I can return you back the same points
projected to ten dimensions, for

59
00:03:52.304 --> 00:03:55.757
example, while keeping
most of the information.

60
00:03:55.757 --> 00:03:58.983
And I can do it in milliseconds,
so it's really fast.

61
00:03:58.983 --> 00:04:05.217
But sometimes people still are not
happy enough with this PCA,

62
00:04:05.217 --> 00:04:11.223
and try to formulate this PCA
in probabilistic terms, why?

63
00:04:11.223 --> 00:04:16.618
Well, is usually formulating your
usual problem in probabilistic terms

64
00:04:16.618 --> 00:04:22.543
may give you some benefits, like being
able handle missing data, for example.

65
00:04:22.543 --> 00:04:28.528
So in the original paper that proposed
this probabilistic version of PCA,

66
00:04:28.528 --> 00:04:33.943
they try to project some
multidimensional data in two dimensions,

67
00:04:33.943 --> 00:04:38.504
so you can now plot this data
on two-dimensional plane.

68
00:04:38.504 --> 00:04:42.332
And they then try to obscure
some of the data, so

69
00:04:42.332 --> 00:04:45.702
introduce missing values into the data.

70
00:04:45.702 --> 00:04:50.523
They thrown away some
parts of the features, and

71
00:04:50.523 --> 00:04:56.649
then they projected this data
set with missing values again.

72
00:04:56.649 --> 00:05:01.700
And you can see that these two
projections doesn't differ that much,

73
00:05:01.700 --> 00:05:06.407
which means that we don't lose
that much information by throwing

74
00:05:06.407 --> 00:05:10.704
away some parts of the data,
which is really cool, right?

75
00:05:10.704 --> 00:05:14.431
We were able to treat this missing values,
and

76
00:05:14.431 --> 00:05:18.837
the solution doesn't change
when we introduce them.

77
00:05:18.837 --> 00:05:21.479
So we're really robust to missing values.

78
00:05:21.479 --> 00:05:26.456
By the way, the paper where they
proposed this principal component

79
00:05:26.456 --> 00:05:28.385
analysis is really good.

80
00:05:28.385 --> 00:05:30.224
So check it out if you have time and

81
00:05:30.224 --> 00:05:32.995
if you want to know more
details about this model.

82
00:05:32.995 --> 00:05:38.386
So let's try to drive the main
ideas behind this probability

83
00:05:38.386 --> 00:05:43.787
principal component analysis
in the following few slides.

84
00:05:43.787 --> 00:05:44.972
So first of all,

85
00:05:44.972 --> 00:05:50.239
it's natural to call this low
dimensional representation of your data.

86
00:05:50.239 --> 00:05:55.227
So in this example, one-dimensional
position of each data point,

87
00:05:55.227 --> 00:05:59.186
of each orange data point,
to call a latent variable.

88
00:05:59.186 --> 00:06:03.062
Because it's something you don't know,

89
00:06:03.062 --> 00:06:08.884
you don't observe directly, and
it causes your data somehow.

90
00:06:08.884 --> 00:06:13.813
So the position of your orange
data point on the line, this ti,

91
00:06:13.813 --> 00:06:19.964
it influences where the data point will
end up on the two-dimensional plane.

92
00:06:19.964 --> 00:06:25.304
So it influences the position
of the observed point, right?

93
00:06:25.304 --> 00:06:30.970
So it's natural to introduce this latent
variable model where you have ti,

94
00:06:30.970 --> 00:06:32.304
which causes xi.

95
00:06:32.304 --> 00:06:35.342
And you have to define some prior for
ti, and

96
00:06:35.342 --> 00:06:38.228
why not to set it just to standard normal?

97
00:06:38.228 --> 00:06:42.584
This will just mean that your projections,

98
00:06:42.584 --> 00:06:45.997
your low dimension projections,

99
00:06:45.997 --> 00:06:52.125
will be somewhere around 0 and
will have variance around 1.

100
00:06:52.125 --> 00:06:53.227
Which, why not?

101
00:06:53.227 --> 00:06:55.123
It's nice property to have.

102
00:06:55.123 --> 00:06:58.558
Now we have to define the likelihood, so

103
00:06:58.558 --> 00:07:03.722
the probability of x given ti,
and how does x and ti connect?

104
00:07:03.722 --> 00:07:08.384
So how does this one-dimensional data and
two-dimensional data is connected?

105
00:07:08.384 --> 00:07:11.482
Well, if you look at the orange,

106
00:07:11.482 --> 00:07:16.905
kind of orange two-dimensional
x on the projection of x,

107
00:07:16.905 --> 00:07:21.554
then it equals to some
vector times the position of

108
00:07:21.554 --> 00:07:26.557
this one-dimensional line
plus some shift vector.

109
00:07:26.557 --> 00:07:32.108
So we can linearly transform
from this one-dimensional line

110
00:07:32.108 --> 00:07:38.202
to two-dimensional space and
get these orange projected points.

111
00:07:38.202 --> 00:07:43.502
Or more generally,
we can multiply ti by some matrix W, and

112
00:07:43.502 --> 00:07:50.074
then add some bisector, b, and
we'll get our orange projections, xi.

113
00:07:50.074 --> 00:07:54.307
And this W and b will be our parameters,

114
00:07:54.307 --> 00:07:57.780
which we aim to learn from data.

115
00:07:57.780 --> 00:08:00.593
Okay, but this is orange points, right?

116
00:08:00.593 --> 00:08:03.679
How can we recover the blue points,
the original data?

117
00:08:05.596 --> 00:08:08.098
Well, it's kind of,
I don't know how to do it.

118
00:08:08.098 --> 00:08:13.400
I mean, it's impossible to exactly say
where the blue point will be if you

119
00:08:13.400 --> 00:08:19.740
know the orange point, because you don't
know how much information you lost, right?

120
00:08:19.740 --> 00:08:22.149
But you don't have to say it exactly,

121
00:08:22.149 --> 00:08:25.401
you can just model this
on how probabilistically.

122
00:08:25.401 --> 00:08:29.664
So let's say that the blue point,
xi, which we observe,

123
00:08:29.664 --> 00:08:35.152
is just orange point plus some random
noise, which is centered around 0.

124
00:08:35.152 --> 00:08:39.827
And has some covariance matrix sigma,
which we'll also treat as parameter.

125
00:08:39.827 --> 00:08:44.460
This way we're kind of saying
that our blue observed

126
00:08:44.460 --> 00:08:48.671
data points are the same
as the projection of our

127
00:08:48.671 --> 00:08:53.950
one-dimensional data into 2D
plus some Gaussian noise.

128
00:08:53.950 --> 00:08:58.690
Which means that we don't actually
know where the blue points occur, but

129
00:08:58.690 --> 00:09:04.056
we expect them to be somewhere around
the orange points, around the projections.

130
00:09:04.056 --> 00:09:08.459
Okay, so
we have a latent variable model like this,

131
00:09:08.459 --> 00:09:12.974
so ti causes xi, and
we have defined the model fully.

132
00:09:12.974 --> 00:09:17.574
So we have prior of ti is standard normal,
and we have likelihood.

133
00:09:17.574 --> 00:09:20.558
So xi given ji is some
normal distribution also.

134
00:09:20.558 --> 00:09:24.792
Now we want to train this kind of model.

135
00:09:24.792 --> 00:09:28.775
So we want to find the parameters
which are, for example,

136
00:09:28.775 --> 00:09:31.144
maximum likelihood estimation.

137
00:09:31.144 --> 00:09:36.223
Well, first of all, as usually we
will assume that the likelihood is

138
00:09:36.223 --> 00:09:42.194
factorized into product of likelihoods
of individual objects and data points.

139
00:09:42.194 --> 00:09:46.761
And the likelihood is equals to
the product of these likelihoods of data

140
00:09:46.761 --> 00:09:47.376
points.

141
00:09:47.376 --> 00:09:52.171
And then we can rewrite
this marginal likelihood of

142
00:09:52.171 --> 00:09:56.640
individual object,
by marginalizing out ti.

143
00:09:56.640 --> 00:10:03.768
So it's the joint distribution, p of xi
and ti, and then we have to sum out ti.

144
00:10:03.768 --> 00:10:07.624
But previously we had sums,
now we have an integral,

145
00:10:07.624 --> 00:10:11.149
because this latent
variable ti is continuous.

146
00:10:11.149 --> 00:10:14.101
And to sum it out it means
to integrate it out.

147
00:10:14.101 --> 00:10:18.825
Note that in general, this integral is
intractable, and it's really hard to

148
00:10:18.825 --> 00:10:23.633
optimize this function, because we can't
even compute it at any given point.

149
00:10:23.633 --> 00:10:26.490
The integral is intractable.

150
00:10:26.490 --> 00:10:31.714
So it's really cool that Algorithm allows
you to optimize these kinds of functions.

151
00:10:31.714 --> 00:10:36.782
Although, you sometimes you can't
even compute them at any given point,

152
00:10:36.782 --> 00:10:40.732
but in this particular situation,
you don't need that.

153
00:10:40.732 --> 00:10:44.000
So everything is normal.

154
00:10:44.000 --> 00:10:45.829
Everything is conjugate here,

155
00:10:45.829 --> 00:10:50.312
which means that you can analytically
integrate this latent variable gi out.

156
00:10:50.312 --> 00:10:53.727
So you can now do this integral,
and it will also be a normal

157
00:10:53.727 --> 00:10:58.075
distribution with some parameters,
which you can look up in Wikipedia.

158
00:10:58.075 --> 00:11:00.943
And then you have a product of Gaussians,
and

159
00:11:00.943 --> 00:11:04.499
you can analytically compute
the optimal parameters.

160
00:11:04.499 --> 00:11:08.373
So you can take
the logarithm of this thing.

161
00:11:08.373 --> 00:11:12.077
You can compute the gradient, and then
you can set this gradient equal to 0 and

162
00:11:12.077 --> 00:11:14.788
find the maximum likelihood
parameters analytically.

163
00:11:16.274 --> 00:11:21.297
And somewhat unexpectedly,
we will find out that

164
00:11:21.297 --> 00:11:26.083
the optional parameters
of this probabilistic

165
00:11:26.083 --> 00:11:31.118
model is exactly the same
as the formulas for PCA.

166
00:11:31.118 --> 00:11:32.505
So look what happened.

167
00:11:32.505 --> 00:11:37.010
We started with PCA,
we interpreted it probabilistically.

168
00:11:37.010 --> 00:11:42.413
We found the maximum likelihood
parameters for this probability model,

169
00:11:42.413 --> 00:11:47.300
and they turned out to be the same
as the original formulas for PCA.

170
00:11:47.300 --> 00:11:50.888
It's kind of unsettling,
because we spent this whole,

171
00:11:50.888 --> 00:11:55.535
I don t know, ten minutes, and
we didn't get anything useful from that.

172
00:11:55.535 --> 00:11:58.129
We get the same formulas as PCA, but

173
00:11:58.129 --> 00:12:03.334
it turns out that this probabilistic
interpretation is still useful.

174
00:12:03.334 --> 00:12:06.510
So here we don't need Algorithm at all,

175
00:12:06.510 --> 00:12:10.232
because we have everything analytical and
nice.

176
00:12:10.232 --> 00:12:12.793
But if we change the model a little bit,

177
00:12:12.793 --> 00:12:17.224
then we will not be able to compute
anything analytically anymore.

178
00:12:17.224 --> 00:12:21.011
But with We'll still be able to train it.

179
00:12:21.011 --> 00:12:24.354
So let's say you introduce missing values.

180
00:12:24.354 --> 00:12:29.564
You do not observe some part of your xis,
then you have more latent

181
00:12:29.564 --> 00:12:35.154
variables than you used to have,
and then you can't find the latent,

182
00:12:35.154 --> 00:12:41.263
you can't find the maximum likelihood
parameters analytically anymore.

183
00:12:41.263 --> 00:12:47.316
But you can still apply And
this will give you some valid solution.

184
00:12:47.316 --> 00:12:48.848
So in the next video,

185
00:12:48.848 --> 00:12:53.627
we will talk a little bit about how
to apply Algorithm in this case

186
00:12:55.749 --> 00:13:05.749
[MUSIC]