WEBVTT

1
00:00:00.000 --> 00:00:04.688
[MUSIC]

2
00:00:04.688 --> 00:00:07.149
Hi, welcome to our course.

3
00:00:07.149 --> 00:00:08.504
In this first video,

4
00:00:08.504 --> 00:00:12.961
we will see basic principles that
we'll use throughout this course.

5
00:00:12.961 --> 00:00:14.864
Let's learn them by example.

6
00:00:14.864 --> 00:00:18.291
Imagine you are running through a park and
you see another man running.

7
00:00:18.291 --> 00:00:20.661
And you ask yourself, why is he running?

8
00:00:20.661 --> 00:00:23.292
And you come up with four
different explanations.

9
00:00:23.292 --> 00:00:25.073
First, he is in a hurry.

10
00:00:25.073 --> 00:00:27.057
Second, he is doing some sports.

11
00:00:27.057 --> 00:00:28.867
Third, he always runs.

12
00:00:28.867 --> 00:00:31.945
And fourth, he saw a dragon.

13
00:00:31.945 --> 00:00:34.699
Principle 1, use prior knowledge.

14
00:00:34.699 --> 00:00:37.962
From our previous experience we
know that dragons do no exist.

15
00:00:37.962 --> 00:00:42.555
And so, we can exclude fourth
option from next consideration.

16
00:00:42.555 --> 00:00:47.438
Principle 2, choose answer that
explains observations the most.

17
00:00:47.438 --> 00:00:50.789
Imagine you saw that he is
not wearing a sports suit.

18
00:00:50.789 --> 00:00:54.746
In this case, it´s very unlikely
that he´s doing sports, and so

19
00:00:54.746 --> 00:00:56.381
we can exclude number two.

20
00:00:56.381 --> 00:00:59.950
Principle 3,
avoid making extra assumptions.

21
00:00:59.950 --> 00:01:03.968
From the last two options,
the third option, does he always runs,

22
00:01:03.968 --> 00:01:07.434
makes a lot of extra assumptions and
so should exclude it.

23
00:01:07.434 --> 00:01:10.074
This principle is also
known as Occam's Razor.

24
00:01:10.074 --> 00:01:15.344
And finally, we are left with only
one case, that he is in a hurry.

25
00:01:15.344 --> 00:01:18.047
To conclude, we've seen three principles.

26
00:01:18.047 --> 00:01:22.568
To use prior knowledge, to choose answer
that explains observations the most, and

27
00:01:22.568 --> 00:01:24.996
finally to avoid making extra assumptions.

28
00:01:27.129 --> 00:01:32.336
Before we continue, let's review some
basic principles from probability theory.

29
00:01:32.336 --> 00:01:34.689
We define probability
in the following way.

30
00:01:34.689 --> 00:01:37.871
Imagine you have some source of
randomness, for example, a dice.

31
00:01:37.871 --> 00:01:40.541
And you repeat an experiment
multiple times.

32
00:01:40.541 --> 00:01:43.813
And as the number of
experiments goes to infinity,

33
00:01:43.813 --> 00:01:48.318
we get the probability as a fraction
of the times some event occurred.

34
00:01:48.318 --> 00:01:52.996
For example, you would expect for
a fair dice that the event that you

35
00:01:52.996 --> 00:01:56.707
threw five would have
a frequency about one-sixth.

36
00:01:56.707 --> 00:01:59.299
And for
events that you threw an odd number,

37
00:01:59.299 --> 00:02:01.761
it would be somewhere around one-half.

38
00:02:01.761 --> 00:02:06.031
We will consider two different types of
random variables depending on which values

39
00:02:06.031 --> 00:02:08.231
they can take, discrete and continuous.

40
00:02:08.231 --> 00:02:12.854
The discrete for random variables can have
either finite number of values that can

41
00:02:12.854 --> 00:02:14.874
take, as for example, for a dice.

42
00:02:14.874 --> 00:02:20.410
Or infinite, if you count the number of
times that some certain event happened.

43
00:02:20.410 --> 00:02:24.219
An example of continuous random variable
would be at tomorrow's temperature.

44
00:02:24.219 --> 00:02:27.639
The most convenient way to find
the discrete distribution is to call

45
00:02:27.639 --> 00:02:29.322
the probability mass function.

46
00:02:29.322 --> 00:02:35.179
It maps a number for each point
that refers to the probability.

47
00:02:35.179 --> 00:02:37.195
For example, in this case,

48
00:02:37.195 --> 00:02:41.569
we'll get a point that equals
to 1 which produces in 0.2.

49
00:02:41.569 --> 00:02:46.338
The 0.3 with probability 0.5 and
so on with probability 0.3 and

50
00:02:46.338 --> 00:02:48.696
other points with probability 0.

51
00:02:48.696 --> 00:02:51.740
Also note that these points sum up to 1.

52
00:02:51.740 --> 00:02:55.262
The most convenient way to define
continuous distributions is called

53
00:02:55.262 --> 00:02:57.001
a probability density function.

54
00:02:57.001 --> 00:03:00.592
It assigns a non-negative value for
each point.

55
00:03:00.592 --> 00:03:05.394
And then to compute the probability that
a point will fall into some range, for

56
00:03:05.394 --> 00:03:10.436
example, from a to b, you should integrate
this function over this given range.

57
00:03:10.436 --> 00:03:12.715
As is given on the slide.

58
00:03:12.715 --> 00:03:15.557
We will also need
a notion of independence.

59
00:03:15.557 --> 00:03:20.381
The two run variables are considered
independent if their joint probability,

60
00:03:20.381 --> 00:03:25.068
that is, a probability of X and Y,
equals to the product of their marginals.

61
00:03:25.068 --> 00:03:28.503
So it will be a probability of
X times a probability of Y.

62
00:03:28.503 --> 00:03:29.700
Let's see an example.

63
00:03:29.700 --> 00:03:34.836
Imagine that you have a deck of 52 cards
and you take, randomly, 2 cards from it.

64
00:03:34.836 --> 00:03:38.411
And the first random variable would
be the picture that is drawn on

65
00:03:38.411 --> 00:03:42.711
the first card and second would be the
picture that is drawn on the second card.

66
00:03:42.711 --> 00:03:47.520
Those kind of variables are dependent
since it is impossible to

67
00:03:47.520 --> 00:03:49.433
take one card two times.

68
00:03:49.433 --> 00:03:53.506
Another example is throwing
two coins independently.

69
00:03:53.506 --> 00:03:57.734
Here the probability that the first
coin will land heads up and

70
00:03:57.734 --> 00:04:03.112
the second would land tails up equals to
the product of the two probabilities.

71
00:04:03.112 --> 00:04:07.175
And so
these random variables are independent.

72
00:04:09.620 --> 00:04:13.308
The last thing we'll need is
a conditional probability.

73
00:04:13.308 --> 00:04:15.247
We want to answer a question,

74
00:04:15.247 --> 00:04:20.221
what is the probability of X given that
something that is called Y happened.

75
00:04:20.221 --> 00:04:23.572
It is given by the formula
that you can see on the slide.

76
00:04:23.572 --> 00:04:29.069
It is the probability of X given Y equals
to the joint probability P of X and

77
00:04:29.069 --> 00:04:32.057
Y over the marginal probability P of Y.

78
00:04:32.057 --> 00:04:33.774
Let's consider an example.

79
00:04:33.774 --> 00:04:37.143
Imagine you are a student and
you want to pass some course.

80
00:04:37.143 --> 00:04:39.968
It has two exams in it,
a midterm and the final.

81
00:04:39.968 --> 00:04:43.732
The probability that the student
will pass a midterm is 0.4 and

82
00:04:43.732 --> 00:04:48.191
the probability that the student will
pass a midterm and the final 0.25.

83
00:04:48.191 --> 00:04:52.221
If you want to find the probability that
you will pass the final, given that you

84
00:04:52.221 --> 00:04:56.393
already passed the midterm, you can apply
the formula from the previous slide.

85
00:04:56.393 --> 00:04:59.984
And this will give you a value around 60%.

86
00:04:59.984 --> 00:05:03.499
We'll need two tricks
to deal with formulas.

87
00:05:03.499 --> 00:05:06.217
The first is called the chain rule.

88
00:05:06.217 --> 00:05:11.116
We can derive it from the definition
of the conditional probability.

89
00:05:11.116 --> 00:05:14.241
That is, the joint probability of X and

90
00:05:14.241 --> 00:05:18.987
Y equals to the product of X given Y and
the probability of Y.

91
00:05:18.987 --> 00:05:22.918
By induction, we can prove the same
formula for three variables.

92
00:05:22.918 --> 00:05:27.903
It will be the probability of X, Y, and
Z equals to probability of X given Y and

93
00:05:27.903 --> 00:05:32.072
Z, the probability of Y given Z,
and finally probability of Z.

94
00:05:32.072 --> 00:05:36.115
And in a similar way,
we can obtain the formula for

95
00:05:36.115 --> 00:05:38.820
the arbitrary number of points.

96
00:05:38.820 --> 00:05:41.958
So this would be the probability
of the current point,

97
00:05:41.958 --> 00:05:43.809
given all its previous points.

98
00:05:43.809 --> 00:05:46.082
The last rule is called the sum rule.

99
00:05:46.082 --> 00:05:51.062
That is, if you want to find out
the marginal distribution p(X), and

100
00:05:51.062 --> 00:05:54.797
you know only the joint
probability that p(X,Y),

101
00:05:54.797 --> 00:06:00.028
you can integrate out the random variable
Y, as it is given on the formula.

102
00:06:00.028 --> 00:06:04.467
And finally, the most important formula
for this course, the Bayes theorem.

103
00:06:04.467 --> 00:06:07.214
We want to find out
the probability of theta given X,

104
00:06:07.214 --> 00:06:09.594
where theta are the parameters
of our model.

105
00:06:09.594 --> 00:06:12.952
For example, we have a neural network and
those are its parameters.

106
00:06:12.952 --> 00:06:14.122
And then we have X.

107
00:06:14.122 --> 00:06:18.109
Those are the observations, for example,
the images that you are dealing with.

108
00:06:18.109 --> 00:06:22.423
From the definition of the conditional
probability, we can say that it is a ratio

109
00:06:22.423 --> 00:06:26.306
between the joint probability and
the marginal probability, P(X).

110
00:06:26.306 --> 00:06:29.073
And also we apply the chain rule,
we'll get the following formula.

111
00:06:29.073 --> 00:06:32.668
It will be the probability
of X given theta,

112
00:06:32.668 --> 00:06:37.148
times the probability of
theta over probability of X.

113
00:06:37.148 --> 00:06:42.888
This formula is so important that each
of its components has its own name.

114
00:06:42.888 --> 00:06:45.298
The probability of theta
is called a prior,

115
00:06:45.298 --> 00:06:48.757
it shows us what prior knowledge
we know about the parameters.

116
00:06:48.757 --> 00:06:53.214
For example, you can know that some
parameters are distributed at around 0.

117
00:06:53.214 --> 00:06:57.423
The term probability of X given
theta is called a likelihood, and

118
00:06:57.423 --> 00:07:00.725
it shows how well the parameters
explain our data.

119
00:07:00.725 --> 00:07:05.605
The thing that we get, the probability of
theta given X, is called a posterior, and

120
00:07:05.605 --> 00:07:09.477
it is the probability of the parameters
after we observe the data.

121
00:07:09.477 --> 00:07:14.596
And finally the term in
the denominator is called evidence

122
00:07:14.596 --> 00:07:24.596
[MUSIC]