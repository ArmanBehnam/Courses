WEBVTT

1
00:00:05.600 --> 00:00:08.600
We've hinted how the assumptions
underlying exploring starts

2
00:00:08.600 --> 00:00:13.000
might limit the applicability of
Monte Carlo control in this video.

3
00:00:13.000 --> 00:00:15.900
We describe how to combine
Epsilon greedy expiration

4
00:00:15.900 --> 00:00:18.600
with Monte Carlo to learn
near optimal policies.

5
00:00:19.700 --> 00:00:23.800
By the end of this video you will
understand why exploring starts can be

6
00:00:23.800 --> 00:00:28.100
problematic in real problems and
you will be able to describe

7
00:00:28.100 --> 00:00:30.600
an alternative expiration method for
Monte Carlo control.

8
00:00:32.000 --> 00:00:35.700
Let's think about some situations
where we cannot use exploring starts

9
00:00:35.700 --> 00:00:40.200
this algorithm must be able to start
from every possible State action pair.

10
00:00:40.200 --> 00:00:42.500
Otherwise the age of may
not explore enough and

11
00:00:42.500 --> 00:00:46.100
could converge to a suboptimal
solution in many problems.

12
00:00:46.100 --> 00:00:49.500
It can be difficult to randomly
sample an initial State action pair.

13
00:00:50.700 --> 00:00:54.900
For example, how would you randomly
sample the initial State action pair for

14
00:00:54.900 --> 00:00:56.100
a self-driving car?

15
00:00:57.200 --> 00:01:01.200
How could we ensure the agent can
start in all possible States?

16
00:01:01.200 --> 00:01:04.000
We would need to put the car in
many different configurations

17
00:01:04.000 --> 00:01:06.100
in the middle of a busy freeway.

18
00:01:06.100 --> 00:01:08.100
This would be dangerous and impractical.

19
00:01:09.500 --> 00:01:14.500
So, how can we learn all the action
values without exploring starts?

20
00:01:14.500 --> 00:01:16.700
Remember Epsilon greedy expiration.

21
00:01:16.700 --> 00:01:18.000
We discussed this simple but

22
00:01:18.000 --> 00:01:23.200
effective method in the Bandit lectures
we can use it with Monte Carlo to

23
00:01:23.200 --> 00:01:27.400
as a quick recap Epsilon greedy
policies are stochastic policies.

24
00:01:27.400 --> 00:01:31.300
They usually take the greedy action,
but occasionally take a random action.

25
00:01:33.000 --> 00:01:37.400
Epsilon greedy policies are a subset of
a larger class of policies called Epsilon

26
00:01:37.400 --> 00:01:41.500
soft policies Epsilon soft
policies take each action

27
00:01:41.500 --> 00:01:45.500
with probability at least Epsilon
over the number of actions.

28
00:01:45.500 --> 00:01:50.800
For example, both policies shown on
the slide are valid Epsilon soft policies.

29
00:01:50.800 --> 00:01:54.700
The uniform random policy is another
notable Epsilon South policy.

30
00:01:55.900 --> 00:02:00.700
Epsilon soft policies Force the agent to
continually explore that means we can drop

31
00:02:00.700 --> 00:02:05.200
the exploring starts requirement from the
Monte Carlo control algorithm an Epsilon

32
00:02:05.200 --> 00:02:10.500
soft policy assigns nonzero probability
to each action in every state because

33
00:02:10.500 --> 00:02:15.300
of this Epsilon soft agents continue to
visit all state action pairs indefinitely.

34
00:02:17.400 --> 00:02:22.400
Epsilon soft policies are always
stochastic deterministic policy specify

35
00:02:22.400 --> 00:02:27.200
a single action to take in each state
stochastic policies instead specify

36
00:02:27.200 --> 00:02:31.000
the probability of taking action
in each state in epsilon.

37
00:02:31.000 --> 00:02:32.000
Soft policies.

38
00:02:32.000 --> 00:02:36.700
All actions have a probability of at
least Epsilon over the number of actions.

39
00:02:36.700 --> 00:02:38.700
They will eventually try all the actions.

40
00:02:40.000 --> 00:02:43.900
Let's look at an example of an Epsilon
greedy policy and a deterministic policy.

41
00:02:45.700 --> 00:02:49.400
Here we have a grid rolled with the arrows
representing the deterministic policy.

42
00:02:50.800 --> 00:02:53.500
From the start State the agent
will follow the exact same

43
00:02:53.500 --> 00:02:55.000
trajectory through the grip world.

44
00:02:57.400 --> 00:03:01.400
The Epsilon greedy policy has more arrows
because every action has some small

45
00:03:01.400 --> 00:03:04.700
probability of being selected accordingly.

46
00:03:04.700 --> 00:03:08.900
The agent will probably follow a slightly
different trajectory every episode.

47
00:03:10.400 --> 00:03:11.500
After enough episodes,

48
00:03:11.500 --> 00:03:17.300
it will have taken every action at least
once in every state this difference

49
00:03:17.300 --> 00:03:22.000
extends the solutions that we find by
exploring with Epsilon soft policies.

50
00:03:23.300 --> 00:03:27.400
If our policy always gives at least
Epsilon probability to each action,

51
00:03:27.400 --> 00:03:31.700
it's impossible to converge to
a deterministic optimal policy

52
00:03:31.700 --> 00:03:35.200
exploring starts can be used
to find the optimal policy.

53
00:03:35.200 --> 00:03:39.800
But that's on soft policies can only be
used to find the optimal Epsilon soft

54
00:03:39.800 --> 00:03:40.800
policy.

55
00:03:40.800 --> 00:03:43.800
That is the policy with
the highest value in each state

56
00:03:43.800 --> 00:03:45.800
out of all the Epsilon soft policies.

57
00:03:46.900 --> 00:03:50.100
This policy performs worse than
the optimal policy in general.

58
00:03:50.100 --> 00:03:52.500
However, it often performs reasonably.

59
00:03:52.500 --> 00:03:55.300
Well and
allows us to get rid of exploring starts.

60
00:03:56.800 --> 00:04:01.700
Let's look at the pseudo code for Monte
Carlo control with Epsilon soft policies.

61
00:04:01.700 --> 00:04:05.500
It's easy to understand this algorithm by
highlighting how it differs from exploring

62
00:04:05.500 --> 00:04:06.100
starts.

63
00:04:07.800 --> 00:04:11.800
There's one change to the initial
conditions one change to the policy

64
00:04:11.800 --> 00:04:16.000
evaluation step and
one change the policy Improvement step.

65
00:04:17.100 --> 00:04:22.000
The initial policy must be Epsilon soft
such as the uniform random policy.

66
00:04:22.000 --> 00:04:25.700
The next change is how we use
the policy to generate training data

67
00:04:25.700 --> 00:04:28.000
since the agents policy is Epsilon soft.

68
00:04:28.000 --> 00:04:31.200
We don't have to rely on
exploring starts for exploration

69
00:04:31.200 --> 00:04:35.700
instead the agent simply generates an
episode following its Epsilon soft policy.

70
00:04:36.900 --> 00:04:37.700
Finally.

71
00:04:37.700 --> 00:04:39.200
We improve the policy.

72
00:04:39.200 --> 00:04:42.000
The new policy is Epsilon greedy
with respect to the current

73
00:04:42.000 --> 00:04:44.100
action value estimates.

74
00:04:44.100 --> 00:04:46.800
You might think that this should
be an Epsilon soft policy

75
00:04:46.800 --> 00:04:49.100
rather than Epsilon greedy.

76
00:04:49.100 --> 00:04:52.900
However, this makes sense because
the optimal Epsilon soft policy

77
00:04:52.900 --> 00:04:54.700
is an Epsilon greedy policy.

78
00:04:56.100 --> 00:04:59.100
Even though this algorithm may
not find the optimal policy.

79
00:04:59.100 --> 00:05:03.400
It does find the optimal Epsilon
softball see in future videos.

80
00:05:03.400 --> 00:05:06.400
We will discuss how to
learn the optimal policy

81
00:05:06.400 --> 00:05:10.800
using a different method called
Q learning in this video.

82
00:05:10.800 --> 00:05:14.700
We talked about the practicality
of exploring starts and

83
00:05:14.700 --> 00:05:18.600
discuss Monte Carlo control
with Epsilon soft policies.

84
00:05:18.600 --> 00:05:21.700
That's it for this time go forth and
explore safely.