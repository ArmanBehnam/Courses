WEBVTT

1
00:00:00.200 --> 00:00:02.100
Guess what Adam?

2
00:00:02.100 --> 00:00:02.910
What Martha?

3
00:00:02.910 --> 00:00:04.650
We have finally made it.

4
00:00:04.650 --> 00:00:05.970
We get to learn about

5
00:00:05.970 --> 00:00:07.740
sample-based ARL methods and

6
00:00:07.740 --> 00:00:09.300
you know what that means, right?

7
00:00:09.300 --> 00:00:10.335
Maybe.

8
00:00:10.335 --> 00:00:12.180
We can learn solely
from trial and

9
00:00:12.180 --> 00:00:16.035
error interaction,
only from experience.

10
00:00:16.035 --> 00:00:19.830
All ages can learn how
the world works on their own.

11
00:00:19.830 --> 00:00:22.500
This means we don't need
a model of the world.

12
00:00:22.500 --> 00:00:24.285
All we need is data.

13
00:00:24.285 --> 00:00:25.830
But we can still use the model,

14
00:00:25.830 --> 00:00:28.380
if we have it, by simulating
experience from it.

15
00:00:28.380 --> 00:00:30.360
In fact, you'll see how many

16
00:00:30.360 --> 00:00:32.550
sample-based learning
algorithms are based on

17
00:00:32.550 --> 00:00:35.520
value functions and ideas
from dynamic programming.

18
00:00:35.520 --> 00:00:37.920
This includes one of the most
fundamental algorithms in

19
00:00:37.920 --> 00:00:40.905
reinforce learning called
temporal difference learning.

20
00:00:40.905 --> 00:00:43.390
We'll mirror the
structure of Course 1.

21
00:00:43.390 --> 00:00:44.840
We first start with methods to

22
00:00:44.840 --> 00:00:46.715
learn value functions
for prediction.

23
00:00:46.715 --> 00:00:49.880
Then you'll see how to learn
action values for control.

24
00:00:49.880 --> 00:00:51.500
Finally, we'll talk
about how to do

25
00:00:51.500 --> 00:00:53.915
planning with the learned model.

26
00:00:53.915 --> 00:00:56.130
We'll kick things
off with Monte Carlo

27
00:00:56.130 --> 00:00:57.190
methods, and after that,

28
00:00:57.190 --> 00:00:59.120
we will discuss how
TD methods can also

29
00:00:59.120 --> 00:01:01.355
be used for prediction
and control.

30
00:01:01.355 --> 00:01:03.230
We will see how
TD control methods

31
00:01:03.230 --> 00:01:04.760
like SARSA and Q-Learning

32
00:01:04.760 --> 00:01:06.950
are connected to
generalized policy iteration

33
00:01:06.950 --> 00:01:08.320
and Bellman operators.

34
00:01:08.320 --> 00:01:11.120
Along the way, we'll talk a
lot about expiration which

35
00:01:11.120 --> 00:01:12.290
is critical when learning from

36
00:01:12.290 --> 00:01:13.790
trial and error interaction.

37
00:01:13.790 --> 00:01:16.520
You'll also be introduced
to off policy learning,

38
00:01:16.520 --> 00:01:18.965
which is essential when
learning from experience.

39
00:01:18.965 --> 00:01:20.680
We'll finish
this course with Dyna,

40
00:01:20.680 --> 00:01:22.280
an integrated architecture for

41
00:01:22.280 --> 00:01:24.425
planning, learning and acting.

42
00:01:24.425 --> 00:01:26.900
We'll talk about some of
the new issues that arise

43
00:01:26.900 --> 00:01:29.960
when planning with learn models
that could be inaccurate.

44
00:01:29.960 --> 00:01:32.195
Course to explore is
the foundations of

45
00:01:32.195 --> 00:01:33.290
online learning algorithms

46
00:01:33.290 --> 00:01:34.970
used throughout
reinforced learning.

47
00:01:34.970 --> 00:01:37.940
We will continue in the tabular
setting for now where we

48
00:01:37.940 --> 00:01:40.925
can learn a table of values
assuming a finite MDP.

49
00:01:40.925 --> 00:01:42.800
In Course 3, we'll extend

50
00:01:42.800 --> 00:01:43.910
our online algorithms to

51
00:01:43.910 --> 00:01:45.710
the function
approximation setting.

52
00:01:45.710 --> 00:01:48.440
But for now, there are
many issues to discuss and

53
00:01:48.440 --> 00:01:51.470
problems to solve even in
this simplified setting.

54
00:01:51.470 --> 00:01:53.360
Let's continue
our journey through

55
00:01:53.360 --> 00:01:55.655
the theory and practice
of reinforced learning.

56
00:01:55.655 --> 00:01:58.295
So grab a drink and something
to take notes with,

57
00:01:58.295 --> 00:02:01.590
look at our artificial
intelligence. Here, we come.