WEBVTT

1
00:00:05.180 --> 00:00:08.490
Now that we have a generalized
policy iteration algorithm

2
00:00:08.490 --> 00:00:10.065
for Monte Carlo control,

3
00:00:10.065 --> 00:00:13.380
let's use it in an example
and see how it works.

4
00:00:13.380 --> 00:00:15.090
By the end of this video,

5
00:00:15.090 --> 00:00:17.760
you'll be able to
apply Monte Carlo with

6
00:00:17.760 --> 00:00:20.835
Exploring Starts to
solve an example MDP.

7
00:00:20.835 --> 00:00:22.830
Let's learn a policy.

8
00:00:22.830 --> 00:00:25.350
We will train our agent
to play blackjack

9
00:00:25.350 --> 00:00:27.720
using Monte Carlo with
Exploring Starts.

10
00:00:27.720 --> 00:00:29.820
Exploring starts requires that

11
00:00:29.820 --> 00:00:32.760
episodes begin with
a random state and action.

12
00:00:32.760 --> 00:00:34.790
Luckily for us, our version of

13
00:00:34.790 --> 00:00:37.535
blackjack naturally
starts in random states.

14
00:00:37.535 --> 00:00:40.010
Then, all we have to do is to

15
00:00:40.010 --> 00:00:42.890
randomly select the first action
in each episode.

16
00:00:42.890 --> 00:00:44.750
That is the agent ignores what

17
00:00:44.750 --> 00:00:46.280
it thinks is the best action in

18
00:00:46.280 --> 00:00:50.015
the first state and randomly
chooses to hit or stick.

19
00:00:50.015 --> 00:00:52.040
The initial policy is hit,

20
00:00:52.040 --> 00:00:54.200
when the agent sum
is less than 20,

21
00:00:54.200 --> 00:00:57.755
and to stick when
the sum is 20 or 21.

22
00:00:57.755 --> 00:00:59.840
Let's step through a game

23
00:00:59.840 --> 00:01:01.790
and see what
the algorithm would do.

24
00:01:01.790 --> 00:01:05.360
Suppose the agents cards
show a total of 13

25
00:01:05.360 --> 00:01:07.370
with no usable ace and

26
00:01:07.370 --> 00:01:09.605
the dealers visible card
is an eight.

27
00:01:09.605 --> 00:01:11.840
According to the randomly sampled

28
00:01:11.840 --> 00:01:14.315
first action, the agent hits.

29
00:01:14.315 --> 00:01:17.810
The agent gets a seven
moving the sum to 20.

30
00:01:17.810 --> 00:01:19.940
On the next step,
the agent chooses

31
00:01:19.940 --> 00:01:21.770
the action according
to its policy.

32
00:01:21.770 --> 00:01:25.910
So it sticks. Now,
it's the dealers turn.

33
00:01:25.910 --> 00:01:28.280
The dealer draws a
nine and goes over

34
00:01:28.280 --> 00:01:30.290
21 losing the game and

35
00:01:30.290 --> 00:01:32.405
resulting in a plus
one reward for the agent.

36
00:01:32.405 --> 00:01:34.970
Great. Now, let's look

37
00:01:34.970 --> 00:01:37.369
at the state action pairs
we saw during the game,

38
00:01:37.369 --> 00:01:38.480
starting from the end of

39
00:01:38.480 --> 00:01:41.155
the episode and
working backwards.

40
00:01:41.155 --> 00:01:43.380
In the last non-terminal state,

41
00:01:43.380 --> 00:01:45.160
the agent had a sum of 20,

42
00:01:45.160 --> 00:01:48.475
no usable ace, and
the dealer had an eight.

43
00:01:48.475 --> 00:01:51.710
From that state,
the agent chose to stick.

44
00:01:51.710 --> 00:01:54.440
The agent adds plus
one to its list of

45
00:01:54.440 --> 00:01:57.215
returns corresponding to
that state action pair.

46
00:01:57.215 --> 00:01:58.700
The estimated value for

47
00:01:58.700 --> 00:02:02.000
the action stick in
this state is simply one.

48
00:02:02.000 --> 00:02:05.270
Let's look at the value of
the two actions in this state.

49
00:02:05.270 --> 00:02:07.525
The agent never
tried to hit action,

50
00:02:07.525 --> 00:02:09.280
so its value is zero.

51
00:02:09.280 --> 00:02:11.675
The value of
the stick action is one.

52
00:02:11.675 --> 00:02:13.220
We just computed that.

53
00:02:13.220 --> 00:02:16.055
So the greedy action
with respect to Q,

54
00:02:16.055 --> 00:02:18.290
in this state, is to stick.

55
00:02:18.290 --> 00:02:21.065
It has the highest
estimated value.

56
00:02:21.065 --> 00:02:24.320
Let's move back in time one
more step to the start state.

57
00:02:24.320 --> 00:02:27.110
The agent had 13
with no usable ace,

58
00:02:27.110 --> 00:02:28.595
and the dealer had an eight.

59
00:02:28.595 --> 00:02:31.285
The randomly chosen
action was to hit.

60
00:02:31.285 --> 00:02:33.620
Again, the agent adds plus one to

61
00:02:33.620 --> 00:02:36.715
the list of returns following
that state action pair.

62
00:02:36.715 --> 00:02:38.660
The average of the list forms

63
00:02:38.660 --> 00:02:40.730
the estimate of the action value.

64
00:02:40.730 --> 00:02:44.090
Finally, the policy is
updated in this state to

65
00:02:44.090 --> 00:02:47.300
be greedy with respect to
the action value estimates.

66
00:02:47.300 --> 00:02:49.670
In this state, we had
never tried to stick

67
00:02:49.670 --> 00:02:52.145
action and its value is zero.

68
00:02:52.145 --> 00:02:55.655
But the hit action resulted
in a return of one.

69
00:02:55.655 --> 00:02:57.770
So the greedy action is to hit,

70
00:02:57.770 --> 00:03:00.725
and the policy is
updated to reflect this.

71
00:03:00.725 --> 00:03:04.805
We then repeat this process
over many episodes.

72
00:03:04.805 --> 00:03:06.950
The action values and the policy

73
00:03:06.950 --> 00:03:09.620
will approach
their optimal values.

74
00:03:09.620 --> 00:03:12.380
Let's look at the optimal
policy that the agent

75
00:03:12.380 --> 00:03:15.650
found after we ran it
for a really long time.

76
00:03:15.650 --> 00:03:19.385
Notice how the agent plays
when it has the usable ace.

77
00:03:19.385 --> 00:03:20.840
For most dealer cards,

78
00:03:20.840 --> 00:03:24.785
the agent hits until it
has the sum near 19.

79
00:03:24.785 --> 00:03:26.645
With a usable ace,

80
00:03:26.645 --> 00:03:28.520
the agent has
a lot more flexibility in

81
00:03:28.520 --> 00:03:30.200
calculating the sum of its cards,

82
00:03:30.200 --> 00:03:33.385
so the policy is much
more aggressive.

83
00:03:33.385 --> 00:03:35.350
Without a usable ace,

84
00:03:35.350 --> 00:03:36.680
the policy depends a lot

85
00:03:36.680 --> 00:03:38.885
more on the cards
the dealer is showing.

86
00:03:38.885 --> 00:03:41.600
The agent sticks when
its sum is 13 or

87
00:03:41.600 --> 00:03:43.880
greater and the dealer
has a low card,

88
00:03:43.880 --> 00:03:45.740
like a two or three.

89
00:03:45.740 --> 00:03:48.320
This is optimal even
though it seems like

90
00:03:48.320 --> 00:03:51.365
the chance of winning will
be low in this situation.

91
00:03:51.365 --> 00:03:53.180
That's all for this time.

92
00:03:53.180 --> 00:03:55.894
In this video, we
showed you how to apply

93
00:03:55.894 --> 00:03:59.880
Monte Carlo with Exploring
Starts to an example MDP.