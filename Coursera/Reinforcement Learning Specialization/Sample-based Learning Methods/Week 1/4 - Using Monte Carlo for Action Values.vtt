WEBVTT

1
00:00:06.050 --> 00:00:09.390
So far, we've talked about
using Monte Carlo to learn

2
00:00:09.390 --> 00:00:12.375
state-value functions
for a fixed policy.

3
00:00:12.375 --> 00:00:15.645
This time, we'll focus on
learning action values.

4
00:00:15.645 --> 00:00:17.685
By the end of this video,

5
00:00:17.685 --> 00:00:19.695
you'll be able to estimate

6
00:00:19.695 --> 00:00:22.085
action-value functions
using Monte Carlo

7
00:00:22.085 --> 00:00:24.410
and understand the importance of

8
00:00:24.410 --> 00:00:28.025
maintaining exploration in
Monte Carlo algorithms.

9
00:00:28.025 --> 00:00:29.960
Learning action values is

10
00:00:29.960 --> 00:00:33.485
almost exactly the same process
as learning state values.

11
00:00:33.485 --> 00:00:36.140
Recall that we learned
the value of a state by

12
00:00:36.140 --> 00:00:39.065
averaging sample returns
from that state.

13
00:00:39.065 --> 00:00:42.905
Conveniently, the same process
works for action values.

14
00:00:42.905 --> 00:00:45.170
We collect returns
following a policy from

15
00:00:45.170 --> 00:00:48.950
state-action pair and
then take their average.

16
00:00:48.950 --> 00:00:52.505
But why do we care about
learning action values at all?

17
00:00:52.505 --> 00:00:55.550
Action values are useful
for learning a policy.

18
00:00:55.550 --> 00:00:56.930
They allow us to compare

19
00:00:56.930 --> 00:00:59.390
different actions
in the same state.

20
00:00:59.390 --> 00:01:01.040
Then, we can switch to

21
00:01:01.040 --> 00:01:03.890
a better action if
one is available.

22
00:01:03.890 --> 00:01:06.710
This is only possible
if we have an estimate

23
00:01:06.710 --> 00:01:09.115
of the values of
the other actions,

24
00:01:09.115 --> 00:01:11.190
but this can be tricky.

25
00:01:11.190 --> 00:01:14.405
Let's consider learning
the action-value function

26
00:01:14.405 --> 00:01:16.205
for a deterministic policy.

27
00:01:16.205 --> 00:01:19.565
Imagine an action that is
never selected by the policy.

28
00:01:19.565 --> 00:01:21.020
The agent will never observe

29
00:01:21.020 --> 00:01:23.240
returns corresponding
to that action.

30
00:01:23.240 --> 00:01:24.740
We won't be able to form

31
00:01:24.740 --> 00:01:27.140
an accurate Monte Carlo estimate.

32
00:01:27.140 --> 00:01:29.870
The agent must try
all the actions in

33
00:01:29.870 --> 00:01:32.720
each state in order to
learn their values.

34
00:01:32.720 --> 00:01:34.340
This is the problem of

35
00:01:34.340 --> 00:01:37.235
maintaining exploration in
reinforcement learning.

36
00:01:37.235 --> 00:01:39.875
Let's think about
a real-world example.

37
00:01:39.875 --> 00:01:43.225
Imagine walking home along
the road you usually take.

38
00:01:43.225 --> 00:01:46.040
Recently, a new road
was built nearby.

39
00:01:46.040 --> 00:01:48.349
If we don't ever try the new way,

40
00:01:48.349 --> 00:01:51.215
then we couldn't know if
it was actually better.

41
00:01:51.215 --> 00:01:53.270
One way to maintain exploration

42
00:01:53.270 --> 00:01:55.180
is called exploring starts.

43
00:01:55.180 --> 00:01:57.780
In exploring starts,
we must guarantee that

44
00:01:57.780 --> 00:02:00.540
episodes start in
every state-action pair.

45
00:02:00.540 --> 00:02:04.460
Afterwards, the agent
simply follows its policy.

46
00:02:04.460 --> 00:02:07.375
Consider the grid world
on the screen.

47
00:02:07.375 --> 00:02:09.320
Let's see how exploring starts

48
00:02:09.320 --> 00:02:12.245
works with a policy shown in red.

49
00:02:12.245 --> 00:02:15.440
Exploring starts will
randomly sample a state and

50
00:02:15.440 --> 00:02:18.740
action at the start of
an episode shown in blue.

51
00:02:18.740 --> 00:02:20.750
In this case, the blue action

52
00:02:20.750 --> 00:02:23.030
is not the same as the red one.

53
00:02:23.030 --> 00:02:24.995
After the initial action,

54
00:02:24.995 --> 00:02:26.885
the agent will follow
the red policy

55
00:02:26.885 --> 00:02:28.865
until the episode ends.

56
00:02:28.865 --> 00:02:31.580
We must be able to set
the start state in

57
00:02:31.580 --> 00:02:34.385
this way to evaluate
a deterministic policy,

58
00:02:34.385 --> 00:02:36.185
like we can in this grid world.

59
00:02:36.185 --> 00:02:39.065
This may not always be possible.

60
00:02:39.065 --> 00:02:41.510
Other exploration strategies like

61
00:02:41.510 --> 00:02:43.160
epsilon-greedy can be used

62
00:02:43.160 --> 00:02:45.335
to evaluate stochastic policies.

63
00:02:45.335 --> 00:02:47.765
We will talk more
about that later.

64
00:02:47.765 --> 00:02:50.120
In this video, we described

65
00:02:50.120 --> 00:02:52.085
the Monte Carlo algorithm
for estimating

66
00:02:52.085 --> 00:02:54.650
action values and discussed

67
00:02:54.650 --> 00:02:57.790
the importance of
maintaining exploration.