WEBVTT

1
00:00:06.000 --> 00:00:09.900
You've made it through the whole course
on Sample based learning methods.

2
00:00:09.900 --> 00:00:10.900
Let's quickly review.

3
00:00:10.900 --> 00:00:12.000
What we've covered.

4
00:00:13.000 --> 00:00:16.400
We talked about two approaches to
learning from sampled experience.

5
00:00:16.400 --> 00:00:21.400
The first was Monte Carlo which averages
returns to estimate the value function.

6
00:00:21.400 --> 00:00:26.000
The second was TD learning which uses the
estimated next day value in his Target.

7
00:00:27.400 --> 00:00:31.500
We saw that TD learns from sample
experience like Monte Carlo but

8
00:00:31.500 --> 00:00:33.700
also bootstraps like dynamic programming.

9
00:00:34.800 --> 00:00:38.800
We showed you how to implement the TD
algorithm to estimate both state value and

10
00:00:38.800 --> 00:00:40.800
action value functions.

11
00:00:40.800 --> 00:00:44.500
We then talked about control
using sarsa expected sarsa, and

12
00:00:44.500 --> 00:00:48.200
q-learning star says
a non-policy algorithm.

13
00:00:48.200 --> 00:00:49.900
It estimates the value of the policy.

14
00:00:49.900 --> 00:00:53.400
It is following q-learning is off policy.

15
00:00:53.400 --> 00:00:56.200
It estimates the value function for
the optimal policy.

16
00:00:57.500 --> 00:01:00.900
Expected start so
I can be used on or off policy

17
00:01:00.900 --> 00:01:05.300
it explicitly calculates the expected
next state value in its update Target.

18
00:01:05.300 --> 00:01:08.400
This expectation can be
conditioned on any policy.

19
00:01:08.400 --> 00:01:09.500
So expect it starts.

20
00:01:09.500 --> 00:01:11.100
It can be used off policy.

21
00:01:12.700 --> 00:01:15.600
We show that Q learning as
a special case of expected sarsa.

22
00:01:17.700 --> 00:01:21.100
In the last module we talked about
planning with model experience.

23
00:01:21.100 --> 00:01:25.200
We introduced the Dyna architecture that
incorporates both planning from model

24
00:01:25.200 --> 00:01:28.100
experience and
learning from environment experience.

25
00:01:29.900 --> 00:01:34.900
We discussed Dinah Q which combines Q
learning and Q planning even experiment.

26
00:01:34.900 --> 00:01:37.800
We saw that Dynamic use planning
updates make it more sample

27
00:01:37.800 --> 00:01:39.200
efficient than Q learning.

28
00:01:40.900 --> 00:01:44.300
Finally, we consider what happens
when the model is inaccurate.

29
00:01:44.300 --> 00:01:48.200
We showed you how Donna Q Plus adds an
exploration bonus in the planning update

30
00:01:48.200 --> 00:01:50.600
to encourage exploration.

31
00:01:50.600 --> 00:01:54.700
This bonus helps Dinah q+ notice changes
in the environment that Dynamic you

32
00:01:54.700 --> 00:01:59.100
would not we hope you've enjoyed learning
about sample based learning methods and

33
00:01:59.100 --> 00:02:02.100
the next course will see how to
combine sample based learning

34
00:02:02.100 --> 00:02:03.500
with function approximation.

35
00:02:04.500 --> 00:02:07.400
We will learn about
generalization a powerful tool to

36
00:02:07.400 --> 00:02:11.900
scale up reinforcement learning and
dramatically improve sample efficiency

37
00:02:11.900 --> 00:02:14.300
many of the concepts you've
learned will transfer but

38
00:02:14.300 --> 00:02:18.400
there will be a few new challenges stick
around to find out how we resolve them.