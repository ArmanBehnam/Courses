WEBVTT

1
00:00:04.550 --> 00:00:06.570
Congratulations.

2
00:00:06.570 --> 00:00:08.730
You've just finished
Module 4 where we

3
00:00:08.730 --> 00:00:11.280
learned about planning,
learning, and acting.

4
00:00:11.280 --> 00:00:12.840
Let's go over the concepts and

5
00:00:12.840 --> 00:00:15.525
algorithms we've
covered in this module.

6
00:00:15.525 --> 00:00:18.690
In Lesson 1, we introduced
models and talked

7
00:00:18.690 --> 00:00:19.950
about the difference between

8
00:00:19.950 --> 00:00:22.230
distribution and sample models.

9
00:00:22.230 --> 00:00:24.720
Distribution models
store the probability

10
00:00:24.720 --> 00:00:26.625
of every possible outcome.

11
00:00:26.625 --> 00:00:29.945
However, this may require
a large amount of memory.

12
00:00:29.945 --> 00:00:32.150
Sample models are usually much

13
00:00:32.150 --> 00:00:34.115
more compact than
distribution models

14
00:00:34.115 --> 00:00:35.990
because they don't
explicitly store

15
00:00:35.990 --> 00:00:38.795
the probability of each outcome.

16
00:00:38.795 --> 00:00:43.390
In Lesson 2, we introduced
one-step Q-planning.

17
00:00:43.390 --> 00:00:46.770
Q-planning uses
the same update as Q-learning.

18
00:00:46.770 --> 00:00:49.340
However, it uses
experience generated

19
00:00:49.340 --> 00:00:52.115
by a model instead
of real experience.

20
00:00:52.115 --> 00:00:55.915
In Lesson 3, we introduced
the Dyna architecture.

21
00:00:55.915 --> 00:00:58.070
Dyna incorporates both planning

22
00:00:58.070 --> 00:01:00.125
and learning in a single agent.

23
00:01:00.125 --> 00:01:03.170
We saw that a Dyna Q-agent
can learn much more

24
00:01:03.170 --> 00:01:06.560
quickly by using
many planning updates.

25
00:01:06.560 --> 00:01:08.860
Models aren't always accurate.

26
00:01:08.860 --> 00:01:10.820
In Lesson 4, we talked about how

27
00:01:10.820 --> 00:01:12.980
inaccurate models
affect planning.

28
00:01:12.980 --> 00:01:16.190
We pointed out how Dyna-Q
can plan effectively

29
00:01:16.190 --> 00:01:19.865
using an incomplete model but
not if the model is wrong.

30
00:01:19.865 --> 00:01:22.750
Finally, we introduced Dyna-Q+.

31
00:01:22.750 --> 00:01:25.490
This algorithm uses
a reward bonus in

32
00:01:25.490 --> 00:01:28.495
its planning updates to
encourage exploration.

33
00:01:28.495 --> 00:01:31.230
By exploring, Dyna-Q+ keeps

34
00:01:31.230 --> 00:01:33.520
its model up-to-date
and accurate,

35
00:01:33.520 --> 00:01:36.110
resulting in better performance.

36
00:01:36.110 --> 00:01:39.020
We covered a lot of
material in this module.

37
00:01:39.020 --> 00:01:41.610
So well done keeping up.