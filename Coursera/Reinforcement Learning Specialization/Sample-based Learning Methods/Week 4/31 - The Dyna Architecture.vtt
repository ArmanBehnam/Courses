WEBVTT

1
00:00:00.000 --> 00:00:05.522
[MUSIC]

2
00:00:05.522 --> 00:00:08.172
So far, we've talked about
two sources of experience.

3
00:00:08.172 --> 00:00:10.453
The environment and the model.

4
00:00:10.453 --> 00:00:14.031
In direct RL, the agent learns
from experience generated by

5
00:00:14.031 --> 00:00:18.242
interacting with the world like in
Q-learning, but the agent can also

6
00:00:18.242 --> 00:00:22.400
generate simulate experience from
the model and use it for planning.

7
00:00:23.600 --> 00:00:27.000
Today, we will talk about one
way to combine direct RL and

8
00:00:27.000 --> 00:00:28.375
planning called Dyna.

9
00:00:29.700 --> 00:00:34.600
By the end of this video,
you will be able to understand how

10
00:00:34.600 --> 00:00:38.700
simulate experience from the model differs
from interacting with the environment.

11
00:00:40.300 --> 00:00:44.800
You will also understand how the Dyna
architecture mixes direct RL updates and

12
00:00:44.800 --> 00:00:45.800
planning updates.

13
00:00:47.200 --> 00:00:51.700
We've seen Q-learning which performs
updates using environment experience.

14
00:00:52.725 --> 00:00:55.400
Q-planning on the other hand performs

15
00:00:55.400 --> 00:00:58.000
updates using simulated
experience from the model.

16
00:00:59.200 --> 00:01:04.455
We can combine these two ideas
through the Dyna architecture.

17
00:01:04.455 --> 00:01:07.344
First, we will describe
the main components of Dyna.

18
00:01:07.344 --> 00:01:10.287
We have the usual environment and policy.

19
00:01:10.287 --> 00:01:13.259
They generate a stream of experience.

20
00:01:13.259 --> 00:01:17.300
We use this experience to
perform direct RL updates.

21
00:01:17.300 --> 00:01:20.000
To do planning, we need a model.

22
00:01:20.000 --> 00:01:21.600
The model has to come from somewhere.

23
00:01:22.800 --> 00:01:26.300
The environment experience can
be used to learn the model.

24
00:01:26.300 --> 00:01:29.600
This model will be used to
generate model experience.

25
00:01:29.600 --> 00:01:35.240
In addition, we want to control how the
model generates this simulated experience,

26
00:01:35.240 --> 00:01:37.762
what states the agent will plan from.

27
00:01:37.762 --> 00:01:40.125
We call this process search control.

28
00:01:41.903 --> 00:01:45.400
Planning updates are performed using
the experience generated by the model.

29
00:01:47.200 --> 00:01:50.646
Let's look at a cartoon of how Dyna works.

30
00:01:50.646 --> 00:01:55.586
Here we have a simple maze and
a robot trying to find its way out.

31
00:01:55.586 --> 00:01:57.279
It's a discounted problem.

32
00:01:57.279 --> 00:02:02.500
The robot receives a reward of plus
1 at the goal and 0, otherwise.

33
00:02:03.800 --> 00:02:06.300
As usual,
the robot starts out knowing nothing.

34
00:02:07.400 --> 00:02:11.200
It randomly traverses the maze until it
stumbles on the goal for the first time.

35
00:02:13.412 --> 00:02:17.500
A robot only updates one
action value depicted in blue.

36
00:02:17.500 --> 00:02:21.446
This was the only transition
that generated nonzero reward.

37
00:02:21.446 --> 00:02:25.095
This update is done by direct RL.

38
00:02:25.095 --> 00:02:27.734
Dyna makes use of all
the experience generated during

39
00:02:27.734 --> 00:02:30.321
the first episode to learn
a model the environment.

40
00:02:30.321 --> 00:02:34.314
The yellow states correspond to states
visited during the first episode.

41
00:02:34.314 --> 00:02:39.039
The robot didn't visit all of them,
but it visited most of them.

42
00:02:39.039 --> 00:02:41.664
Dyna performs planning on every time step.

43
00:02:41.664 --> 00:02:46.375
However, planning has no impact on
the policy during the first episode

44
00:02:46.375 --> 00:02:50.472
even though the model became
more accurate on each time step.

45
00:02:50.472 --> 00:02:53.990
After the first episode completes,
planning can really shine.

46
00:02:53.990 --> 00:02:55.092
In our cartoon,

47
00:02:55.092 --> 00:02:59.985
the thought bubble represents planning
happening in the robot's mind.

48
00:02:59.985 --> 00:03:03.200
The grid world in the bubble represents
the robots model of the world.

49
00:03:04.300 --> 00:03:07.900
The yellow states represent where
the model knows what will happen next.

50
00:03:09.100 --> 00:03:14.555
The model knows about all the states the
robot visited during the first episode.

51
00:03:14.555 --> 00:03:18.645
Let's look what happens if we run
the planning loop at the beginning

52
00:03:18.645 --> 00:03:20.090
of the second episode.

53
00:03:20.090 --> 00:03:24.903
Dyna can simulate transitions from any of
the state action pairs the agent visited

54
00:03:24.903 --> 00:03:26.516
during the first episode.

55
00:03:26.516 --> 00:03:28.600
In this case, there are a lot of them.

56
00:03:28.600 --> 00:03:33.500
Planning replays the simulated transitions
as if they happened in the real world.

57
00:03:33.500 --> 00:03:37.600
Each step of planning simply applies
the Q-learning update to a simulated

58
00:03:37.600 --> 00:03:39.900
transition and updates the value function.

59
00:03:41.800 --> 00:03:46.669
With enough planning steps, the agent
can approve the policy in all the states

60
00:03:46.669 --> 00:03:48.896
visited during the first episode.

61
00:03:48.896 --> 00:03:51.655
Here, planning produce
a pretty good policy.

62
00:03:51.655 --> 00:03:53.442
The policy is not perfect.

63
00:03:53.442 --> 00:03:55.600
Can you see where take suboptimal actions?

64
00:03:57.000 --> 00:04:01.000
The policy is still random in states
that were not visited before.

65
00:04:01.000 --> 00:04:07.716
We represent these dates for the policy
was not updated with white squares.

66
00:04:07.716 --> 00:04:10.774
Let's see how the plan policy
works in the real world.

67
00:04:10.774 --> 00:04:15.737
The robot now knows how to get to
the goal quickly after just one episode.

68
00:04:15.737 --> 00:04:21.358
In comparison, Q-learning require many
episodes to achieve similar performance.

69
00:04:21.358 --> 00:04:28.346
Dyna does more computation per step, but
uses limited experience more efficiently.

70
00:04:28.346 --> 00:04:30.709
Remember, this is only a cartoon.

71
00:04:30.709 --> 00:04:34.155
Dyna would act slightly
differently than shown here.

72
00:04:34.155 --> 00:04:36.111
The robot would continue exploring and

73
00:04:36.111 --> 00:04:38.369
planning on each step
of the second episode.

74
00:04:38.369 --> 00:04:41.933
So the policy would take random
actions once in a while and

75
00:04:41.933 --> 00:04:46.176
the policy would continue to change,
and improve due to planning.

76
00:04:46.176 --> 00:04:48.400
We will discuss how to
implement Dyna the next time.

77
00:04:49.900 --> 00:04:54.768
In this video, we introduced the Dyna
architecture which uses direct RL and

78
00:04:54.768 --> 00:04:59.423
planning updates to learn from both
environment and model experience.

79
00:04:59.423 --> 00:05:00.871
That's all for this video.

80
00:05:00.871 --> 00:05:02.800
I hope you plan on coming back next time.