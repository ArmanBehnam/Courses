WEBVTT

1
00:00:04.730 --> 00:00:09.015
Up until now, we've seen
three TD algorithms for control,

2
00:00:09.015 --> 00:00:12.765
Sarsa, Q-learning,
and Expected Sarsa.

3
00:00:12.765 --> 00:00:14.790
Two of those algorithms,

4
00:00:14.790 --> 00:00:16.755
Sarsa and Expected Sarsa

5
00:00:16.755 --> 00:00:19.845
both approximate
the same Bellman equation.

6
00:00:19.845 --> 00:00:21.570
Today, let's look at how

7
00:00:21.570 --> 00:00:24.900
Expected Sarsa is
related to Q-learning.

8
00:00:24.900 --> 00:00:27.030
By the end of this video,

9
00:00:27.030 --> 00:00:29.595
you will understand
how Expected Sarsa

10
00:00:29.595 --> 00:00:32.250
can do off-policy
learning without using

11
00:00:32.250 --> 00:00:35.250
importance sampling and explain

12
00:00:35.250 --> 00:00:39.045
how Expected Sarsa
generalizes Q-learning.

13
00:00:39.045 --> 00:00:41.995
Let's start with
the on-policy case,

14
00:00:41.995 --> 00:00:43.580
where the behavior policy and

15
00:00:43.580 --> 00:00:45.665
the target policy are equal.

16
00:00:45.665 --> 00:00:48.935
Consider the
Expected Sarsa update.

17
00:00:48.935 --> 00:00:53.275
The next action is sampled
from Pi in this case.

18
00:00:53.275 --> 00:00:57.470
However, notice that
the expectation over actions is

19
00:00:57.470 --> 00:00:59.750
computed independently
of the action

20
00:00:59.750 --> 00:01:02.540
actually selected
in the next state.

21
00:01:02.540 --> 00:01:07.020
In fact, Pi need not be equal
to the behavior policy.

22
00:01:07.020 --> 00:01:10.770
This means that Expected Sarsa,
like Q-learning,

23
00:01:10.770 --> 00:01:11.985
can be used to learn

24
00:01:11.985 --> 00:01:15.150
off-policy without
importance sampling.

25
00:01:15.150 --> 00:01:18.185
Now, what happens if
the target policy is greedy

26
00:01:18.185 --> 00:01:20.665
with respect to it's
action value estimates?

27
00:01:20.665 --> 00:01:23.720
We can see that
only the highest value action

28
00:01:23.720 --> 00:01:26.000
is considered in the expectation.

29
00:01:26.000 --> 00:01:29.120
This is equivalent to
computing the maximum over

30
00:01:29.120 --> 00:01:32.390
actions in the next state
just like in Q-learning.

31
00:01:32.390 --> 00:01:34.580
In other words, Q-Learning is

32
00:01:34.580 --> 00:01:37.400
a special case of Expected Sarsa.

33
00:01:37.400 --> 00:01:39.770
In this video, we showed that

34
00:01:39.770 --> 00:01:42.350
expected Sarsa and
Q-Learning both use

35
00:01:42.350 --> 00:01:45.350
the expectation over
their target policies

36
00:01:45.350 --> 00:01:47.210
in their update targets.

37
00:01:47.210 --> 00:01:48.550
This allows them to learn

38
00:01:48.550 --> 00:01:51.135
off-policy without
importance sampling.

39
00:01:51.135 --> 00:01:54.110
Expected Sarsa with
the target policy that's

40
00:01:54.110 --> 00:01:55.400
greedy with respect to

41
00:01:55.400 --> 00:01:58.800
its action values, is
exactly Q-learning.