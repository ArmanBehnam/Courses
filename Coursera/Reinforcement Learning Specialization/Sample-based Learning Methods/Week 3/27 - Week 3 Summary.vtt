WEBVTT

1
00:00:00.000 --> 00:00:05.379
[MUSIC]

2
00:00:05.379 --> 00:00:08.800
This week we learned how to
use TD methods for control.

3
00:00:08.800 --> 00:00:11.700
In this video we'll recap
everything we covered this week.

4
00:00:12.900 --> 00:00:16.100
The TD control algorithms
are based on Bellman equations.

5
00:00:17.700 --> 00:00:20.939
We learned about three of them.

6
00:00:20.939 --> 00:00:25.400
Sarsa uses a sample based
version of the Bellman equation.

7
00:00:25.400 --> 00:00:26.800
It learns Q-pi.

8
00:00:28.425 --> 00:00:31.700
Q-learning uses the Bellman
optimality equation.

9
00:00:31.700 --> 00:00:32.900
It learns Q-star.

10
00:00:35.100 --> 00:00:38.800
Expected sarsa uses the same
Bellman equation as Sarsa, but

11
00:00:38.800 --> 00:00:40.600
samples it differently.

12
00:00:40.600 --> 00:00:44.800
It takes an expectation over
the next action values.

13
00:00:44.800 --> 00:00:48.128
What's the story with on-policy and
off-policy learning?

14
00:00:48.128 --> 00:00:52.100
Sarsa is a on-policy algorithm
that learns the action values for

15
00:00:52.100 --> 00:00:53.906
the policy it's currently following.

16
00:00:53.906 --> 00:00:59.108
Q-learning is an off-policy algorithm
that learns the optimal action values.

17
00:00:59.108 --> 00:01:02.652
And Expected Sarsa is
both an on-policy and

18
00:01:02.652 --> 00:01:06.900
an off-policy algorithm that can learn
the action values for any policy.

19
00:01:07.900 --> 00:01:12.400
Sarsa can do better than Q-learning
when performance is measured online.

20
00:01:12.400 --> 00:01:17.883
This is because on-policy control methods
account for their own exploration.

21
00:01:17.883 --> 00:01:22.514
In the cliff world we saw that
q-learning frequently fell off the cliff

22
00:01:22.514 --> 00:01:24.997
because of its exploratory actions.

23
00:01:24.997 --> 00:01:29.357
Sarsa learned the longer but
safer path that rarely fell off the cliff,

24
00:01:29.357 --> 00:01:32.300
this resulted in higher reward.

25
00:01:32.300 --> 00:01:37.031
We then studied an improvement
over Sarsa called Expected Sarsa.

26
00:01:37.031 --> 00:01:40.375
In the cliff world Expected Sarsa
outperformed Sarsa for

27
00:01:40.375 --> 00:01:43.100
all the step size
parameter values we tested.

28
00:01:44.600 --> 00:01:50.228
This is because Expected Sarsa mitigates
the variance due to its own policy.

29
00:01:50.228 --> 00:01:52.740
Expected Sarsa, like the name suggests,

30
00:01:52.740 --> 00:01:55.400
takes the expectation
over the next action.

31
00:01:56.700 --> 00:01:58.347
That's all for this module.

32
00:01:58.347 --> 00:02:03.400
Next we'll learn how to combine planning
learning and acting see you then.