WEBVTT

1
00:00:07.130 --> 00:00:10.120
Q-Learning is an
off-policy algorithm.

2
00:00:10.120 --> 00:00:11.975
But so far, we've only seen

3
00:00:11.975 --> 00:00:15.195
off-policy algorithms that
use important sampling.

4
00:00:15.195 --> 00:00:16.920
How can Q-learning be off

5
00:00:16.920 --> 00:00:19.215
policy without using
important sampling?

6
00:00:19.215 --> 00:00:21.555
Let's dive in and find out.

7
00:00:21.555 --> 00:00:23.595
By the end of this video,

8
00:00:23.595 --> 00:00:25.500
you will understand
how Q-learning

9
00:00:25.500 --> 00:00:27.630
can be off-policy without using

10
00:00:27.630 --> 00:00:31.050
important sampling and
be able to describe

11
00:00:31.050 --> 00:00:32.520
how learning on-policy or

12
00:00:32.520 --> 00:00:36.520
off-policy might affect
performance in control.

13
00:00:36.980 --> 00:00:39.420
Recall that an agent estimates

14
00:00:39.420 --> 00:00:40.890
its value function according to

15
00:00:40.890 --> 00:00:44.370
expected returns under
their target policy.

16
00:00:44.370 --> 00:00:47.600
They actually behave according
to their behavior policy.

17
00:00:47.600 --> 00:00:51.125
When the target policy and
behavior policy are the same,

18
00:00:51.125 --> 00:00:53.315
the agent is learning on-policy,

19
00:00:53.315 --> 00:00:56.095
otherwise, the agent is
learning off-policy.

20
00:00:56.095 --> 00:00:58.635
Sarsa is an on-policy algorithm.

21
00:00:58.635 --> 00:01:01.320
In Sarsa, the agent
bootstraps off

22
00:01:01.320 --> 00:01:04.155
of the value of the action
it's going to take next,

23
00:01:04.155 --> 00:01:07.070
which is sampled from
its behavior policy.

24
00:01:07.070 --> 00:01:09.890
Q-learning instead,
bootstraps off of

25
00:01:09.890 --> 00:01:12.785
the largest action value
in its next state.

26
00:01:12.785 --> 00:01:15.800
This is like sampling
an action under an estimate of

27
00:01:15.800 --> 00:01:19.805
the optimal policy rather
than the behavior policy.

28
00:01:19.805 --> 00:01:22.880
Since Q-learning learns about
the best action it could

29
00:01:22.880 --> 00:01:26.745
possibly take rather than
the actions it actually takes,

30
00:01:26.745 --> 00:01:29.590
it is learning off-policy.

31
00:01:29.590 --> 00:01:32.720
Whenever you see a reinforced
learning algorithm,

32
00:01:32.720 --> 00:01:34.745
a natural question to ask is,

33
00:01:34.745 --> 00:01:37.555
"What are the target
in behavior policies?"

34
00:01:37.555 --> 00:01:40.070
Q-learning's target
policy is always

35
00:01:40.070 --> 00:01:43.055
greedy with respect to
its current values.

36
00:01:43.055 --> 00:01:46.400
However, is behavior policy
can be anything that

37
00:01:46.400 --> 00:01:47.870
continues to visit all state

38
00:01:47.870 --> 00:01:50.075
action pairs during learning.

39
00:01:50.075 --> 00:01:53.315
One possible policy
is epsilon greedy.

40
00:01:53.315 --> 00:01:55.700
The difference here
between the target and

41
00:01:55.700 --> 00:01:59.920
behavior policies confirms
that Q-learning is off-policy.

42
00:01:59.920 --> 00:02:02.740
But if Q-learning
learns off-policy,

43
00:02:02.740 --> 00:02:06.020
why don't we see any
important sampling ratios?

44
00:02:06.020 --> 00:02:08.675
It is because the
agent is estimating

45
00:02:08.675 --> 00:02:11.075
action values with
unknown policy.

46
00:02:11.075 --> 00:02:13.400
It does not need
important sampling ratios to

47
00:02:13.400 --> 00:02:16.400
correct for the difference
in action selection.

48
00:02:16.400 --> 00:02:19.580
The action value function
represents the returns

49
00:02:19.580 --> 00:02:23.190
following each action
in a given state.

50
00:02:23.290 --> 00:02:26.090
The agents target
policy represents

51
00:02:26.090 --> 00:02:29.270
the probability of taking
each action in a given state.

52
00:02:29.270 --> 00:02:31.759
Putting these two
elements together,

53
00:02:31.759 --> 00:02:34.325
the agent can calculate
the expected return

54
00:02:34.325 --> 00:02:37.235
under its target policy
from any given state,

55
00:02:37.235 --> 00:02:41.035
in particular,
the next state, S_t plus 1.

56
00:02:41.035 --> 00:02:43.940
Q-learning uses
exactly this technique

57
00:02:43.940 --> 00:02:45.860
to learn off-policy.

58
00:02:45.860 --> 00:02:48.395
Since the agents
target policies greedy,

59
00:02:48.395 --> 00:02:50.540
with respect to
its action values,

60
00:02:50.540 --> 00:02:54.365
all non-maximum actions
have probability 0.

61
00:02:54.365 --> 00:02:57.605
As a result, the expected return
from that state is

62
00:02:57.605 --> 00:03:01.415
equal to a maximal action value
from that state.

63
00:03:01.415 --> 00:03:04.880
Earlier, we talked about how
Q-learning doesn't iterate

64
00:03:04.880 --> 00:03:08.135
between policy evaluation
and policy improvement,

65
00:03:08.135 --> 00:03:12.170
but rather learns
the optimal values directly.

66
00:03:12.170 --> 00:03:14.750
Directly learning
the optimal value function

67
00:03:14.750 --> 00:03:16.385
and policy sounds great,

68
00:03:16.385 --> 00:03:18.860
but there are some subtleties
that make it less

69
00:03:18.860 --> 00:03:22.025
desirable in specific situations.

70
00:03:22.025 --> 00:03:24.590
Let's look at this cliff
walking example

71
00:03:24.590 --> 00:03:27.690
to understand some
of those subtleties.

72
00:03:27.940 --> 00:03:30.560
The cliff walking environment is

73
00:03:30.560 --> 00:03:32.840
an undiscounted
episodic gridworld

74
00:03:32.840 --> 00:03:35.135
with a cliff on the bottom edge.

75
00:03:35.135 --> 00:03:39.620
On most steps, the agent
receives a reward of minus 1.

76
00:03:39.620 --> 00:03:42.590
Falling off the cliff
however sends agent back to

77
00:03:42.590 --> 00:03:45.995
the start and gives
a reward of minus 100.

78
00:03:45.995 --> 00:03:48.200
We compare Q-learning
and Sarsa in

79
00:03:48.200 --> 00:03:51.470
this environment with
an epsilon of 0.1.

80
00:03:51.470 --> 00:03:54.365
Here's the average
performance of Q-learning.

81
00:03:54.365 --> 00:03:57.110
Since Q-learning learns
the optimal value function,

82
00:03:57.110 --> 00:03:58.250
it quickly learns that

83
00:03:58.250 --> 00:04:02.000
an optimal policy travels
right alongside the cliff.

84
00:04:02.000 --> 00:04:04.820
However, since his actions
or epsilon greedy,

85
00:04:04.820 --> 00:04:06.500
traveling alongside the cliff

86
00:04:06.500 --> 00:04:09.890
occasionally results and
falling off of the cliff.

87
00:04:09.890 --> 00:04:12.485
Sarsa learns about
his current policy,

88
00:04:12.485 --> 00:04:14.090
taking into account the effect of

89
00:04:14.090 --> 00:04:16.175
epsilon greedy action selection.

90
00:04:16.175 --> 00:04:19.040
Accounting for occasional
exploratory actions,

91
00:04:19.040 --> 00:04:22.385
it learns to take the longer
but more reliable path.

92
00:04:22.385 --> 00:04:26.320
They usually avoids randomly
falling into the cliff.

93
00:04:26.320 --> 00:04:28.470
Because of it's safer path,

94
00:04:28.470 --> 00:04:31.670
Sarsa is able to reach
the goal more reliably.

95
00:04:31.670 --> 00:04:34.820
In this video, we showed
that Q-learning is

96
00:04:34.820 --> 00:04:37.720
off-policy without using
important sampling,

97
00:04:37.720 --> 00:04:41.170
and that learning
on-policy or off-policy

98
00:04:41.170 --> 00:04:45.480
may perform differently in
control depending on the task.