WEBVTT

1
00:00:04.730 --> 00:00:09.525
Congrats. You are
now an expert on TD.

2
00:00:09.525 --> 00:00:12.000
In previous modules, we discussed

3
00:00:12.000 --> 00:00:14.760
dynamic programming
and Monte Carlo.

4
00:00:14.760 --> 00:00:17.280
In this module, we
took some of the

5
00:00:17.280 --> 00:00:19.965
best of both with TD learning.

6
00:00:19.965 --> 00:00:22.800
We started by borrowing
important concept

7
00:00:22.800 --> 00:00:25.800
from dynamic programming
called bootstrapping.

8
00:00:25.800 --> 00:00:29.145
TD updates towards
a bootstrap return.

9
00:00:29.145 --> 00:00:32.370
We can replace the return
after time step, t,

10
00:00:32.370 --> 00:00:34.070
with the reward plus

11
00:00:34.070 --> 00:00:36.815
our estimate of the value
in the next state.

12
00:00:36.815 --> 00:00:40.285
We say TD updates
a guess from a guess.

13
00:00:40.285 --> 00:00:42.680
It bootstraps its value estimates

14
00:00:42.680 --> 00:00:45.260
towards the value
of the next state.

15
00:00:45.260 --> 00:00:49.175
Then we discussed
the tabular TD zero algorithm.

16
00:00:49.175 --> 00:00:52.460
The main takeaway here
is that TD can update

17
00:00:52.460 --> 00:00:55.670
its value estimates on
each step of the episode.

18
00:00:55.670 --> 00:00:58.580
It does not have to wait for
the episode to complete.

19
00:00:58.580 --> 00:01:01.505
It just has to remember
the previous state.

20
00:01:01.505 --> 00:01:03.200
How does TD compare to

21
00:01:03.200 --> 00:01:05.570
Monte Carlo and
dynamic programming?

22
00:01:05.570 --> 00:01:10.005
Well, TD often converges
faster than Monte Carlo,

23
00:01:10.005 --> 00:01:14.395
TD does not require a model
unlike dynamic programming,

24
00:01:14.395 --> 00:01:17.285
and TD is online and
fully incremental

25
00:01:17.285 --> 00:01:20.735
unlike either Monte Carlo
or dynamic programming.

26
00:01:20.735 --> 00:01:22.880
We learned how TD updates

27
00:01:22.880 --> 00:01:26.420
its predictions as it
receives new information.

28
00:01:26.420 --> 00:01:29.000
We demonstrated
this by predicting

29
00:01:29.000 --> 00:01:31.670
how long it takes to
get home from work.

30
00:01:31.670 --> 00:01:34.670
We use TD to continually refine

31
00:01:34.670 --> 00:01:36.245
our predicted arrival time

32
00:01:36.245 --> 00:01:38.800
at each stage of our drive home.

33
00:01:38.800 --> 00:01:41.570
Finally, we ran
an experiment comparing

34
00:01:41.570 --> 00:01:44.660
TD and Monte Carlo
on a random walk.

35
00:01:44.660 --> 00:01:47.510
On every transition, TD updates

36
00:01:47.510 --> 00:01:48.860
its value function to reflect

37
00:01:48.860 --> 00:01:50.795
the most recent information.

38
00:01:50.795 --> 00:01:53.190
In contrast, Monte Carlo only

39
00:01:53.190 --> 00:01:56.345
updates its value function
at the end of each episode.

40
00:01:56.345 --> 00:01:59.240
In the long run, TD
learned more quickly than

41
00:01:59.240 --> 00:02:02.975
Monte Carlo and achieved
better final error.

42
00:02:02.975 --> 00:02:06.175
That's it for today.
In the coming weeks,

43
00:02:06.175 --> 00:02:08.985
we will build control
algorithms using TD.

44
00:02:08.985 --> 00:02:11.470
I can't wait to see you then.