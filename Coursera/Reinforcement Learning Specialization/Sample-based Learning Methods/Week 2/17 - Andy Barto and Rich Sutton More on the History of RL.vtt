WEBVTT

1
00:00:00.000 --> 00:00:02.565
I thought we might talk about

2
00:00:02.565 --> 00:00:05.040
when we first decided

3
00:00:05.040 --> 00:00:07.110
the reinforcement
learning was the thing.

4
00:00:07.110 --> 00:00:09.690
Did we ever decide that?

5
00:00:09.690 --> 00:00:11.880
I'd like to talk about
those early days too

6
00:00:11.880 --> 00:00:14.460
because there was
some striking things.

7
00:00:14.460 --> 00:00:19.425
So Rich was my first
graduate student.

8
00:00:19.425 --> 00:00:22.320
I want to tell you something
I don't think you know.

9
00:00:22.320 --> 00:00:27.840
So Harry Cloth, who we
wrote about it in the book,

10
00:00:27.840 --> 00:00:28.920
in a number of places,

11
00:00:28.920 --> 00:00:34.470
but he created a contract at

12
00:00:34.470 --> 00:00:36.870
UMass to see whether

13
00:00:36.870 --> 00:00:41.220
his hypothesis about Hedonistic
Neurons made any sense.

14
00:00:41.220 --> 00:00:44.280
So I was hired as a postdoc,

15
00:00:44.280 --> 00:00:49.740
and Harry according
to Michael Herbet,

16
00:00:49.740 --> 00:00:52.950
Michael was one of the
PIs on the contract.

17
00:00:52.950 --> 00:00:55.095
Wasn't a grant, it
was a contract.

18
00:00:55.095 --> 00:00:59.390
But Michael's memoirs which
have come out now said that

19
00:00:59.390 --> 00:01:04.390
Harry required them
to bring euro-bond,

20
00:01:04.390 --> 00:01:07.390
or they wouldn't get the funding.

21
00:01:07.550 --> 00:01:10.880
You didn't know that. I
didn't know that either

22
00:01:10.880 --> 00:01:13.310
until I read Michael's
[inaudible].

23
00:01:13.310 --> 00:01:15.170
The difference between, I don't

24
00:01:15.170 --> 00:01:17.450
think one reason we've
worked together so well.

25
00:01:17.450 --> 00:01:19.730
I am by nature a risk

26
00:01:19.730 --> 00:01:24.620
averse and Rich is a
risk seeking person.

27
00:01:24.620 --> 00:01:27.425
I mean swims with sharks,

28
00:01:27.425 --> 00:01:29.855
that's the farthest thing.

29
00:01:29.855 --> 00:01:36.270
I think the risk aversion has a-

30
00:01:36.270 --> 00:01:37.965
You're contrary.

31
00:01:37.965 --> 00:01:39.680
You don't want to go in

32
00:01:39.680 --> 00:01:41.605
the same direction as
all the other people.

33
00:01:41.605 --> 00:01:42.990
Which means one thing,

34
00:01:42.990 --> 00:01:46.260
that is that you as reinforcement
has been so popular,

35
00:01:46.260 --> 00:01:49.860
you almost go on to
step aside, you are.

36
00:01:49.860 --> 00:01:52.215
I go on and fill out on,

37
00:01:52.215 --> 00:01:54.080
I'm ailing because I
can't keep up with

38
00:01:54.080 --> 00:01:56.045
the literature to find a few that

39
00:01:56.045 --> 00:02:00.440
is sparsely covered so that
I can make it contribute.

40
00:02:00.440 --> 00:02:02.880
But now that I'm retired,

41
00:02:02.990 --> 00:02:07.385
I'm not going in any orthogonal
direction at the moment,

42
00:02:07.385 --> 00:02:09.560
I'm amazed with what

43
00:02:09.560 --> 00:02:12.170
everybody is doing with
reinforcement matter,

44
00:02:12.170 --> 00:02:14.900
and I'm thankful not to have

45
00:02:14.900 --> 00:02:17.720
to be in the thick of things.

46
00:02:17.720 --> 00:02:20.510
The industrial scale reinforced

47
00:02:20.510 --> 00:02:22.730
and hence it's great stuff,

48
00:02:22.730 --> 00:02:26.105
and it's just a flag of

49
00:02:26.105 --> 00:02:31.520
interesting papers but
I'm not keeping up.

50
00:02:31.520 --> 00:02:34.640
Well, I want to talk about

51
00:02:34.640 --> 00:02:38.510
this topic, concerning
this risk-seeking.

52
00:02:38.510 --> 00:02:39.770
You are the risk thing.

53
00:02:39.770 --> 00:02:43.550
Well, that you left

54
00:02:43.550 --> 00:02:47.210
your tenure-track position to

55
00:02:47.210 --> 00:02:49.950
study field that was unpopular.

56
00:02:49.950 --> 00:02:53.850
To be opposed, I don't recommend
leaving a tenure study.

57
00:02:53.850 --> 00:02:55.955
To study a field
that was unpopular.

58
00:02:55.955 --> 00:02:58.370
So I know you don't see this as

59
00:02:58.370 --> 00:02:59.450
risk-seeking but I think

60
00:02:59.450 --> 00:03:01.935
any other normal
human being would.

61
00:03:01.935 --> 00:03:04.290
You wanted to pursue what
you thought was important.

62
00:03:04.290 --> 00:03:06.405
Anyway, I don't want
to make it about you,

63
00:03:06.405 --> 00:03:07.725
I want to make it
about the field.

64
00:03:07.725 --> 00:03:11.310
So the field, well,

65
00:03:11.310 --> 00:03:12.365
it wasn't interested in learning,

66
00:03:12.365 --> 00:03:13.970
but I wanted

67
00:03:13.970 --> 00:03:16.960
reinforcement learning
and supervised learning.

68
00:03:17.270 --> 00:03:20.880
How in the 60s of

69
00:03:20.880 --> 00:03:25.060
the first wave of neural
networks and machine learning,

70
00:03:25.220 --> 00:03:27.675
it turned into
supervised learning.

71
00:03:27.675 --> 00:03:29.910
For the early guys
in the 50s talked

72
00:03:29.910 --> 00:03:32.520
about things that
were like reward,

73
00:03:32.520 --> 00:03:36.005
but then they ended up
studying supervised learning.

74
00:03:36.005 --> 00:03:37.760
So that's true.

75
00:03:37.760 --> 00:03:39.170
That's happened over and over

76
00:03:39.170 --> 00:03:40.880
again through all the waves.

77
00:03:40.880 --> 00:03:43.380
There's always things coming up,

78
00:03:43.380 --> 00:03:44.900
there's always a little
bit of supervised learning

79
00:03:44.900 --> 00:03:47.855
versus reinforcement
learning that the-

80
00:03:47.855 --> 00:03:51.020
Well, I've seen things in

81
00:03:51.020 --> 00:03:54.410
the current literature
by sophisticated people

82
00:03:54.410 --> 00:03:58.505
misunderstanding the
distinction between

83
00:03:58.505 --> 00:04:03.425
error correction and
trial and error. It just-

84
00:04:03.425 --> 00:04:05.285
Almost in the same way,

85
00:04:05.285 --> 00:04:08.435
it's been misunderstood
throughout history.

86
00:04:08.435 --> 00:04:09.560
Yeah, well.

87
00:04:09.560 --> 00:04:14.310
Throughout three wave
[inaudible] since the 50s.

88
00:04:14.310 --> 00:04:16.305
Yeah, that's true about it.

89
00:04:16.305 --> 00:04:20.385
We were trying to obtain
with Harry's idea,

90
00:04:20.385 --> 00:04:23.200
here is we're worth studying.

91
00:04:23.600 --> 00:04:26.790
So we went back and
looked at everything,

92
00:04:26.790 --> 00:04:28.490
and we discovered

93
00:04:28.490 --> 00:04:32.105
real reinforcement learning
among some people.

94
00:04:32.105 --> 00:04:34.130
But the field as a whole was

95
00:04:34.130 --> 00:04:37.120
focused on supervised learning.

96
00:04:37.120 --> 00:04:38.970
Those distinctions were not

97
00:04:38.970 --> 00:04:40.040
being remembered by the people.

98
00:04:40.040 --> 00:04:42.480
That was a real revelation.

99
00:04:43.460 --> 00:04:45.840
I remember my first round of

100
00:04:45.840 --> 00:04:49.155
talks back in those
days was about that.

101
00:04:49.155 --> 00:04:51.180
There's trial and error learning,

102
00:04:51.180 --> 00:04:53.400
it's not supervised learning.

103
00:04:53.400 --> 00:04:56.145
So that's true.

104
00:04:56.145 --> 00:04:57.540
It was a surprise.

105
00:04:57.540 --> 00:05:00.740
So I wrote this AI
magazine article

106
00:05:00.740 --> 00:05:03.860
recently that went through
some of the surprises,

107
00:05:03.860 --> 00:05:07.055
and that was like number
one, that was wow.

108
00:05:07.055 --> 00:05:12.110
This is actually a course
that appealed to me

109
00:05:12.110 --> 00:05:14.270
because then maybe we could do

110
00:05:14.270 --> 00:05:18.440
something innovative that
hadn't been done before.

111
00:05:18.440 --> 00:05:21.740
Because most people
hadn't studied that,

112
00:05:21.740 --> 00:05:24.305
except in engineering and in

113
00:05:24.305 --> 00:05:29.330
statistical stochastic
control people and I

114
00:05:29.330 --> 00:05:32.345
thought had been doing things
like this but it wasn't

115
00:05:32.345 --> 00:05:37.545
AI community that
really latch onto that.

116
00:05:37.545 --> 00:05:39.210
All the way I remember it,

117
00:05:39.210 --> 00:05:41.930
we've looked through all
the different fields.

118
00:05:41.930 --> 00:05:44.810
But we couldn't find
someone who studied

119
00:05:44.810 --> 00:05:50.665
learning for to
optimize a signal.

120
00:05:50.665 --> 00:05:52.200
For what?

121
00:05:52.200 --> 00:05:54.270
To optimize a signal.

122
00:05:54.270 --> 00:05:57.660
I except for the learning
on top of people.

123
00:05:57.660 --> 00:05:59.595
Yeah.

124
00:05:59.595 --> 00:06:01.515
Which nobody knew about.

125
00:06:01.515 --> 00:06:03.255
Which was not associative.

126
00:06:03.255 --> 00:06:04.440
Yeah, pretty much.

127
00:06:04.440 --> 00:06:07.670
I mean there were a few
works with bandits.

128
00:06:07.670 --> 00:06:13.040
But basically which you

129
00:06:13.040 --> 00:06:17.680
came up dry or the
very few things,

130
00:06:17.680 --> 00:06:20.340
and it was strength because

131
00:06:20.340 --> 00:06:23.460
it's an important
kind of learning.

132
00:06:23.460 --> 00:06:26.070
So it is just a strange fact

133
00:06:26.070 --> 00:06:27.840
that these in reinforcement we

134
00:06:27.840 --> 00:06:28.980
now see you like we're having

135
00:06:28.980 --> 00:06:32.010
a second edition of
a textbook on it,

136
00:06:32.010 --> 00:06:33.800
and we are having
a course all about

137
00:06:33.800 --> 00:06:36.630
it and like it wasn't a thing.

138
00:06:38.590 --> 00:06:42.820
In 1980, it wasn't a topic.

139
00:06:42.820 --> 00:06:48.705
It was subsumed and swelled
up and obliterated,

140
00:06:48.705 --> 00:06:52.290
could be seen because
of supervised learning.

141
00:06:52.290 --> 00:06:57.345
But it also had
suggestions of behaviors.

142
00:06:57.345 --> 00:06:59.355
Behaviors was bad.

143
00:06:59.355 --> 00:07:01.665
Everybody knows behavior was bad

144
00:07:01.665 --> 00:07:04.385
One of the most unfashionable
thing is around,

145
00:07:04.385 --> 00:07:08.240
and so our report
line is behaviors and

146
00:07:08.240 --> 00:07:12.015
then that includes elements

147
00:07:12.015 --> 00:07:15.610
of behavior some but
not all of them.

148
00:07:16.700 --> 00:07:19.130
We felt that people threw

149
00:07:19.130 --> 00:07:20.870
the baby out of the bathwater by

150
00:07:20.870 --> 00:07:26.675
rejecting common sense
reinforcement learning principles.

151
00:07:26.675 --> 00:07:28.805
But even today, I mean
it's already prone to

152
00:07:28.805 --> 00:07:31.280
new behaviors which is irritating

153
00:07:31.280 --> 00:07:33.859
because all these models

154
00:07:33.859 --> 00:07:37.575
and value functions and
all kinds of things

155
00:07:37.575 --> 00:07:40.355
that pure behaviorists would just

156
00:07:40.355 --> 00:07:44.745
not allow in interference.

157
00:07:44.745 --> 00:07:49.225
So it's not behaviors.

158
00:07:49.225 --> 00:07:51.635
Well, it's all about
what's going on

159
00:07:51.635 --> 00:07:54.780
in the algorithms
inside the head.

160
00:07:54.880 --> 00:07:57.815
I guess pure behaviorism.

161
00:07:57.815 --> 00:08:00.170
While we may come to [inaudible]
that's in our head but

162
00:08:00.170 --> 00:08:03.030
you can't observe behaviorist,

163
00:08:03.030 --> 00:08:09.735
at least pure behaviors
when we cut into that.

164
00:08:09.735 --> 00:08:14.560
But I learned a lot from
the pure behaviorists,

165
00:08:16.040 --> 00:08:18.420
and I really it's a shame that

166
00:08:18.420 --> 00:08:29.010
it just became unpopular.

167
00:08:29.010 --> 00:08:32.160
That's much that's been lost,

168
00:08:32.160 --> 00:08:34.220
all they have learned.

169
00:08:34.220 --> 00:08:38.600
While the other hand it's
sort of given me like

170
00:08:38.600 --> 00:08:40.460
a secret weapon because I can

171
00:08:40.460 --> 00:08:43.630
understand what
behavior is learned.

172
00:08:43.630 --> 00:08:47.045
So other people haven't
bothered to learn.

173
00:08:47.045 --> 00:08:50.450
That's where temporal
difference learning came from.

174
00:08:55.910 --> 00:08:58.979
Intrinsically
motivated persistence,

175
00:08:58.979 --> 00:09:01.750
because throughout all of that,

176
00:09:01.750 --> 00:09:03.320
there are periods
when there wasn't

177
00:09:03.320 --> 00:09:07.580
an extrinsic source of
reward for the researchers.

178
00:09:07.580 --> 00:09:12.990
But Jeffrey and I
think us and others

179
00:09:12.990 --> 00:09:16.430
but I mean some
people would think

180
00:09:16.430 --> 00:09:21.470
it's foolish to keep
studying the same thing.

181
00:09:21.470 --> 00:09:23.270
Allen Newell gave a talk.

182
00:09:23.270 --> 00:09:25.365
There's a video of that

183
00:09:25.365 --> 00:09:28.815
and he characterized different
kinds of researchers.

184
00:09:28.815 --> 00:09:31.660
I forget all the names
they gave him but there

185
00:09:31.660 --> 00:09:35.215
were nomads, nomadic research.

186
00:09:35.215 --> 00:09:37.395
So something hot pops up,

187
00:09:37.395 --> 00:09:40.475
all the nomads move
over to that area,

188
00:09:40.475 --> 00:09:43.670
study that for a while until
something else pops down,

189
00:09:43.670 --> 00:09:45.430
and move over to that area.

190
00:09:45.430 --> 00:09:48.980
I think we've been
the opposite of that.

191
00:09:48.980 --> 00:09:50.750
I mean all sorts of
things that popped

192
00:09:50.750 --> 00:09:55.065
up but we've ignored.

193
00:09:55.065 --> 00:09:57.885
So that makes us the
old defining that is.

194
00:09:57.885 --> 00:09:58.140
Yeah.

195
00:09:58.140 --> 00:10:03.450
Yeah.

196
00:10:03.450 --> 00:10:06.315
I don't know it's
contrary or stubborn or-

197
00:10:06.315 --> 00:10:09.735
Well, I got interested
in intrinsic,

198
00:10:09.735 --> 00:10:13.365
what psychologists gone
through as well they film her,

199
00:10:13.365 --> 00:10:15.645
you do something
for its own sake,

200
00:10:15.645 --> 00:10:19.690
rather than perform an
extrinsic reward of some kind.

201
00:10:19.690 --> 00:10:22.780
I think it's very powerful,

202
00:10:22.820 --> 00:10:26.950
I mean I've got actually
because I got and still think

203
00:10:26.950 --> 00:10:30.210
it's a good way to
create subskills,

204
00:10:30.210 --> 00:10:34.635
options or whatever you want
to call them from intrinsic.

205
00:10:34.635 --> 00:10:37.855
They might be useful in the
future but they might not.

206
00:10:37.855 --> 00:10:39.879
It's like basic research,

207
00:10:39.879 --> 00:10:42.850
is intrinsically
motivated whether

208
00:10:42.850 --> 00:10:45.955
there's utility or not,

209
00:10:45.955 --> 00:10:48.415
and it turns out there has been,

210
00:10:48.415 --> 00:10:50.890
it's incredible
utility but that was

211
00:10:50.890 --> 00:10:53.500
not to drive imports as opposed

212
00:10:53.500 --> 00:10:57.925
to explicitly project driven

213
00:10:57.925 --> 00:11:01.390
and which are important
too but the intrinsic.

214
00:11:01.390 --> 00:11:04.310
I think animals go through

215
00:11:04.310 --> 00:11:06.175
intrinsically
motivated state where

216
00:11:06.175 --> 00:11:07.520
they allow kinds of skills,

217
00:11:07.520 --> 00:11:09.995
become competent at
dealing with their worlds.

218
00:11:09.995 --> 00:11:13.730
Then when they have problems
that they have to solve,

219
00:11:13.730 --> 00:11:14.975
they have an armament.

220
00:11:14.975 --> 00:11:19.555
They have some skills that
they can bring to bear.

221
00:11:19.555 --> 00:11:22.450
I think that's an
important principle,

222
00:11:23.510 --> 00:11:26.600
and I think I've been

223
00:11:26.600 --> 00:11:29.790
intrinsically
motivated with too for

224
00:11:29.790 --> 00:11:31.500
these issues that now

225
00:11:31.500 --> 00:11:37.350
are reinforcement
learning a set of ideas.

226
00:11:37.350 --> 00:11:40.605
To me they're just interesting,

227
00:11:40.605 --> 00:11:44.025
and the fact that they work is

228
00:11:44.025 --> 00:11:47.600
a wonderful reward but that was

229
00:11:47.600 --> 00:11:52.900
not the driving force
at least for me.

230
00:11:54.050 --> 00:12:00.175
We've just had great success
in reinforcement learning

231
00:12:00.175 --> 00:12:02.530
and I just want to

232
00:12:02.530 --> 00:12:04.280
congratulate you and celebrate

233
00:12:04.280 --> 00:12:06.990
with you and give you
like a high-five.