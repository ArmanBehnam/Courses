WEBVTT

1
00:00:05.030 --> 00:00:07.305
In the last few videos,

2
00:00:07.305 --> 00:00:09.000
we introduced TD and discussed

3
00:00:09.000 --> 00:00:12.555
its advantages over Dynamic
Programming and Monte Carlo.

4
00:00:12.555 --> 00:00:16.110
In this video, we will dive
deeper and compare TD and

5
00:00:16.110 --> 00:00:17.310
Monte Carlo in

6
00:00:17.310 --> 00:00:20.595
a carefully constructed
scientific experiment.

7
00:00:20.595 --> 00:00:22.665
By the end of this video,

8
00:00:22.665 --> 00:00:24.530
you'll be able to: identify

9
00:00:24.530 --> 00:00:27.335
the empirical benefits
of TD Learning.

10
00:00:27.335 --> 00:00:29.840
Consider this simple NDP.

11
00:00:29.840 --> 00:00:32.750
We have five non-terminal states.

12
00:00:32.750 --> 00:00:34.490
In each state, we have

13
00:00:34.490 --> 00:00:38.105
two deterministic actions,
left and right.

14
00:00:38.105 --> 00:00:41.375
Let's evaluate
the uniform random policy.

15
00:00:41.375 --> 00:00:44.225
We want to estimate
its value function.

16
00:00:44.225 --> 00:00:47.630
All episodes start in state C,

17
00:00:47.630 --> 00:00:51.440
episodes terminate either on
the left or on the right.

18
00:00:51.440 --> 00:00:53.120
The reward is zero on

19
00:00:53.120 --> 00:00:55.970
all transitions except plus

20
00:00:55.970 --> 00:00:58.745
one for terminating on the right.

21
00:00:58.745 --> 00:01:01.595
Let's set the discount
factor to be one.

22
00:01:01.595 --> 00:01:04.835
In this problem, the value
has an intuitive meaning.

23
00:01:04.835 --> 00:01:07.070
The value of each state
is the probability

24
00:01:07.070 --> 00:01:10.280
terminating on the right when
starting from each state.

25
00:01:10.280 --> 00:01:13.205
The value of the start
state is 0.5,

26
00:01:13.205 --> 00:01:15.170
that means the probability
of terminating

27
00:01:15.170 --> 00:01:17.330
from the center is a half.

28
00:01:17.330 --> 00:01:20.150
Randomly wandering
from the center has

29
00:01:20.150 --> 00:01:22.205
a 50-50 chance of terminating

30
00:01:22.205 --> 00:01:25.295
on either side. That makes sense.

31
00:01:25.295 --> 00:01:28.850
Let's label the rest of
the states with their values.

32
00:01:28.850 --> 00:01:31.805
Let's run an
experiment right now.

33
00:01:31.805 --> 00:01:33.770
We have labeled the NDP with

34
00:01:33.770 --> 00:01:36.290
the true values at
the top of the slide.

35
00:01:36.290 --> 00:01:39.785
The agent, our cute little robot

36
00:01:39.785 --> 00:01:43.340
estimates the value
function using TD.

37
00:01:43.340 --> 00:01:47.230
At the bottom, we have
a Monte Carlo agent.

38
00:01:47.690 --> 00:01:51.200
Let's initialize
the approximate value function

39
00:01:51.200 --> 00:01:54.110
for both agents to 0.5.

40
00:01:54.110 --> 00:01:55.850
Let's look at the performance of

41
00:01:55.850 --> 00:01:58.800
both agents during
the first episode.

42
00:02:03.200 --> 00:02:07.145
Notice, the TD agent
only updated the value

43
00:02:07.145 --> 00:02:10.190
of state E. To understand this,

44
00:02:10.190 --> 00:02:12.920
consider the transition
from state C to

45
00:02:12.920 --> 00:02:15.710
D. The TD error is

46
00:02:15.710 --> 00:02:18.590
equal to the reward on
the transition plus

47
00:02:18.590 --> 00:02:22.220
the value of the next
state D minus the value of

48
00:02:22.220 --> 00:02:25.340
state C. The reward is

49
00:02:25.340 --> 00:02:28.995
zero because this is
a non-terminal transition.

50
00:02:28.995 --> 00:02:33.165
The values of C and
D are both 0.5.

51
00:02:33.165 --> 00:02:35.720
So the TDR is zero and we make

52
00:02:35.720 --> 00:02:39.620
no change to the estimate for
C. The same thing happens

53
00:02:39.620 --> 00:02:42.080
on every step except
the transition into

54
00:02:42.080 --> 00:02:46.415
this terminal state from
state E. In contrast,

55
00:02:46.415 --> 00:02:48.095
Monte Carlo updated the values of

56
00:02:48.095 --> 00:02:51.215
all the states the robot
saw during the episode.

57
00:02:51.215 --> 00:02:53.600
Let's look at the next episode.

58
00:02:53.600 --> 00:02:55.460
Pay attention to how the values

59
00:02:55.460 --> 00:02:57.290
are updated during the episode.

60
00:02:57.290 --> 00:02:59.390
Eventually, the TD agent makes

61
00:02:59.390 --> 00:03:01.790
updates to the values
on every step.

62
00:03:01.790 --> 00:03:04.850
In contrast, the Monte
Carlo agent waited

63
00:03:04.850 --> 00:03:08.195
until the end of the episode
before it made its updates.

64
00:03:08.195 --> 00:03:11.670
Let's look at
a few more episodes.

65
00:03:20.210 --> 00:03:23.045
The value estimates
of the TD agent

66
00:03:23.045 --> 00:03:25.805
seemed to be moving
towards the true values.

67
00:03:25.805 --> 00:03:27.845
This is taking too long.

68
00:03:27.845 --> 00:03:31.655
Let's jump ahead and evaluate
the asymptotic performance.

69
00:03:31.655 --> 00:03:33.830
Let's plot the estimated values

70
00:03:33.830 --> 00:03:36.080
at several points
during learning.

71
00:03:36.080 --> 00:03:40.205
The x-axis represents
each state in the NDP.

72
00:03:40.205 --> 00:03:43.805
On the y-axis, we have
the estimated value.

73
00:03:43.805 --> 00:03:47.030
Here are the true values.

74
00:03:47.030 --> 00:03:50.270
Lets also plot
the initial value estimates.

75
00:03:50.270 --> 00:03:53.510
Recall, we initialize
them to 0.5.

76
00:03:53.510 --> 00:03:55.640
The red curve shows the values

77
00:03:55.640 --> 00:03:57.530
learned after the first episode.

78
00:03:57.530 --> 00:03:59.425
By the end of the first episode,

79
00:03:59.425 --> 00:04:01.430
TD only updated the value of

80
00:04:01.430 --> 00:04:03.875
a single state as we discussed.

81
00:04:03.875 --> 00:04:06.290
The estimates after
a 100 episodes

82
00:04:06.290 --> 00:04:08.780
are about as good as
they're ever going to get.

83
00:04:08.780 --> 00:04:12.995
Remember, we're using
a constant step size of 0.1.

84
00:04:12.995 --> 00:04:15.320
This means the values
will fluctuate in

85
00:04:15.320 --> 00:04:18.515
response to the outcomes of
the most recent episodes.

86
00:04:18.515 --> 00:04:20.720
If you use a smaller
learning rate or

87
00:04:20.720 --> 00:04:22.625
better yet a decaying
learning rate,

88
00:04:22.625 --> 00:04:24.695
we might get a better estimate.

89
00:04:24.695 --> 00:04:26.875
A natural next question is,

90
00:04:26.875 --> 00:04:29.730
does TD learn faster
than Monte Carlo?

91
00:04:29.730 --> 00:04:31.940
Let's compare
the performance of TD and

92
00:04:31.940 --> 00:04:34.745
Monte Carlo on
our example problem.

93
00:04:34.745 --> 00:04:38.555
Here, the x-axis represents
the number of episodes.

94
00:04:38.555 --> 00:04:41.420
The y-axis represents
the root mean

95
00:04:41.420 --> 00:04:43.850
squared error between
the value function

96
00:04:43.850 --> 00:04:46.110
and the learned estimates.

97
00:04:46.270 --> 00:04:49.340
The red learning curves
represent the performance of

98
00:04:49.340 --> 00:04:52.065
Monte Carlo for
several values of Alpha.

99
00:04:52.065 --> 00:04:56.455
Each curve is averaged over
a 100 independent runs.

100
00:04:56.455 --> 00:04:58.790
For instance, this point

101
00:04:58.790 --> 00:05:01.130
represents the average
error achieve after

102
00:05:01.130 --> 00:05:06.205
50 episodes for the
learning rate 0.01.

103
00:05:06.205 --> 00:05:09.670
Now, let's look at
TD's performance.

104
00:05:09.670 --> 00:05:11.930
We see that TD perform

105
00:05:11.930 --> 00:05:14.030
consistently better
than Monte Carlo.

106
00:05:14.030 --> 00:05:16.290
Let's look a little more closely.

107
00:05:16.290 --> 00:05:17.990
Notice, the error reduces

108
00:05:17.990 --> 00:05:20.735
faster with a learning
rate of 0.15,

109
00:05:20.735 --> 00:05:24.050
but ultimately, results
in higher final error.

110
00:05:24.050 --> 00:05:26.120
With a smaller learning rate,

111
00:05:26.120 --> 00:05:30.270
TD learns more slowly but
achieves lower final error.

112
00:05:30.270 --> 00:05:32.420
In summary, we ran

113
00:05:32.420 --> 00:05:35.770
a careful experiment
comparing TD and Monte Carlo,

114
00:05:35.770 --> 00:05:39.230
and the results suggest
that TD converges faster to

115
00:05:39.230 --> 00:05:43.440
a lower final error in
this problem. See you next time.