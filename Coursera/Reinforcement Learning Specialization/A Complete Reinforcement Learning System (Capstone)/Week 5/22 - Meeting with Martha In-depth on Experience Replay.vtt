WEBVTT

1
00:00:00.006 --> 00:00:05.759
[MUSIC]

2
00:00:05.759 --> 00:00:10.162
In Course Three, the agents you
implemented update the value function or

3
00:00:10.162 --> 00:00:12.700
policy only once with each sample.

4
00:00:12.700 --> 00:00:16.500
But this is likely not the most
sample efficient way to use our data.

5
00:00:16.500 --> 00:00:20.800
You have actually seen a smarter approach
in Course Two where we talked about Dyna

6
00:00:20.800 --> 00:00:23.300
as a way to be more sample efficient.

7
00:00:23.300 --> 00:00:25.925
But we only talked about Dyna for
the tabular setting.

8
00:00:25.925 --> 00:00:28.803
>> [SOUND]
>> In this video, we will talk about how

9
00:00:28.803 --> 00:00:33.000
to make your agent more sample efficient
when using function approximation.

10
00:00:33.000 --> 00:00:37.607
We will discuss a simple method
called experience replay and

11
00:00:37.607 --> 00:00:39.421
how it relates to Dyna.

12
00:00:39.421 --> 00:00:42.151
To get some intuition for
experience replay,

13
00:00:42.151 --> 00:00:45.729
let's first remember a method
that we know well, Dyna-Q.

14
00:00:45.729 --> 00:00:49.600
The idea is to learn a model
using sample experience.

15
00:00:49.600 --> 00:00:55.100
Then simulated experience can be obtained
from this model to update the values.

16
00:00:55.100 --> 00:00:58.700
This procedure of using simulated
experience to improve the value estimates

17
00:00:58.700 --> 00:00:59.600
is called planning.

18
00:01:01.000 --> 00:01:06.312
To focus on the key ideas in Dyna, we made
a few simplifying assumptions, namely

19
00:01:06.312 --> 00:01:11.700
Dyna-Q assumes deterministic transitions
for a relatively small state space.

20
00:01:11.700 --> 00:01:14.300
For this setting,
learning the model was simple.

21
00:01:14.300 --> 00:01:20.399
For every visit state action pair, we
record the observed reward and next state.

22
00:01:20.399 --> 00:01:24.080
This only works in the tabular setting
where the number of state action pairs is

23
00:01:24.080 --> 00:01:26.100
small enough to fit into computer memory.

24
00:01:27.200 --> 00:01:30.033
If we were to attempt this in
a continuous state domain,

25
00:01:30.033 --> 00:01:33.244
we would have an infinite amount
of state action pair at store.

26
00:01:33.244 --> 00:01:39.333
Or worse than that, we will not see
the same state twice if the world is.

27
00:01:39.333 --> 00:01:43.324
One option is to learn a model using
all your new knowledge about function

28
00:01:43.324 --> 00:01:44.314
approximation.

29
00:01:44.314 --> 00:01:48.778
For example, you can learn a parameterize
function that input states and

30
00:01:48.778 --> 00:01:51.587
actions and
outputs next states and rewards.

31
00:01:51.587 --> 00:01:56.551
Unfortunately, even small errors in such a
learn dynamics model seem to cause serious

32
00:01:56.551 --> 00:01:57.600
issues.

33
00:01:57.600 --> 00:02:00.300
It remains an open resource
question how to effectively

34
00:02:00.300 --> 00:02:02.825
use learn models in an RL agent.

35
00:02:02.825 --> 00:02:04.732
>> [SOUND]
>> Fortunately,

36
00:02:04.732 --> 00:02:07.567
there is a simpler option for
your Capstone.

37
00:02:07.567 --> 00:02:12.216
Experience replay is a simple method for
trying to get the advantages of Dyna.

38
00:02:12.216 --> 00:02:17.628
The basic idea is to save a buffer of
experience and let the data be the model.

39
00:02:17.628 --> 00:02:23.008
We sample experience from this buffer and
update the value function with those

40
00:02:23.008 --> 00:02:28.482
samples similarly to how we sample from
the model and update the values in Dyna.

41
00:02:28.482 --> 00:02:32.663
The key choices for experience
replay are the size of the buffer,

42
00:02:32.663 --> 00:02:37.086
what experience to store, and
how many updates to execute per step?

43
00:02:37.086 --> 00:02:40.163
Let's first take a look at
how we store the experiences.

44
00:02:40.163 --> 00:02:43.967
For simplicity,
let's go back to the tabular grid world.

45
00:02:43.967 --> 00:02:48.587
As you interact with the world,
we observe a state, action, reward,

46
00:02:48.587 --> 00:02:49.898
next state tupple.

47
00:02:49.898 --> 00:02:54.780
We add these experiences to a buffer that
we'll call the experience replay buffer.

48
00:02:54.780 --> 00:02:57.668
As we continue interacting with the world,

49
00:02:57.668 --> 00:03:01.971
we add more samples to this buffer
until we eventually fill it up.

50
00:03:01.971 --> 00:03:06.216
When this happens, we can pick
an older experienced to delete and

51
00:03:06.216 --> 00:03:08.310
replace with a new experience.

52
00:03:08.310 --> 00:03:11.259
We also need to consider
the size of our buffer.

53
00:03:11.259 --> 00:03:13.729
By allowing the buffer to be really large,

54
00:03:13.729 --> 00:03:18.126
we can remember potentially useful
transitions from many times steps ago.

55
00:03:18.126 --> 00:03:21.332
However, the agent does
have practical limitations.

56
00:03:21.332 --> 00:03:25.518
We will need to consider the amount
of memory used by large buffers and

57
00:03:25.518 --> 00:03:29.863
any computational impacts from storing and
accessing large buffers.

58
00:03:29.863 --> 00:03:34.800
What we have decided to store buffer
a natural idea is to use mini batches.

59
00:03:34.800 --> 00:03:39.700
Using a single sample from the experience
replay buffer can produce a noisy update.

60
00:03:39.700 --> 00:03:44.000
Instead, we can use several samples from
the buffer to create an average update to

61
00:03:44.000 --> 00:03:45.900
reduce that noise.

62
00:03:45.900 --> 00:03:50.956
The small collection of samples from
the larger buffer is called a mini batch.

63
00:03:50.956 --> 00:03:55.158
The mini batch updates simply involves
averaging the updates across the K

64
00:03:55.158 --> 00:03:57.132
random samples in the mini batch.

65
00:03:57.132 --> 00:04:01.367
Here's an example of computing this
average update for q-learning.

66
00:04:01.367 --> 00:04:06.384
Restore the average update in a variable,
let's call it U bar, then we grab several

67
00:04:06.384 --> 00:04:10.995
samples from the buffer and compute
the q-learning update for each sample.

68
00:04:10.995 --> 00:04:14.300
Finally, we average those updates
to get the mini batch update.

69
00:04:15.900 --> 00:04:18.600
Putting it all together,
we get the following pseudocode for

70
00:04:18.600 --> 00:04:20.508
experience replay.

71
00:04:20.508 --> 00:04:24.590
As in Dyna, we can think of the planning
steps occurring in the background.

72
00:04:24.590 --> 00:04:27.977
The agent can perform several
of these planning updates,

73
00:04:27.977 --> 00:04:31.930
which are the replay updates per
each real step in the environment.

74
00:04:31.930 --> 00:04:36.028
More updates means we extract more
from the data we have observed.

75
00:04:36.028 --> 00:04:40.246
Each of these replay updates consists of
a mini batch update to reduce the noise of

76
00:04:40.246 --> 00:04:41.025
that update.

77
00:04:41.025 --> 00:04:42.715
>> [SOUND]
>> And that's it.

78
00:04:42.715 --> 00:04:47.099
Experience replay allows the agent to
make better use of its data and so

79
00:04:47.099 --> 00:04:48.818
be more sample efficient.

80
00:04:48.818 --> 00:04:53.356
Further, it makes sure that the agent is
updating in many parts of the environment

81
00:04:53.356 --> 00:04:54.421
just like in Dyna.

82
00:04:54.421 --> 00:04:57.800
This week, you will see how to use
experience replay within your agent.