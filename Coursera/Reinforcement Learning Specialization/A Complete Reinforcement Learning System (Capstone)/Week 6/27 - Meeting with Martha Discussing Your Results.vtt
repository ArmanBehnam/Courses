WEBVTT

1
00:00:00.000 --> 00:00:05.437
[MUSIC]

2
00:00:05.437 --> 00:00:07.948
Congratulations, you finished
your capstone project.

3
00:00:07.948 --> 00:00:12.856
By successfully completing each of the
steps in the capstone you should have been

4
00:00:12.856 --> 00:00:16.600
able to train an agent to
land a module on the moon.

5
00:00:16.600 --> 00:00:19.400
And you should have some insight
into how sensitive your agent is to

6
00:00:19.400 --> 00:00:21.600
one of its meta parameters.

7
00:00:21.600 --> 00:00:24.800
You already know a lot about
the details of your solution and

8
00:00:24.800 --> 00:00:28.400
have seen learning curves and
parameter sensitivity plots.

9
00:00:28.400 --> 00:00:32.254
So in this video we're instead going
to get some insight into the agent

10
00:00:32.254 --> 00:00:33.312
during learning.

11
00:00:33.312 --> 00:00:37.106
We'll visualize the agent in one run
to get a sense of how an agent might

12
00:00:37.106 --> 00:00:39.100
learn on this problem.

13
00:00:39.100 --> 00:00:42.921
And of course, because it's fun
to actually see our agent learn.

14
00:00:42.921 --> 00:00:46.000
This is a video of our
agent after some training.

15
00:00:46.000 --> 00:00:49.820
You'll notice that the policy still
isn't perfect, but it is quite good.

16
00:00:49.820 --> 00:00:52.400
But the agent definitely
did not start there.

17
00:00:52.400 --> 00:00:55.369
Remember, it did not know
anything about this world and

18
00:00:55.369 --> 00:00:57.082
had to learn by trial and error.

19
00:00:57.082 --> 00:01:01.200
Early on in training the agent
behaves in clearly suboptimal ways.

20
00:01:01.200 --> 00:01:05.800
For instance, perhaps the agent only
decided to turn on one side thruster for

21
00:01:05.800 --> 00:01:07.300
the entire episode.

22
00:01:07.300 --> 00:01:09.546
That obviously didn't work out.

23
00:01:09.546 --> 00:01:12.916
After that sometimes the agent learned
to only use the bottom thruster.

24
00:01:12.916 --> 00:01:15.830
However, this pushed
the agent far off screen,

25
00:01:15.830 --> 00:01:18.467
making it receive a large negative reward.

26
00:01:18.467 --> 00:01:22.822
If the agent can use the main thruster all
the time, and it can use a side thruster

27
00:01:22.822 --> 00:01:26.800
all the time, obviously the correct
choice is to just do nothing at all.

28
00:01:27.800 --> 00:01:31.800
We can try to hypothesize some
reasons why this might be happening.

29
00:01:31.800 --> 00:01:35.700
One possibility is that the neural
network is producing poor representations

30
00:01:35.700 --> 00:01:36.300
early on.

31
00:01:37.300 --> 00:01:39.897
For example,
the agent might be overgeneralizing.

32
00:01:39.897 --> 00:01:44.323
If the side thruster action led to high
negative reward in one state, the agent

33
00:01:44.323 --> 00:01:49.400
may over generalize that the site thruster
action is terrible in all states.

34
00:01:49.400 --> 00:01:51.127
As the representation improves,

35
00:01:51.127 --> 00:01:54.052
the agent is better able to
discriminate between states.

36
00:01:54.052 --> 00:01:56.400
Of course, this is just a hypothesis.

37
00:01:56.400 --> 00:02:00.527
Isolating the real cause will
require some experimentation.

38
00:02:00.527 --> 00:02:04.400
After many more episodes of learning,
the agent is performing much better.

39
00:02:04.400 --> 00:02:08.960
The agent is falling just a little too
quickly, causing one of the corners to hit

40
00:02:08.960 --> 00:02:13.125
the ground, but fine-tuning its
policy from here is relatively easy.

41
00:02:13.125 --> 00:02:14.900
And that's it for this video.

42
00:02:14.900 --> 00:02:17.800
It is fun to watch videos
of your agent landing, but

43
00:02:17.800 --> 00:02:21.315
it can also be a useful way to debug and
understand your agent.

44
00:02:21.315 --> 00:02:25.491
Whenever is possible it's worth taking the
time to do this visualization to gain more

45
00:02:25.491 --> 00:02:27.000
understanding of your agent.