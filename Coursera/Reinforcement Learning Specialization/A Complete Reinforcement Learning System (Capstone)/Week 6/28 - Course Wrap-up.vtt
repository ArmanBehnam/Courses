WEBVTT

1
00:00:00.000 --> 00:00:05.340
[MUSIC]

2
00:00:05.340 --> 00:00:11.967
Congratulations, you have completed course
for and built a complete RL system.

3
00:00:11.967 --> 00:00:14.938
Let's quickly review the system you built.

4
00:00:14.938 --> 00:00:18.656
Throughout the Capstone we've been
incrementally adding tools for

5
00:00:18.656 --> 00:00:20.585
solving the lunar lander problem.

6
00:00:20.585 --> 00:00:25.877
We started by formalizing the problem as
an MVP and implementing the environment.

7
00:00:25.877 --> 00:00:30.341
This mainly involve precisely
specifying a reward function

8
00:00:30.341 --> 00:00:33.323
to get the agent to appropriately land.

9
00:00:33.323 --> 00:00:38.006
We then discussed how to design
an agent to learn a good policy in this

10
00:00:38.006 --> 00:00:39.085
environment.

11
00:00:39.085 --> 00:00:42.363
The first step was to figure out which
reinforced learning algorithms were

12
00:00:42.363 --> 00:00:44.348
appropriate for the given problem setting.

13
00:00:46.583 --> 00:00:51.510
We followed the course map that we
built in course 3 to narrow it down to

14
00:00:51.510 --> 00:00:56.117
three algorithms, expected SARSA,
SARSA, and Q-learning.

15
00:00:56.117 --> 00:01:00.711
Next, we investigated have the choice
of meta parameters influence the agent

16
00:01:00.711 --> 00:01:01.342
success.

17
00:01:01.342 --> 00:01:03.733
All the specific implementation details,

18
00:01:03.733 --> 00:01:07.481
including those about function
approximation, exploration, and

19
00:01:07.481 --> 00:01:10.268
optimization can be
considered meta parameters.

20
00:01:10.268 --> 00:01:15.416
How we specify these can have a big
impact on the performance of our agent.

21
00:01:15.416 --> 00:01:19.168
We discussed several of these choices and
common rules of thumb.

22
00:01:19.168 --> 00:01:22.917
For example, we decided on the size and
activation functions for

23
00:01:22.917 --> 00:01:25.860
the neural network based
on conventional wisdom.

24
00:01:25.860 --> 00:01:31.324
However, some meta parameters like
the step size can be difficult to select.

25
00:01:31.324 --> 00:01:36.564
This led to the final part of the project,
a parameter study of the step size.

26
00:01:36.564 --> 00:01:42.022
Understanding our algorithms and systems,
is critical for successful deployment.

27
00:01:42.022 --> 00:01:46.212
Finally, you got to see
your agent in action.

28
00:01:46.212 --> 00:01:50.532
We hope that you've learned a bit about
the nuances of applying reinforce learning

29
00:01:50.532 --> 00:01:51.713
methods in practice.

30
00:01:51.713 --> 00:01:56.097
There are a lot of choices to make and
it requires quite a lot of expertise and

31
00:01:56.097 --> 00:01:57.777
experience to get it right.

32
00:01:57.777 --> 00:02:00.919
Our goal was to give you some
of that hands-on experience.

33
00:02:00.919 --> 00:02:04.856
Hopefully, this will help you in
your future endeavors applying RL in

34
00:02:04.856 --> 00:02:05.800
the real world.