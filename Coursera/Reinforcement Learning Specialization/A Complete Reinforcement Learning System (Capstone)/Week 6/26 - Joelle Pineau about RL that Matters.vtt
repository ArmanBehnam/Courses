WEBVTT

1
00:00:00.000 --> 00:00:06.120
[MUSIC]

2
00:00:06.120 --> 00:00:08.296
Hello, my name is Joelle Pineau, and

3
00:00:08.296 --> 00:00:13.100
I'm a faculty member at the School of
Computer Science at McGill University.

4
00:00:13.100 --> 00:00:18.973
And I'm also a research manager at
the Facebook AI research lab in Montreal.

5
00:00:18.973 --> 00:00:23.707
It's my pleasure to be here today and tell
you a little bit about some of the work

6
00:00:23.707 --> 00:00:28.302
we've been doing on the issues of
building systems that are reproducible,

7
00:00:28.302 --> 00:00:32.580
reusable, and robust, in particular for
reinforcement learning.

8
00:00:32.580 --> 00:00:36.639
And so it may be interesting to you
to know why do I tie these three R's

9
00:00:36.639 --> 00:00:39.073
together, what do they have in common?

10
00:00:39.073 --> 00:00:43.777
And let me draw from this quote from
the National Science Foundation

11
00:00:43.777 --> 00:00:48.399
that says that reproducibility
actually refers to the ability for

12
00:00:48.399 --> 00:00:53.021
researcher to duplicate the results
of a prior study using the same

13
00:00:53.021 --> 00:00:57.005
materials that were used by
the original investigator.

14
00:00:57.005 --> 00:01:00.733
And reproducibility is also
the minimum necessary condition for

15
00:01:00.733 --> 00:01:03.249
finding to be believable and informative.

16
00:01:03.249 --> 00:01:04.792
And so as scientists,

17
00:01:04.792 --> 00:01:10.325
all of these components are actually very
important to how we practice science.

18
00:01:10.325 --> 00:01:16.799
But it's also very important terms of
how we share the findings from our work.

19
00:01:16.799 --> 00:01:21.078
There's been talks of this reproducibility
crisis in science broadly,

20
00:01:21.078 --> 00:01:22.881
not just in machine learning.

21
00:01:22.881 --> 00:01:26.856
And so the journal Nature ran
a very interesting study in 2016,

22
00:01:26.856 --> 00:01:30.624
surveying 1,500 or so
scientists asking them whether they

23
00:01:30.624 --> 00:01:34.500
thought there was a reproducibility
crisis in their field.

24
00:01:34.500 --> 00:01:37.640
And a large number of them said, yes,

25
00:01:37.640 --> 00:01:42.657
there's either slight crisis or
a serious crisis going on.

26
00:01:42.657 --> 00:01:45.411
Now, in some of our work,
we've been trying to figure out,

27
00:01:45.411 --> 00:01:47.949
it's one thing to be alarmist and
answer some surveys.

28
00:01:47.949 --> 00:01:51.346
But then it's also another
thing to look carefully at what

29
00:01:51.346 --> 00:01:54.548
the data tells us when we
conduct our own comparisons.

30
00:01:54.548 --> 00:01:58.862
And in the case of reinforcement learning,
it's been very impressive the rate of

31
00:01:58.862 --> 00:02:02.400
progress we've seen in the last
couple of years, really.

32
00:02:02.400 --> 00:02:07.100
Systems demonstrating the ability to play,
in many cases,

33
00:02:07.100 --> 00:02:11.902
play games at a performance
level exceeding the best humans.

34
00:02:11.902 --> 00:02:14.914
And where we thought some of these
games were really difficult,

35
00:02:14.914 --> 00:02:18.390
might be decades away from solving,
we've had some impressive result.

36
00:02:18.390 --> 00:02:23.307
In particular on the game of Go, but
also on some challenging video games

37
00:02:23.307 --> 00:02:28.320
where RL-trained system are able to
achieve these really good results.

38
00:02:28.320 --> 00:02:32.436
Doing well in games and
simulation is wonderful,

39
00:02:32.436 --> 00:02:37.148
it really makes us see the potential for
the technology.

40
00:02:37.148 --> 00:02:42.355
But what we care about in the long term
is, really, how well can this technology,

41
00:02:42.355 --> 00:02:45.840
or algorithms,
help us solve real-world problems.

42
00:02:45.840 --> 00:02:49.818
And so in an attempt to understand
how far the science has come and

43
00:02:49.818 --> 00:02:54.312
how close we are to solving these
hard difficult real-world problems,

44
00:02:54.312 --> 00:02:58.305
we started digging in the literature,
the recent literature.

45
00:02:58.305 --> 00:03:01.892
And we looked at the last 25 years or
so of papers in RL.

46
00:03:01.892 --> 00:03:05.490
So you see this curve,
it's actually growing very rapidly.

47
00:03:05.490 --> 00:03:10.183
Early 2000s, just a few hundred papers
on the topic published every year,

48
00:03:10.183 --> 00:03:14.960
more recently, upwards of 20,000
papers being published on this topic.

49
00:03:14.960 --> 00:03:19.274
And so that's a lot of new
findings coming out every year.

50
00:03:19.274 --> 00:03:23.233
And when we're asking, what of all
of these methods actually works

51
00:03:23.233 --> 00:03:28.171
in the real-world, it makes it difficult
to sift through all of that information.

52
00:03:28.171 --> 00:03:33.813
And so going back to the core topic for
today, it becomes very important

53
00:03:33.813 --> 00:03:39.363
that the way the findings are reported
in the papers match our criteria

54
00:03:39.363 --> 00:03:44.180
in terms of reliability,
robustness, reproducibility.

55
00:03:44.180 --> 00:03:47.887
And so to check this out,
we conducted our own little investigation.

56
00:03:47.887 --> 00:03:51.820
And we picked four papers out of
the whole bunch of them, we picked four.

57
00:03:51.820 --> 00:03:55.242
In particular four that were
very well cited, commonly used,

58
00:03:55.242 --> 00:03:56.738
there was code available.

59
00:03:56.738 --> 00:04:02.177
And we started looking at how robust
were the findings in those papers.

60
00:04:02.177 --> 00:04:06.861
And the message today that I'm going
to deliver actually goes beyond those

61
00:04:06.861 --> 00:04:11.933
papers and is really looking at how we
practice empirical science in this field.

62
00:04:11.933 --> 00:04:15.169
And so we started comparing some
of these four different methods,

63
00:04:15.169 --> 00:04:19.117
these are policy gradient algorithms,
different flavors of policy gradient.

64
00:04:19.117 --> 00:04:23.894
On this training of a very simple
agent which learns how to walk,

65
00:04:23.894 --> 00:04:26.372
we held the HalfCheetah domain so

66
00:04:26.372 --> 00:04:30.983
that the cheetah cartoon's split in half,
so only two legs.

67
00:04:30.983 --> 00:04:33.988
It has to learn to run
without falling on its belly.

68
00:04:33.988 --> 00:04:35.851
And we compare four different algorithms.

69
00:04:35.851 --> 00:04:38.450
And on the left,
you see a plot with those four algorithms.

70
00:04:38.450 --> 00:04:41.311
And on the basis of this,
you would probably think that the red

71
00:04:41.311 --> 00:04:43.799
algorithm is doing quite
a bit better than the other.

72
00:04:43.799 --> 00:04:47.128
Doesn't really matter which of
the algorithms is which on this,

73
00:04:47.128 --> 00:04:50.214
really, our message is about
how do we do a fair comparison,

74
00:04:50.214 --> 00:04:52.527
draw conclusions that
are robust from this.

75
00:04:52.527 --> 00:04:56.961
So based on this one, it seems
this red algorithm's doing better.

76
00:04:56.961 --> 00:05:01.881
Now, we extended this to slightly
different variations of controllers.

77
00:05:01.881 --> 00:05:05.616
There's a another example which is
a hopper and another one that's called

78
00:05:05.616 --> 00:05:09.838
a swimmer, slightly different versions of
this character that we have to control.

79
00:05:09.838 --> 00:05:12.064
And when you look at the bottom two plots,

80
00:05:12.064 --> 00:05:16.014
what you see is actually the blue
algorithm seems to be doing a lot better.

81
00:05:16.014 --> 00:05:17.526
And in the left bottom side,

82
00:05:17.526 --> 00:05:21.878
the blue algorithm's doing quite well
with a really tight confidence interval.

83
00:05:21.878 --> 00:05:24.976
Whereas on the right side,
the blue algorithm's doing well, but

84
00:05:24.976 --> 00:05:26.925
with a really wide confidence interval.

85
00:05:26.925 --> 00:05:29.255
And so
taking all of these results together,

86
00:05:29.255 --> 00:05:33.242
you would think if you try the performance
of each algorithm on more domains,

87
00:05:33.242 --> 00:05:36.247
you'd have a better understanding
of how well it works and

88
00:05:36.247 --> 00:05:39.832
better prediction of whether it
might be useful in the real-world.

89
00:05:39.832 --> 00:05:44.397
And what we found here, we just got more
and more confused with each of these

90
00:05:44.397 --> 00:05:49.337
experiments because the results just were
not consistent from one to the other.

91
00:05:49.337 --> 00:05:53.241
We started digging in a little bit more
about what might be going on behind

92
00:05:53.241 --> 00:05:53.950
the scenes.

93
00:05:53.950 --> 00:05:57.170
We thought, well, we didn't implement
these algorithms on our own.

94
00:05:57.170 --> 00:06:01.555
Of course, we used some available
open source code to do that.

95
00:06:01.555 --> 00:06:04.615
So we thought maybe we didn't
get the right source for that,

96
00:06:04.615 --> 00:06:07.202
maybe we got some poor
implementation somewhere.

97
00:06:07.202 --> 00:06:09.958
We started digging out
different implementation.

98
00:06:09.958 --> 00:06:14.512
And that what I'm showing you in the plot
is three different implementations

99
00:06:14.512 --> 00:06:18.522
publicly available from one of
these four algorithms called TRPO.

100
00:06:18.522 --> 00:06:22.085
And what you see here is one of
these versions, a blue version,

101
00:06:22.085 --> 00:06:24.179
does a lot better than the other two.

102
00:06:24.179 --> 00:06:29.189
We thought maybe TRPO is a policy gradient
method that's a little bit unstable,

103
00:06:29.189 --> 00:06:30.896
code is hard to implement.

104
00:06:30.896 --> 00:06:32.885
We looked at some other cases.

105
00:06:32.885 --> 00:06:36.893
Very similar results,
you go from one code base to the other.

106
00:06:36.893 --> 00:06:41.118
It's purporting to be the same algorithm,
it should work the same, right?

107
00:06:41.118 --> 00:06:48.084
And yet, we're seeing really significant
difference in the empirical performance.

108
00:06:48.084 --> 00:06:52.082
As we were asking more and
more questions and diving deep into what

109
00:06:52.082 --> 00:06:56.877
might explain those differences, one
thing we noticed is that in many cases,

110
00:06:56.877 --> 00:07:01.401
there's a lot of different hyperparameters
that have to be configured.

111
00:07:01.401 --> 00:07:05.543
And depending how you set those,
you get very different results.

112
00:07:05.543 --> 00:07:10.396
Most of the code bases come with
a default set of hyperparameters, and

113
00:07:10.396 --> 00:07:14.920
it's very tempting to take that
default set, not change it, and

114
00:07:14.920 --> 00:07:16.749
use it as is out of the box.

115
00:07:16.749 --> 00:07:20.428
And what we found is once you
start manipulating these different

116
00:07:20.428 --> 00:07:24.732
hyperparameters, it really changes
the performance of your algorithm.

117
00:07:24.732 --> 00:07:28.971
So that's one lesson to take home,
really be very careful about what

118
00:07:28.971 --> 00:07:33.357
are the hyperparameters, how they're set,
and how that might affect

119
00:07:33.357 --> 00:07:37.401
the performance of the algorithm
once you run it on a new problem.

120
00:07:37.401 --> 00:07:40.681
You might be tempted to say,
we're comparing different algorithms.

121
00:07:40.681 --> 00:07:45.240
The most fair way to do those comparisons
is to give them the same amount of data,

122
00:07:45.240 --> 00:07:48.318
same amount of computation,
everything's right.

123
00:07:48.318 --> 00:07:50.980
But from our analysis, as I've mentioned,

124
00:07:50.980 --> 00:07:54.621
what we've really found is that
to draw robust conclusions,

125
00:07:54.621 --> 00:07:58.562
it's important to think a little
bit more deeply about the issue.

126
00:07:58.562 --> 00:08:03.380
And particularly acknowledge the fact
that different algorithms are going to

127
00:08:03.380 --> 00:08:05.571
have different hyperparameters.

128
00:08:05.571 --> 00:08:08.858
You can't just use the same
from one to the other.

129
00:08:08.858 --> 00:08:11.514
The different methods are going to
have different sensitivity to

130
00:08:11.514 --> 00:08:12.623
those hyperparameters.

131
00:08:12.623 --> 00:08:17.441
You might need to search longer and more
carefully about some of them than others.

132
00:08:17.441 --> 00:08:22.350
And which method is best often
depends on how much data you have and

133
00:08:22.350 --> 00:08:25.216
how much computation you can afford.

134
00:08:25.216 --> 00:08:29.522
And so that's going to change drastically
from one application to the other,

135
00:08:29.522 --> 00:08:32.011
depending where your
data is coming from and

136
00:08:32.011 --> 00:08:35.725
on what kind of platform you're
trying to deploy your AI system.

137
00:08:35.725 --> 00:08:40.176
Taking all this together,
stepping back to the main question.

138
00:08:40.176 --> 00:08:45.335
I think my main message for you today is
that we have to be very careful about

139
00:08:45.335 --> 00:08:50.414
how we interpret the information that
we read in papers, be very careful

140
00:08:50.414 --> 00:08:55.519
about how we set up our experiments,
how we compare different methods.

141
00:08:55.519 --> 00:08:59.852
And when we do that,
we can draw much more robust conclusions.

142
00:08:59.852 --> 00:09:02.860
And really,
it goes back to the foundation of science.

143
00:09:02.860 --> 00:09:06.576
Science is an activity,
a collective activity in which we engage,

144
00:09:06.576 --> 00:09:10.762
and the purpose of it is, really,
to understand, to explain phenomenon.

145
00:09:10.762 --> 00:09:11.752
And to do that,

146
00:09:11.752 --> 00:09:16.492
you always have to be very careful
about the way in which you're doing it.

147
00:09:16.492 --> 00:09:16.992
Thank you very much.