WEBVTT

1
00:00:05.400 --> 00:00:10.200
Congratulations, you've completed
the reinforcer learning specialization.

2
00:00:10.200 --> 00:00:14.600
>> We've come a long way since the first
course where you learned about the basic

3
00:00:14.600 --> 00:00:18.800
ideas of value functions, policies and
the sequential decision making problem.

4
00:00:19.800 --> 00:00:23.831
>> The easiest way to remember all that
you've learned is to look again at our

5
00:00:23.831 --> 00:00:24.555
course map.

6
00:00:24.555 --> 00:00:28.403
We started in the tabular setting to
allow you to get comfortable with many

7
00:00:28.403 --> 00:00:32.687
of the key concepts in RL without getting
bogged down by the additional issues that

8
00:00:32.687 --> 00:00:35.000
arise when using function approximation.

9
00:00:36.700 --> 00:00:39.700
>> We started by assuming
you have a perfect model and

10
00:00:39.700 --> 00:00:42.900
focus on the key ideas and
prediction and control.

11
00:00:42.900 --> 00:00:45.617
We talked about dynamic
programming methods and

12
00:00:45.617 --> 00:00:47.610
how they solve Bellman equations.

13
00:00:47.610 --> 00:00:51.277
>> We then moved to sample base methods
where the agent is no longer given

14
00:00:51.277 --> 00:00:51.921
the model.

15
00:00:51.921 --> 00:00:55.500
It mainly learns from trial and
error interaction with the world.

16
00:00:55.500 --> 00:00:59.100
We started by considering algorithms
where the agent does not learn a model,

17
00:00:59.100 --> 00:01:03.183
including both Monte Carlo algorithms and
temporal difference learning algorithms.

18
00:01:03.183 --> 00:01:05.400
They use bootstrapping and
so can learn and

19
00:01:05.400 --> 00:01:08.100
update their policy on every time step.

20
00:01:08.100 --> 00:01:10.800
These included Q-Learning,
SARSA, and Expected SARSA for

21
00:01:10.800 --> 00:01:12.900
control and TD for prediction.

22
00:01:14.000 --> 00:01:18.722
>> We then revisited all these concepts
using function approximation to

23
00:01:18.722 --> 00:01:21.531
learn approximate values and policies.

24
00:01:21.531 --> 00:01:23.542
We first talked about how Monte Carlo and

25
00:01:23.542 --> 00:01:26.395
TD algorithms extend to using
function approximation.

26
00:01:26.395 --> 00:01:28.315
This required defining a loss function and

27
00:01:28.315 --> 00:01:31.100
understanding gradient based
approaches to optimization.

28
00:01:32.300 --> 00:01:37.449
We then discuss to new ideas, average
reward and policy gradient methods.

29
00:01:37.449 --> 00:01:41.382
We learned about using average award as a
new way to formulate the agent's goal and

30
00:01:41.382 --> 00:01:42.800
continuing test.

31
00:01:42.800 --> 00:01:47.519
We also learned how policy gradient
methods can learn a policy directly by

32
00:01:47.519 --> 00:01:50.269
optimizing the average award objective.

33
00:01:50.269 --> 00:01:53.700
>> Our goal in the specialization was
to give you foundational knowledge.

34
00:01:53.700 --> 00:01:57.144
Understanding the fundamentals
is essential to using reinforcer

35
00:01:57.144 --> 00:01:58.565
learning appropriately.

36
00:01:58.565 --> 00:02:02.774
You hopefully now better understand when
RL is well suited to a particular problem

37
00:02:02.774 --> 00:02:06.000
and how to formulate your
problem as an RL problem.

38
00:02:06.000 --> 00:02:08.943
This seeming simple thing
represents a big step.

39
00:02:08.943 --> 00:02:13.827
Too often, people think, how can I apply
the state-of-the-art in RL to my problem,

40
00:02:13.827 --> 00:02:16.731
when either a very simple
approach is appropriate or

41
00:02:16.731 --> 00:02:21.000
the problem wasn't even well posed
as an RL problem in the first place.

42
00:02:21.000 --> 00:02:22.869
We want you to know better.

43
00:02:22.869 --> 00:02:26.211
Hopefully you have learned something
about being methodical and

44
00:02:26.211 --> 00:02:27.500
scientific in using RL.

45
00:02:27.500 --> 00:02:30.300
You now know how important it
is to formalize your mdp or

46
00:02:30.300 --> 00:02:34.000
Bandit, identify appropriate
algorithms and carefully evaluate and

47
00:02:34.000 --> 00:02:36.700
understand the performance of
your autonomous Learning System.

48
00:02:38.300 --> 00:02:41.700
>> Knowing the fundamentals is
the first step towards learning more

49
00:02:41.700 --> 00:02:44.100
about advanced topics in RL.

50
00:02:44.100 --> 00:02:47.400
What you have learned here will
also help demystify algorithms

51
00:02:47.400 --> 00:02:50.500
you might read about in blog posts or
scientific papers.

52
00:02:50.500 --> 00:02:53.100
Once it seemed complex at first glance.

53
00:02:53.100 --> 00:02:56.400
You should now be able to
see how state-of-the-art RL

54
00:02:56.400 --> 00:03:00.200
systems are actually built on
relatively straightforward ideas.

55
00:03:00.200 --> 00:03:03.600
Never be impressed by things
you don't understand.

56
00:03:03.600 --> 00:03:06.900
>> My goal is to help more people
understand and effectively use RL.

57
00:03:06.900 --> 00:03:10.100
I'm looking forward to seeing the things
you accomplish with your new skills.

58
00:03:11.600 --> 00:03:13.936
>> We have really enjoyed
our time with you.

59
00:03:13.936 --> 00:03:17.700
It is so exciting to learn about
a new topic for the first time.

60
00:03:17.700 --> 00:03:22.900
We also strongly believe that RL and
AI will change the world for the better.

61
00:03:22.900 --> 00:03:25.800
You are now ready to play your
part in that better world.