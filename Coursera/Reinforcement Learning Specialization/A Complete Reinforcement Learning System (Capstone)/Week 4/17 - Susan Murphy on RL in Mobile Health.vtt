WEBVTT

1
00:00:05.043 --> 00:00:07.526
[MUSIC]

2
00:00:07.526 --> 00:00:08.191
Hi, everybody.

3
00:00:08.191 --> 00:00:09.900
I'm Susan Murphy.

4
00:00:09.900 --> 00:00:11.412
I'm at Harvard University,.

5
00:00:11.412 --> 00:00:16.100
I'm joined in statistics and
in computer science.

6
00:00:16.100 --> 00:00:21.200
And what I do is, I work in
mobile health and, in particular,

7
00:00:21.200 --> 00:00:25.300
I work on how we can use reinforcement
learning in mobile health.

8
00:00:25.300 --> 00:00:27.000
And on the slide, in front of you,

9
00:00:27.000 --> 00:00:32.500
you can see all the different kinds of
studies that we've been involved in.

10
00:00:33.800 --> 00:00:35.535
Many of the studies involve addictions.

11
00:00:35.535 --> 00:00:39.814
Lot of them involve adolescents,
and so on.

12
00:00:39.814 --> 00:00:41.323
So what will help?

13
00:00:41.323 --> 00:00:42.423
What do we want to do?

14
00:00:42.423 --> 00:00:47.500
What we want to do is, we want to help
people who are struggling with a problem.

15
00:00:47.500 --> 00:00:52.600
Often, these are chronic illnesses like
someone who's struggled with addiction and

16
00:00:52.600 --> 00:00:55.400
how are they going to stay clean.

17
00:00:55.400 --> 00:00:58.600
Might be someone who's struggling
with depression, so on.

18
00:00:58.600 --> 00:01:01.900
So long term support to someone.

19
00:01:01.900 --> 00:01:05.367
We also want to be able
to test causal inference.

20
00:01:05.367 --> 00:01:11.600
So we really want to understand, is this
treatment an action in the case of RL?

21
00:01:11.600 --> 00:01:13.925
Does it really impact that person or not?

22
00:01:13.925 --> 00:01:17.523
>> [COUGH]
>> Also, in Mobile Health we think

23
00:01:17.523 --> 00:01:22.363
about how we can continually learn,
we can continually experiment, so

24
00:01:22.363 --> 00:01:27.800
that we can always improve that the mobile
health application for each person.

25
00:01:27.800 --> 00:01:29.900
That's what this slide is talking about.

26
00:01:31.200 --> 00:01:32.663
So what's the basic setup?

27
00:01:32.663 --> 00:01:34.696
So for any given user,

28
00:01:34.696 --> 00:01:40.800
they can receive a whole variety
of interventions that is actions.

29
00:01:40.800 --> 00:01:42.400
I listed some on the slide.

30
00:01:42.400 --> 00:01:45.900
You see reminders,
maybe reminders to take your medication,

31
00:01:45.900 --> 00:01:49.700
reminders to practice certain
types of exercise, and so on.

32
00:01:50.800 --> 00:01:54.200
Suggestions about what
you can do in your life.

33
00:01:54.200 --> 00:01:57.000
How you can link you up with
someone else has been through

34
00:01:57.000 --> 00:01:58.400
similar problems, and so on.

35
00:01:58.400 --> 00:02:01.500
And this can happen repeatedly over time.

36
00:02:01.500 --> 00:02:05.400
After each time we deliver
a different type of

37
00:02:07.200 --> 00:02:10.400
treatment action, we observe a reward.

38
00:02:10.400 --> 00:02:12.700
So the reward could be, for example,

39
00:02:12.700 --> 00:02:17.500
total step count over the next 30 minutes
if I'm trying to help you be more active.

40
00:02:17.500 --> 00:02:21.300
Could be how long it is until you become

41
00:02:21.300 --> 00:02:26.400
stressed again if I'm trying to that was
trying to help you manage your stress.

42
00:02:26.400 --> 00:02:30.400
On the next slide, I'm going to show
you a study that just finished.

43
00:02:30.400 --> 00:02:35.500
And this study,
our goal was to help people be

44
00:02:35.500 --> 00:02:41.400
more active in answering questions
about their stress and loneliness.

45
00:02:41.400 --> 00:02:45.949
And this study recruited young adults,
adolescents and

46
00:02:45.949 --> 00:02:52.119
college students aged, not always
college students, who were in trouble.

47
00:02:52.119 --> 00:02:56.800
They were recruited at our emergency room
because they had been drinking too much.

48
00:02:56.800 --> 00:03:00.300
And the idea was,
if we can learn about what

49
00:03:01.400 --> 00:03:06.000
their feelings of loneliness are,
how depressed they are.

50
00:03:06.000 --> 00:03:09.300
Then we can develop an app
which helps them manage that.

51
00:03:09.300 --> 00:03:14.400
So all the actions in this
RL problem have to do with,

52
00:03:14.400 --> 00:03:19.300
how do we give you reinforcements so
that you will stay engaged and

53
00:03:19.300 --> 00:03:22.100
answer our questions?

54
00:03:22.100 --> 00:03:25.300
You can see we had a gamified interface.

55
00:03:25.300 --> 00:03:26.600
It was an aquarium.

56
00:03:26.600 --> 00:03:30.600
And as you answer questions,
you built your aquarium became more and

57
00:03:30.600 --> 00:03:36.000
more rich and you also could level up to
higher levels in the aquarium environment.

58
00:03:36.000 --> 00:03:41.678
But I was mostly interested in,
how do we provide rewards to you?

59
00:03:41.678 --> 00:03:46.187
If you answer questions,
that's the top line in this diagram,

60
00:03:46.187 --> 00:03:50.865
what kinds of rewards means gifts,
will help you be more engaged so

61
00:03:50.865 --> 00:03:54.000
you'll answer our questions tomorrow.

62
00:03:54.000 --> 00:03:58.900
I was also interested in,
if we give you a gift, a present,

63
00:03:58.900 --> 00:04:03.000
today, are you more likely to
answer our questions this evening?

64
00:04:03.000 --> 00:04:08.691
So the whole goal of this study
is to find good policies for

65
00:04:08.691 --> 00:04:15.586
how we keep people engaged in
answering questions about their life.

66
00:04:15.586 --> 00:04:16.899
So what are we doing now?

67
00:04:16.899 --> 00:04:21.899
We're using these kinds
of studies to initialize

68
00:04:21.899 --> 00:04:26.600
reinforcement learning
algorithms on in mobile health.

69
00:04:26.600 --> 00:04:31.100
And the idea here, is that the
reinforcement learning algorithm should

70
00:04:31.100 --> 00:04:37.300
personalize the app to each user from
from our perspective years in mind.

71
00:04:37.300 --> 00:04:41.900
This is the same as learning
an optimal policy for each user.

72
00:04:41.900 --> 00:04:46.900
In my lab, we mainly use
Bayesian Reinforcement learning.

73
00:04:46.900 --> 00:04:50.000
I don't think you've learned about
Bayesian Reinforcement learning.

74
00:04:50.000 --> 00:04:52.300
But the main tool for

75
00:04:52.300 --> 00:04:56.800
us, one of the main advantages of
Bayesian Reinforcement learning,

76
00:04:56.800 --> 00:05:01.300
is the way it updates information
on an individual user over time.

77
00:05:01.300 --> 00:05:07.700
And also, it selects actions that each
time in a random way stochastically.

78
00:05:07.700 --> 00:05:13.900
And this helps us learn from the data
after the study is over as well.

79
00:05:15.600 --> 00:05:19.400
So what are the challenges to
reinforcement learning when you try to use

80
00:05:19.400 --> 00:05:22.200
it in my setting, that is,
in Mobile Health?

81
00:05:22.200 --> 00:05:25.300
Well, one of the issues is,
if you've ever downloaded an app,

82
00:05:25.300 --> 00:05:27.900
you know exactly what I'm talking about.

83
00:05:27.900 --> 00:05:32.500
It could get really aggravating
when that app notifies you a lot.

84
00:05:32.500 --> 00:05:37.800
So normally, apps have a lot of
potential to help you in the moment.

85
00:05:37.800 --> 00:05:43.300
So positive effects on the immediate
rewards, but negative delayed effects.

86
00:05:43.300 --> 00:05:48.857
This is the classical setting for
Reinforcement Learning.

87
00:05:48.857 --> 00:05:52.344
Some of the things we have
to figure out how to do,

88
00:05:52.344 --> 00:05:57.279
are how to approximate the value
function so that we can learn fast yet

89
00:05:57.279 --> 00:06:00.200
not make mistakes in choosing actions.

90
00:06:00.200 --> 00:06:03.800
If we always have many users,
they're all very different.

91
00:06:03.800 --> 00:06:05.496
Each person is different.

92
00:06:05.496 --> 00:06:09.070
But how can we use one user's
data to personalize, or

93
00:06:09.070 --> 00:06:13.129
find, or determine a good policy for
each particular user?

94
00:06:13.129 --> 00:06:17.204
This is a big challenge
we're working on now.

95
00:06:17.204 --> 00:06:21.432
Also, after a study is over, we always
want to conduct causal inference.

96
00:06:21.432 --> 00:06:25.881
That is, we want to help
scientists learn more about

97
00:06:25.881 --> 00:06:29.200
why people enact certain behaviors.

98
00:06:29.200 --> 00:06:31.718
Why do people relapse to drug use?

99
00:06:31.718 --> 00:06:34.347
Why did they relapse to smoking?

100
00:06:34.347 --> 00:06:37.233
These are causal inference questions.

101
00:06:37.233 --> 00:06:40.296
How can the treatment help them so
that they don't relapse?

102
00:06:40.296 --> 00:06:42.204
So we want to use this data to do this.

103
00:06:42.204 --> 00:06:45.683
And even though the data collected be
a reinforcement learning algorithm,

104
00:06:45.683 --> 00:06:47.107
we want to use the data for that.

105
00:06:47.107 --> 00:06:48.766
We have to figure out how.

106
00:06:48.766 --> 00:06:53.724
And then, how can we use data from
the current study for future users so

107
00:06:53.724 --> 00:06:59.036
that they will be able to benefit from
what we learn from the current study?

108
00:06:59.036 --> 00:07:02.480
Okay, I want to show you,
this is my last slide and

109
00:07:02.480 --> 00:07:04.751
I want to show you one more study.

110
00:07:04.751 --> 00:07:06.172
This study is pretty cool.

111
00:07:06.172 --> 00:07:08.151
It's almost over.

112
00:07:08.151 --> 00:07:12.530
And in this study,
each person is trying to quit smoking.

113
00:07:12.530 --> 00:07:16.268
The study starts on the day they
decide they are going to quit.

114
00:07:16.268 --> 00:07:19.695
And they're wearing a whole
variety of wearables.

115
00:07:19.695 --> 00:07:24.022
And they go through their whole life and
all this sensor data streaming in.

116
00:07:24.022 --> 00:07:28.087
And the sensor data's used to form
a whole variety of predictions.

117
00:07:28.087 --> 00:07:30.839
And one prediction,
in particular, is whether or

118
00:07:30.839 --> 00:07:33.719
not they're currently
physiologically stressed.

119
00:07:33.719 --> 00:07:40.850
And we're using algorithms to test whether
or not, if you're currently stressed,

120
00:07:40.850 --> 00:07:46.903
we should provide a reminder to
practice stress management exercises.

121
00:07:46.903 --> 00:07:51.800
Of course, the long term goal is to
find optimal policy for each user.

122
00:07:51.800 --> 00:07:54.775
Thanks, good luck on your class, bye.