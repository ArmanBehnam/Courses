WEBVTT

1
00:00:00.000 --> 00:00:06.322
[MUSIC]

2
00:00:06.322 --> 00:00:09.718
Okay, so my name is Estefania and

3
00:00:09.718 --> 00:00:14.869
I'm a professor here at the U
of A teaching Computing Science.

4
00:00:14.869 --> 00:00:19.148
So one of the things is that of
course if you want to have a solution

5
00:00:19.148 --> 00:00:23.510
then you should know what your
problem is because your solution is

6
00:00:23.510 --> 00:00:27.663
going to be a function of
the problem that you're working on.

7
00:00:27.663 --> 00:00:33.999
And the other thing is that is that while
you might think that this determines

8
00:00:33.999 --> 00:00:39.351
what you do always but
sometimes you just want to develop tools.

9
00:00:39.351 --> 00:00:44.515
And when you're developing tools
then you want to know which

10
00:00:44.515 --> 00:00:49.681
problems these tools fit and
which problems they fit well and

11
00:00:49.681 --> 00:00:53.342
which problems they are not fit that well.

12
00:00:53.342 --> 00:00:58.390
So they're two important things
that you should start with when

13
00:00:58.390 --> 00:01:03.543
thinking about problems to solve
with reinforcement learning.

14
00:01:03.543 --> 00:01:08.510
First, you should be really clear
about the goal that you have and

15
00:01:08.510 --> 00:01:11.499
you can have different types of goals.

16
00:01:11.499 --> 00:01:16.043
You can have a goal of solving
a specific type of problem or

17
00:01:16.043 --> 00:01:18.694
just like some part of a problem or

18
00:01:18.694 --> 00:01:23.254
you could be interested in
developing some kind of tool.

19
00:01:23.254 --> 00:01:28.000
And then, in that case,
you should be clear about

20
00:01:28.000 --> 00:01:32.417
what are the goals in
developing those tools?

21
00:01:32.417 --> 00:01:34.912
So where do you want to
apply the tools for?

22
00:01:36.825 --> 00:01:41.486
And when it comes to specifying
a problem the things you should

23
00:01:41.486 --> 00:01:44.361
be thinking about are the following.

24
00:01:44.361 --> 00:01:47.624
You should be thinking about how
the data is going to be collected?

25
00:01:47.624 --> 00:01:53.085
And how you're going to evaluate
the algorithms that you're going to

26
00:01:53.085 --> 00:01:58.464
develop but at the specific
properties of the environment if any?

27
00:01:58.464 --> 00:02:00.834
And I'm going to expand on these.

28
00:02:00.834 --> 00:02:06.496
So when talking about
the data that you're going to

29
00:02:06.496 --> 00:02:12.298
use in your algorithm there
are different types of

30
00:02:12.298 --> 00:02:18.805
problems depending on how
you're acquiring the data.

31
00:02:18.805 --> 00:02:22.965
So maybe your system is going to
interact with the real world or

32
00:02:22.965 --> 00:02:26.245
maybe it's going to interact
with a simulator or

33
00:02:26.245 --> 00:02:31.060
there will be no interaction at all and
the data is going to sit on a disk.

34
00:02:31.060 --> 00:02:36.355
And depending on which situation you
are in you may need to choose very

35
00:02:36.355 --> 00:02:41.847
different tools and techniques and
you will have different concerns.

36
00:02:41.847 --> 00:02:46.555
So if your system is interacting
with the real world then,

37
00:02:46.555 --> 00:02:51.071
of course, the real world is
running at our own speeds so

38
00:02:51.071 --> 00:02:55.031
data efficiency is going
to be of major concern.

39
00:02:55.031 --> 00:02:59.141
If you're interacting with
a simulator then maybe

40
00:02:59.141 --> 00:03:03.738
all you care about is how much
compute you're going to use.

41
00:03:03.738 --> 00:03:09.763
Why, if there is no interaction at all
then you have all the data saved on a disk

42
00:03:09.763 --> 00:03:15.435
then you care about how much information
you can extract from this data.

43
00:03:15.435 --> 00:03:23.090
So you will have different goals depending
on where the data is coming from.

44
00:03:23.090 --> 00:03:28.260
And the algorithms that you will need to
apply are going to change accordingly.

45
00:03:30.414 --> 00:03:36.297
So the other aspect of specifying
what reinforcement learning

46
00:03:36.297 --> 00:03:43.404
problem you have is how you're going
to evaluate your algorithms, right?

47
00:03:43.404 --> 00:03:46.295
So do you care about efficient learning?

48
00:03:46.295 --> 00:03:51.758
And if you do care about efficient
learning as probably you should.

49
00:03:51.758 --> 00:03:59.138
You, You care about how well your system
is going to do at the end of the learning.

50
00:03:59.138 --> 00:04:04.109
Or you're trying to develop a learning
system that is also doing well during

51
00:04:04.109 --> 00:04:04.893
learning.

52
00:04:04.893 --> 00:04:10.783
So depending on the situation you are in
you might choose different algorithms.

53
00:04:10.783 --> 00:04:15.768
So the algorithm, for example,
that cares about like if you

54
00:04:15.768 --> 00:04:20.655
care about how well you're going
to do at the end of learning

55
00:04:20.655 --> 00:04:25.251
then your algorithm may do
crazy exploration that may be

56
00:04:25.251 --> 00:04:29.982
very expensive in terms of
reward while it's learning.

57
00:04:29.982 --> 00:04:35.499
But if you're in the second situation
where you care about the reward

58
00:04:35.499 --> 00:04:40.253
collected during learning then
you may not want to do that,

59
00:04:40.253 --> 00:04:45.409
that much so you will need to
choose different solutions again.

60
00:04:45.409 --> 00:04:49.708
And finally, you will need to
choose your performance metrics and

61
00:04:49.708 --> 00:04:53.253
there's a wide range of
possibilities there as well.

62
00:04:53.253 --> 00:04:59.088
You might care about total reward,
maybe you will use a discount factor.

63
00:04:59.088 --> 00:05:05.001
Maybe you care about
the risk-sensitivity of your solution.

64
00:05:05.001 --> 00:05:08.942
Maybe you care about
performance across different

65
00:05:08.942 --> 00:05:13.166
environments in worse-case sense or
average sense.

66
00:05:13.166 --> 00:05:18.013
Maybe you don't care about the reward
maybe you care about how much knowledge is

67
00:05:18.013 --> 00:05:22.369
acquired or what skills are acquired
at the end of the learning period.

68
00:05:22.369 --> 00:05:25.803
So in all of these different
settings you're going to evaluate

69
00:05:25.803 --> 00:05:30.147
your algorithm differently and you might
choose very different algorithms for

70
00:05:30.147 --> 00:05:31.403
different settings.

71
00:05:34.118 --> 00:05:39.819
And finally,
it's also very important to think a little

72
00:05:39.819 --> 00:05:44.809
bit about the specific
aspects of the environment

73
00:05:44.809 --> 00:05:49.111
that you're interested in dealing with.

74
00:05:49.111 --> 00:05:53.097
So the simplest of this is
just like how many states and

75
00:05:53.097 --> 00:05:56.120
actions do you have in the environment?

76
00:05:56.120 --> 00:06:00.306
So if you have like a large
number of states are there any

77
00:06:00.306 --> 00:06:04.772
irregularities between
the states that you can exploit?

78
00:06:04.772 --> 00:06:09.621
If you have a large number of actions
then if you have continuously

79
00:06:09.621 --> 00:06:13.679
many actions than some
algorithms become unavailable

80
00:06:13.679 --> 00:06:17.838
because they're not able to
deal with those settings.

81
00:06:17.838 --> 00:06:22.297
But if you have finitely many actions,
of actions then maybe use of those

82
00:06:22.297 --> 00:06:25.657
algorithms become available so
you may want to choose them.

83
00:06:25.657 --> 00:06:31.419
And the other aspect could
be that maybe the rewards

84
00:06:31.419 --> 00:06:36.511
in the environment or
the cost can blow up, so

85
00:06:36.511 --> 00:06:41.603
they can become unbounded and
then you don't

86
00:06:41.603 --> 00:06:46.966
want to run the risk of
facing unbounded costs.

87
00:06:46.966 --> 00:06:52.637
Maybe the rewards random or the
environment is random or maybe it's not.

88
00:06:52.637 --> 00:06:57.689
And again, if the environment is not
random then you can have specialized

89
00:06:57.689 --> 00:07:02.514
algorithm that take advantage of that,
the problem is much simpler.

90
00:07:02.514 --> 00:07:09.217
So all of these aspects of
the environment are pretty important and

91
00:07:09.217 --> 00:07:15.933
you will choose your algorithm
based on what situation you are in.

92
00:07:15.933 --> 00:07:20.708
And why is this important you may ask?

93
00:07:20.708 --> 00:07:25.658
Well, just as an illustration
just think about the simple

94
00:07:25.658 --> 00:07:29.698
Bandit Problem where you
have maybe two arms and

95
00:07:29.698 --> 00:07:36.282
the underlying distribution of the arms
are Stochastic Bandit are unknown.

96
00:07:36.282 --> 00:07:41.451
But if you happen to know that the
distribution belongs to a specific family

97
00:07:41.451 --> 00:07:46.209
like you have normal distributions or
you have Bandit distributions

98
00:07:46.209 --> 00:07:51.069
then you will choose algorithms
that depend on these distributions.

99
00:07:51.069 --> 00:07:55.807
And distribution depends
on the knowledge that you

100
00:07:55.807 --> 00:08:00.546
have about these distributions and
your algorithm

101
00:08:00.546 --> 00:08:05.518
will do much better if you
explore these properties.

102
00:08:05.518 --> 00:08:10.495
So in a nutshell,
it's important to be clear about

103
00:08:10.495 --> 00:08:14.779
what is the problem
that you're solving for

104
00:08:14.779 --> 00:08:19.873
the sake of choosing the right
algorithm and also for

105
00:08:19.873 --> 00:08:24.734
the sake of clarity of
the scoop of your solution is

106
00:08:24.734 --> 00:08:30.100
going to depend on this and
you want to be clear about it.