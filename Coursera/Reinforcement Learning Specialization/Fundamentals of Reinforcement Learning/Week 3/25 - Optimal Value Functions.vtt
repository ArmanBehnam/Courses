WEBVTT

1
00:00:05.870 --> 00:00:08.340
Previously, we learned about

2
00:00:08.340 --> 00:00:10.650
how optimal policies
achieve the goal of

3
00:00:10.650 --> 00:00:12.630
reinforcement learning
to obtain as

4
00:00:12.630 --> 00:00:15.210
much for more as possible
in the long run.

5
00:00:15.210 --> 00:00:17.070
In this video, we'll describe

6
00:00:17.070 --> 00:00:19.875
the related notion of
an optimal value function,

7
00:00:19.875 --> 00:00:23.880
and introduce an associated set
of Bellman equations.

8
00:00:23.880 --> 00:00:26.110
By the end of this video,

9
00:00:26.110 --> 00:00:27.590
you will be able to derive

10
00:00:27.590 --> 00:00:29.225
the Bellman optimality equation

11
00:00:29.225 --> 00:00:30.965
for the state-value function,

12
00:00:30.965 --> 00:00:33.155
derive the Bellman
optimality equation

13
00:00:33.155 --> 00:00:34.925
for the action-value function,

14
00:00:34.925 --> 00:00:37.880
and understand how
the Bellman optimality equations

15
00:00:37.880 --> 00:00:41.425
relate to the previously
introduced Bellman equations.

16
00:00:41.425 --> 00:00:45.165
Recalling that for two
policies Pi_1 and Pi_2.

17
00:00:45.165 --> 00:00:47.570
Pi_1 is considered as good as or

18
00:00:47.570 --> 00:00:49.910
better than Pi_2 if and only if

19
00:00:49.910 --> 00:00:52.370
the value under Pi_1
is greater than or

20
00:00:52.370 --> 00:00:55.730
equal to the value under
Pi_2 for all states.

21
00:00:55.730 --> 00:00:58.160
An optimal policy is one which as

22
00:00:58.160 --> 00:01:00.920
good as or better than
every other policy.

23
00:01:00.920 --> 00:01:03.680
The value function for
the optimal policy

24
00:01:03.680 --> 00:01:07.040
thus has the greatest value
possible in every state.

25
00:01:07.040 --> 00:01:09.400
We can express this
mathematically,

26
00:01:09.400 --> 00:01:12.170
by writing that vPi star of S is

27
00:01:12.170 --> 00:01:15.940
equal to the maximum value
over all policies.

28
00:01:15.940 --> 00:01:19.175
This holds for every state
in our state-space.

29
00:01:19.175 --> 00:01:22.475
Taking a maximum over policies
might not be intuitive.

30
00:01:22.475 --> 00:01:25.115
So let's take a moment to
break down what it means.

31
00:01:25.115 --> 00:01:26.690
Imagine we were to consider

32
00:01:26.690 --> 00:01:28.490
every possible policy and

33
00:01:28.490 --> 00:01:30.485
compute each of
their values for the state

34
00:01:30.485 --> 00:01:33.484
S. The value of an optimal policy

35
00:01:33.484 --> 00:01:36.860
is defined to be the largest
of all the computed values.

36
00:01:36.860 --> 00:01:39.560
We could repeat this for
every state and the value of

37
00:01:39.560 --> 00:01:42.650
an optimal policy would
always be the largest.

38
00:01:42.650 --> 00:01:44.600
All optimal policies have

39
00:01:44.600 --> 00:01:47.060
this same optimal
state-value function,

40
00:01:47.060 --> 00:01:50.040
which we denote by v star.

41
00:01:50.050 --> 00:01:52.760
Optimal policies also share

42
00:01:52.760 --> 00:01:54.875
the same optimal
action-value function,

43
00:01:54.875 --> 00:01:56.300
which is again the maximum

44
00:01:56.300 --> 00:01:58.910
possible for
every state action pair.

45
00:01:58.910 --> 00:02:03.260
We denote this shared action
value function by q star.

46
00:02:03.260 --> 00:02:07.280
Recall the Bellman equation
for the state value function.

47
00:02:07.280 --> 00:02:09.860
This equation holds for
the value function of

48
00:02:09.860 --> 00:02:13.235
any policy including
an optimal policy.

49
00:02:13.235 --> 00:02:16.600
By substituting
the optimal policy Pi star

50
00:02:16.600 --> 00:02:18.210
into this Bellman equation,

51
00:02:18.210 --> 00:02:20.630
we get the Bellman
equation for v star.

52
00:02:20.630 --> 00:02:23.150
So far, we haven't
done anything special.

53
00:02:23.150 --> 00:02:25.490
This is simply the Bellman
equation we introduced

54
00:02:25.490 --> 00:02:29.270
previously for the specific case
of an optimal policy.

55
00:02:29.270 --> 00:02:32.720
However, because this
is an optimal policy,

56
00:02:32.720 --> 00:02:35.330
we can rewrite the equation
in a special form,

57
00:02:35.330 --> 00:02:37.985
which doesn't reference
the policy itself.

58
00:02:37.985 --> 00:02:40.280
Remember there always exists

59
00:02:40.280 --> 00:02:42.500
an optimal deterministic policy,

60
00:02:42.500 --> 00:02:45.515
one that selects an optimal
action in every state.

61
00:02:45.515 --> 00:02:48.050
Such a deterministic
optimal policy

62
00:02:48.050 --> 00:02:50.025
will assign Probability 1,

63
00:02:50.025 --> 00:02:51.380
for an action that achieves

64
00:02:51.380 --> 00:02:54.425
the highest value
and Probability 0,

65
00:02:54.425 --> 00:02:56.300
for all other actions.

66
00:02:56.300 --> 00:02:59.269
We can express this
another way by replacing

67
00:02:59.269 --> 00:03:02.420
the sum over Pi star
with a max over a.

68
00:03:02.420 --> 00:03:06.425
Notice that Pi star no longer
appears in the equation.

69
00:03:06.425 --> 00:03:08.510
We have derived
a relationship that

70
00:03:08.510 --> 00:03:11.750
applies directly
to v star itself.

71
00:03:11.750 --> 00:03:13.805
We call this special form,

72
00:03:13.805 --> 00:03:17.210
the Bellman optimality
equation for v star.

73
00:03:17.210 --> 00:03:19.580
We can make the
same replacement in

74
00:03:19.580 --> 00:03:22.550
the Bellman equation for
the action-value function.

75
00:03:22.550 --> 00:03:26.980
Here the optimal policy
appears in the inner sum.

76
00:03:26.980 --> 00:03:29.640
Once again, we
replace the sum over

77
00:03:29.640 --> 00:03:32.180
Pi star with a max over a.

78
00:03:32.180 --> 00:03:36.720
This gives us the Bellman
optimality equation for q star.

79
00:03:36.770 --> 00:03:38.965
In an earlier lecture,

80
00:03:38.965 --> 00:03:41.360
we discussed how
the Bellman equations form

81
00:03:41.360 --> 00:03:43.190
a linear system of equations that

82
00:03:43.190 --> 00:03:45.740
can be solved by
standard methods.

83
00:03:45.740 --> 00:03:48.170
The Bellman's optimality
equation gives us

84
00:03:48.170 --> 00:03:51.575
a similar system of equations
for the optimal value.

85
00:03:51.575 --> 00:03:53.430
One natural question is,

86
00:03:53.430 --> 00:03:55.010
can we solve this system in

87
00:03:55.010 --> 00:03:58.675
a similar way to find the
optimal state-value function?

88
00:03:58.675 --> 00:04:01.280
Unfortunately, the answer is no.

89
00:04:01.280 --> 00:04:03.050
Taking the maximum over

90
00:04:03.050 --> 00:04:05.600
actions is not
a linear operation.

91
00:04:05.600 --> 00:04:07.955
So standard techniques
from linear algebra

92
00:04:07.955 --> 00:04:10.930
for solving linear
systems won't apply.

93
00:04:10.930 --> 00:04:13.610
In this course, we will
not form and solve

94
00:04:13.610 --> 00:04:15.950
systems of equations
in the usual way.

95
00:04:15.950 --> 00:04:19.160
Instead, we will use
other techniques based on

96
00:04:19.160 --> 00:04:20.660
the Bellman equations to compute

97
00:04:20.660 --> 00:04:23.580
value functions and policies.

98
00:04:23.860 --> 00:04:28.010
You might be wondering why we
can't simply use Pi star in

99
00:04:28.010 --> 00:04:30.050
the ordinary Bellman
equation to get

100
00:04:30.050 --> 00:04:32.780
a system of linear
equations for v star.

101
00:04:32.780 --> 00:04:34.385
The answer is simple.

102
00:04:34.385 --> 00:04:36.055
We don't know Pi star.

103
00:04:36.055 --> 00:04:38.060
If we did, then we
would have already

104
00:04:38.060 --> 00:04:41.360
achieved the fundamental goal
of reinforcement learning.

105
00:04:41.360 --> 00:04:43.400
If we can manage to solve

106
00:04:43.400 --> 00:04:45.430
the Bellman optimality
equation for v star,

107
00:04:45.430 --> 00:04:49.325
we can use the result to
obtain Pi star fairly easily.

108
00:04:49.325 --> 00:04:52.775
We will see how this can
be done in the next video.

109
00:04:52.775 --> 00:04:56.510
So we introduced
optimal value functions

110
00:04:56.510 --> 00:04:59.840
and derive the associative
Bellman optimality equations.

111
00:04:59.840 --> 00:05:02.060
The Bellman optimality
equations relate

112
00:05:02.060 --> 00:05:04.255
the value of a state
or state-action pair,

113
00:05:04.255 --> 00:05:07.670
to it's possible successors
under any optimal policy.

114
00:05:07.670 --> 00:05:10.370
See you next time, where
we'll learn how to find

115
00:05:10.370 --> 00:05:14.010
the optimal policy from
the optimal value function.