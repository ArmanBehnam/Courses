WEBVTT

1
00:00:00.000 --> 00:00:05.850
I'm Andrew Barto, and I want
to introduce Rich Sutton,

2
00:00:05.850 --> 00:00:07.500
who was my first student,

3
00:00:07.500 --> 00:00:12.430
and we spent many years
working together.

4
00:00:12.890 --> 00:00:16.890
Both of us are amazed I think

5
00:00:16.890 --> 00:00:17.940
as the app [inaudible] to

6
00:00:17.940 --> 00:00:21.000
the outgrowth of that early work.

7
00:00:21.000 --> 00:00:28.275
And rich, is an amazingly
focused persistent,

8
00:00:28.275 --> 00:00:32.280
man of genius, I think and fancy.

9
00:00:32.280 --> 00:00:39.090
I'm embarrassing
him. A lot of London

10
00:00:39.090 --> 00:00:42.890
the core ideas I really owe

11
00:00:42.890 --> 00:00:47.920
an enormous amount to
Rich, so that's Rich.

12
00:00:51.530 --> 00:00:53.730
Well, I feel like I've

13
00:00:53.730 --> 00:00:56.290
learned an enormous
amount from you Andy.

14
00:00:57.230 --> 00:01:03.510
You have just been essential
to making our work scholarly

15
00:01:03.510 --> 00:01:11.915
and relevant to the modern
age and to the old.

16
00:01:11.915 --> 00:01:14.330
I feel enormously
grounded by all things

17
00:01:14.330 --> 00:01:17.550
I've learned as your
students in those years.

18
00:01:18.160 --> 00:01:20.400
I want to say that

19
00:01:20.400 --> 00:01:23.855
the main thing we did where
we wrote that textbook,

20
00:01:23.855 --> 00:01:26.135
but the main thing we did was we

21
00:01:26.135 --> 00:01:28.760
rediscovered the field
of reinforcement.

22
00:01:28.760 --> 00:01:31.380
I'd like something I'd like
to say that we discovered it,

23
00:01:31.380 --> 00:01:34.050
but I know that you
would react to that

24
00:01:34.050 --> 00:01:35.820
because there's nothing new

25
00:01:35.820 --> 00:01:37.365
under the sun, you
used to tell me.

26
00:01:37.365 --> 00:01:37.800
Great.

27
00:01:37.800 --> 00:01:42.855
If you do find something,

28
00:01:42.855 --> 00:01:44.900
you read, there's nothing wrong

29
00:01:44.900 --> 00:01:46.655
with rediscovering something old,

30
00:01:46.655 --> 00:01:48.605
and you should embrace that.

31
00:01:48.605 --> 00:01:51.080
If you rediscovered the wheel,

32
00:01:51.080 --> 00:01:53.970
as you say, we should
call it the wheel.

33
00:01:54.730 --> 00:01:56.945
There may be a better way.

34
00:01:56.945 --> 00:01:59.040
Maybe a better way, but I

35
00:01:59.040 --> 00:02:01.070
do feel we rediscovered
reinforcement.

36
00:02:01.070 --> 00:02:04.775
We woke it up because it would
add fallings on collect,

37
00:02:04.775 --> 00:02:06.590
and we clarified what it

38
00:02:06.590 --> 00:02:09.385
was and how it's different
from supervised learning.

39
00:02:09.385 --> 00:02:12.065
This is sort of what I was

40
00:02:12.065 --> 00:02:13.700
characterizing as
the origin story

41
00:02:13.700 --> 00:02:14.945
of reinforcement learning.

42
00:02:14.945 --> 00:02:17.180
It was an obvious idea,

43
00:02:17.180 --> 00:02:21.840
Marvin Minsky knew
it in 1960 or '59.

44
00:02:22.130 --> 00:02:25.415
It's so obvious that
everyone knew it,

45
00:02:25.415 --> 00:02:27.905
but then it became overshadowed

46
00:02:27.905 --> 00:02:30.790
by supervised learning until,

47
00:02:30.790 --> 00:02:34.510
Eric Loft started knocking on
people's doors and saying,

48
00:02:34.510 --> 00:02:37.395
"Hey, this is something
that's been neglected."

49
00:02:37.395 --> 00:02:39.050
And as real and as important

50
00:02:39.050 --> 00:02:41.850
and tasked us to figure it out.

51
00:02:42.070 --> 00:02:47.290
In fact, the very first
neural network simulation

52
00:02:47.290 --> 00:02:51.600
on a digital computer by
farmland Clark was a-

53
00:02:51.600 --> 00:02:52.665
1954.

54
00:02:52.665 --> 00:02:54.895
Was a reinforcement
learning system.

55
00:02:54.895 --> 00:02:56.560
In their second paper,

56
00:02:56.560 --> 00:02:58.420
Clark and Farley a year later,

57
00:02:58.420 --> 00:03:00.520
it was the same system

58
00:03:00.520 --> 00:03:03.085
but they focused
on generalization.

59
00:03:03.085 --> 00:03:08.820
Certainly it departed
from the root zone.

60
00:03:08.820 --> 00:03:11.710
After that, your
perceptron and withdraw

61
00:03:11.710 --> 00:03:15.170
Hoff was Error correction.

62
00:03:15.170 --> 00:03:18.010
Even though, some of
the words they used

63
00:03:18.010 --> 00:03:22.525
were trial and error,
they really worked.

64
00:03:22.525 --> 00:03:25.165
Became supervised.

65
00:03:25.165 --> 00:03:27.310
Yes, and there were exceptions,

66
00:03:27.310 --> 00:03:28.720
but as we just said,

67
00:03:28.720 --> 00:03:30.930
Minsky, in admins case,

68
00:03:30.930 --> 00:03:34.125
thesis and his steps paper,

69
00:03:34.125 --> 00:03:37.140
is full of prediction,

70
00:03:37.140 --> 00:03:42.295
TV like things, certainly the
credit assignment problem.

71
00:03:42.295 --> 00:03:45.605
Then his thesis at Princeton

72
00:03:45.605 --> 00:03:49.835
with a reinforcement
learning physical network,

73
00:03:49.835 --> 00:03:55.790
that learn to go through a maze.

74
00:03:55.790 --> 00:03:57.620
I think Minsky lost interest in

75
00:03:57.620 --> 00:03:59.840
it for a number of reasons,

76
00:03:59.840 --> 00:04:02.630
or maybe it was
embarrassed by it.

77
00:04:02.630 --> 00:04:03.530
I don't know, but that is

78
00:04:03.530 --> 00:04:06.110
a reinforcement learning system.

79
00:04:06.110 --> 00:04:08.180
Andy, typical of yourself you are

80
00:04:08.180 --> 00:04:11.615
talking about what other
people did historically,

81
00:04:11.615 --> 00:04:13.610
but maybe typical of me I wanted

82
00:04:13.610 --> 00:04:15.320
to bring it back to what we did,

83
00:04:15.320 --> 00:04:17.700
because I think we
did do something.

84
00:04:19.150 --> 00:04:23.510
I almost did very little
but we recognized,

85
00:04:23.510 --> 00:04:25.495
we just stood up and said,

86
00:04:25.495 --> 00:04:27.085
"This is a thing.

87
00:04:27.085 --> 00:04:28.970
This is the thing that
hasn't been investigated

88
00:04:28.970 --> 00:04:30.890
and it's deserving
an investigation."

89
00:04:30.890 --> 00:04:34.790
And we wrote papers on
associative search networks,

90
00:04:34.790 --> 00:04:38.600
and just really simple ideas,

91
00:04:38.600 --> 00:04:42.320
and made the claim that
this cannot be done.

92
00:04:42.320 --> 00:04:48.170
The combination of association
and trial and error.

93
00:04:48.170 --> 00:04:51.390
I think as you said that
I think at the time,

94
00:04:51.760 --> 00:04:54.350
search for something
that works and

95
00:04:54.350 --> 00:04:56.890
then you will remember,

96
00:04:56.890 --> 00:04:59.600
combining search and memory.

97
00:04:59.600 --> 00:05:02.025
That is the essence
of reinforcing,

98
00:05:02.025 --> 00:05:06.480
then strangely had been wrong.

99
00:05:06.480 --> 00:05:08.460
I discovered something
relevant to that.

100
00:05:08.460 --> 00:05:11.750
Mentalization, that problem
[inaudible] who was

101
00:05:11.750 --> 00:05:13.310
my colleague at

102
00:05:13.310 --> 00:05:16.230
the University of
Massachusetts for a while.

103
00:05:19.210 --> 00:05:25.750
Donald Mickey, talked about
mentalization which is one,

104
00:05:25.750 --> 00:05:29.270
RL is a memorized search.

105
00:05:29.270 --> 00:05:31.805
You do some operation,

106
00:05:31.805 --> 00:05:33.640
and then you remember
the results,

107
00:05:33.640 --> 00:05:35.450
and the next time
you have to do it.

108
00:05:35.450 --> 00:05:37.970
You look it up instead
of recomputing,

109
00:05:37.970 --> 00:05:40.115
and it saves a lot
of time and so on.

110
00:05:40.115 --> 00:05:44.135
In a sense, our RL at its root

111
00:05:44.135 --> 00:05:49.230
is memorized
context-sensitive search.

112
00:05:49.840 --> 00:05:53.630
Pople stone, and actually
at the end of one of

113
00:05:53.630 --> 00:05:57.560
his paper on this 50th,
I forget the date,

114
00:05:57.560 --> 00:06:02.245
but talks about interpolator,

115
00:06:02.245 --> 00:06:05.615
like using polynomials
instead of Lookup table

116
00:06:05.615 --> 00:06:11.015
to look up something
with generalization.

117
00:06:11.015 --> 00:06:14.915
That's what neural
networks do for example.

118
00:06:14.915 --> 00:06:17.869
So we didn't-

119
00:06:17.869 --> 00:06:19.305
We discovered.

120
00:06:19.305 --> 00:06:23.190
We didn't invent memorization,

121
00:06:23.190 --> 00:06:26.285
but through a new use for it.

122
00:06:26.285 --> 00:06:27.800
I don't know if people were doing

123
00:06:27.800 --> 00:06:31.815
memorized search the way
reinforcement learner's.

124
00:06:31.815 --> 00:06:35.555
Here he had this idea of
a distributed approach.

125
00:06:35.555 --> 00:06:39.275
Gold seeking systems made up
of gold seeking components.

126
00:06:39.275 --> 00:06:41.540
He also had the idea of

127
00:06:41.540 --> 00:06:44.020
what you call a
generalized reinforcement,

128
00:06:44.020 --> 00:06:48.470
that one of these units could
be reinforced by all kinds

129
00:06:48.470 --> 00:06:53.640
of signals and not just
our binary signal.

130
00:06:53.740 --> 00:07:00.990
The way I remember
it, the essential,

131
00:07:00.990 --> 00:07:02.400
I know I'm in column,

132
00:07:02.400 --> 00:07:04.310
so it's like an
insight but it's like

133
00:07:04.310 --> 00:07:07.020
that on an absence of insight.

134
00:07:07.490 --> 00:07:09.730
We decided one day,

135
00:07:09.730 --> 00:07:13.880
well maybe just the goal
seeking unit thing.

136
00:07:13.880 --> 00:07:18.140
Maybe that's an interesting
idea all on its own.

137
00:07:18.140 --> 00:07:20.405
Without making
goal-seeking systems

138
00:07:20.405 --> 00:07:21.880
out of ballsy proponents,

139
00:07:21.880 --> 00:07:23.490
and without having a
generalized reinforcement,

140
00:07:23.490 --> 00:07:27.020
would have just have a
specialized reward signal.

141
00:07:27.020 --> 00:07:28.940
We just study that Learning,

142
00:07:28.940 --> 00:07:31.205
and maybe that's the thing.

143
00:07:31.205 --> 00:07:33.995
Maybe that's something that
needs to be worked out.

144
00:07:33.995 --> 00:07:38.940
Like. To me, it was remedies
like maybe just that.

145
00:07:38.940 --> 00:07:42.380
It was like a lethal
level and fancy stuff,

146
00:07:42.380 --> 00:07:43.925
or you're confusing stuff aside.

147
00:07:43.925 --> 00:07:46.955
For the moment, there's
a point to be made,

148
00:07:46.955 --> 00:07:48.830
and I think that's what
reinforcement learning.

149
00:07:48.830 --> 00:07:51.080
It is just focusing on a
learning system that actually

150
00:07:51.080 --> 00:07:52.250
wants something
that does trial and

151
00:07:52.250 --> 00:07:53.540
error and remembers it,

152
00:07:53.540 --> 00:07:56.760
and has to specialized
reward signal.