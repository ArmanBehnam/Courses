WEBVTT

1
00:00:06.410 --> 00:00:09.480
This week, we learned
all about policies,

2
00:00:09.480 --> 00:00:12.330
value functions, and
the relationships between them.

3
00:00:12.330 --> 00:00:15.090
We also learned about
Bellman equations which help

4
00:00:15.090 --> 00:00:17.655
us reason about
the value of policies.

5
00:00:17.655 --> 00:00:19.050
In this video, we'll do

6
00:00:19.050 --> 00:00:21.915
a quick recap of
everything we covered.

7
00:00:21.915 --> 00:00:25.155
Policies tell an agent
how to behave.

8
00:00:25.155 --> 00:00:29.085
Deterministic policies map
each state to an action.

9
00:00:29.085 --> 00:00:31.005
Each time a state is visited,

10
00:00:31.005 --> 00:00:32.580
a deterministic policy selects

11
00:00:32.580 --> 00:00:34.620
the associated action Pi of

12
00:00:34.620 --> 00:00:37.925
S. Stochastic policies map

13
00:00:37.925 --> 00:00:41.825
each state to a distribution
over all possible actions.

14
00:00:41.825 --> 00:00:43.670
Each time state is visited,

15
00:00:43.670 --> 00:00:45.710
a stochastic policy
randomly draws

16
00:00:45.710 --> 00:00:48.830
an action from the
associated distribution with

17
00:00:48.830 --> 00:00:50.720
probability Pi of A given

18
00:00:50.720 --> 00:00:53.390
S. A policy by

19
00:00:53.390 --> 00:00:56.120
definition depends only
on the current state.

20
00:00:56.120 --> 00:01:00.440
It cannot depend on things
like time or previous states.

21
00:01:00.440 --> 00:01:02.120
This is best thought of as

22
00:01:02.120 --> 00:01:04.760
a restriction on
the state, not the agent.

23
00:01:04.760 --> 00:01:06.755
The state should
provide the agent with

24
00:01:06.755 --> 00:01:09.700
all the information it needs
to make a good decision.

25
00:01:09.700 --> 00:01:11.825
This is an important assumption

26
00:01:11.825 --> 00:01:14.970
for many of the techniques
in reinforcement learning.

27
00:01:15.130 --> 00:01:18.110
Value functions are like magic.

28
00:01:18.110 --> 00:01:21.035
Value functions capture
the future total reward

29
00:01:21.035 --> 00:01:23.030
under a particular policy.

30
00:01:23.030 --> 00:01:24.770
We discussed two kinds of

31
00:01:24.770 --> 00:01:27.230
value functions:
state value functions,

32
00:01:27.230 --> 00:01:29.525
and action value functions.

33
00:01:29.525 --> 00:01:31.760
The state value function gives

34
00:01:31.760 --> 00:01:33.470
the expected return from

35
00:01:33.470 --> 00:01:35.555
the current state under a policy.

36
00:01:35.555 --> 00:01:38.990
The action value function
gives the expected return from

37
00:01:38.990 --> 00:01:41.430
state S if the agent
first selects

38
00:01:41.430 --> 00:01:45.060
actions A and follows
Pi after that.

39
00:01:45.060 --> 00:01:48.410
Value functions simplify
things by aggregating

40
00:01:48.410 --> 00:01:52.380
many possible future returns
into a single number.

41
00:01:52.700 --> 00:01:54.960
Bellman equations define

42
00:01:54.960 --> 00:01:56.750
a relationship
between the value of

43
00:01:56.750 --> 00:02:00.845
a state or state-action pair
and its successor states.

44
00:02:00.845 --> 00:02:02.840
The Bellman equation for

45
00:02:02.840 --> 00:02:05.210
the state value function
gives the value of

46
00:02:05.210 --> 00:02:07.790
the current state as
a sum over the values

47
00:02:07.790 --> 00:02:11.285
of all the successor states,
and immediate rewards.

48
00:02:11.285 --> 00:02:13.610
The Bellman equation
for the action value

49
00:02:13.610 --> 00:02:15.650
function gives the value of

50
00:02:15.650 --> 00:02:18.770
a particular state-action pair
as the sum over

51
00:02:18.770 --> 00:02:20.540
the values of all possible next

52
00:02:20.540 --> 00:02:23.550
state-action pairs and rewards.

53
00:02:24.010 --> 00:02:26.600
The Bellman equations
can be directly

54
00:02:26.600 --> 00:02:28.925
solved to find
the value function.

55
00:02:28.925 --> 00:02:32.795
These Bellman equations
help us evaluate policies,

56
00:02:32.795 --> 00:02:35.780
but they do not yet
achieve our ultimate goal,

57
00:02:35.780 --> 00:02:39.365
to find a policy that attains
as much reward as possible.

58
00:02:39.365 --> 00:02:41.555
To make this goal more precise,

59
00:02:41.555 --> 00:02:43.580
we defined optimal policies,

60
00:02:43.580 --> 00:02:45.260
the optimal value function,

61
00:02:45.260 --> 00:02:48.770
and the associated Bellman's
optimality equations.

62
00:02:48.770 --> 00:02:51.980
An optimal policy is
a policy which achieves

63
00:02:51.980 --> 00:02:55.220
the highest value
possible in every state.

64
00:02:55.220 --> 00:02:58.145
There's always
at least one optimal policy,

65
00:02:58.145 --> 00:03:00.570
but there may be more than one.

66
00:03:01.150 --> 00:03:04.490
The optimal state value
function is equal to

67
00:03:04.490 --> 00:03:07.430
the highest possible value
in every state.

68
00:03:07.430 --> 00:03:09.680
Every optimal policy shares

69
00:03:09.680 --> 00:03:12.185
the same optimal
state value function.

70
00:03:12.185 --> 00:03:15.125
The same is true for
optimal action value functions

71
00:03:15.125 --> 00:03:16.820
and optimal policies.

72
00:03:16.820 --> 00:03:18.440
Like all value functions,

73
00:03:18.440 --> 00:03:21.170
the optimal value functions
have Bellman equations.

74
00:03:21.170 --> 00:03:22.790
These Bellman equations do not

75
00:03:22.790 --> 00:03:24.980
reference a specific policy.

76
00:03:24.980 --> 00:03:27.440
This amounts to replace
in the policy in

77
00:03:27.440 --> 00:03:30.620
the Bellman equation with
a max over all actions.

78
00:03:30.620 --> 00:03:32.300
The optimal policy must always

79
00:03:32.300 --> 00:03:34.745
select the best available action.

80
00:03:34.745 --> 00:03:37.295
We can extract the optimal policy

81
00:03:37.295 --> 00:03:39.520
from the optimal
state value function.

82
00:03:39.520 --> 00:03:41.540
But to do so, we also need

83
00:03:41.540 --> 00:03:44.285
the one-step dynamics of the MDP.

84
00:03:44.285 --> 00:03:46.460
We can get the
optimal policy with

85
00:03:46.460 --> 00:03:48.200
much less work if we have

86
00:03:48.200 --> 00:03:50.395
the optimal action
value function.

87
00:03:50.395 --> 00:03:52.220
We simply select the action with

88
00:03:52.220 --> 00:03:54.870
the highest value in each state.

89
00:03:54.940 --> 00:03:57.800
Next week, we will
see how to compute

90
00:03:57.800 --> 00:03:59.210
optimal policies using

91
00:03:59.210 --> 00:04:02.550
these Bellman equations.
See you then.