WEBVTT

1
00:00:06.440 --> 00:00:09.060
Imagine you maintained
a website that

2
00:00:09.060 --> 00:00:11.265
received millions of hits a day.

3
00:00:11.265 --> 00:00:14.400
You can frame this as
a k-armed bandit problem.

4
00:00:14.400 --> 00:00:16.560
The estimated values
correspond to

5
00:00:16.560 --> 00:00:19.540
the total amount of money
generated by each ad,

6
00:00:19.540 --> 00:00:20.960
and your goal is to display

7
00:00:20.960 --> 00:00:23.390
the ad which will
generate the most money.

8
00:00:23.390 --> 00:00:26.090
How can you keep your value
estimates up-to-date without

9
00:00:26.090 --> 00:00:29.320
storing the data for
millions of clicks?

10
00:00:29.810 --> 00:00:31.915
By the end of this video,

11
00:00:31.915 --> 00:00:34.130
you'll be able to: describe how

12
00:00:34.130 --> 00:00:37.135
action values can be
estimated incrementally,

13
00:00:37.135 --> 00:00:39.920
identify how the
incremental update rule is

14
00:00:39.920 --> 00:00:42.635
an instance of a more
general learning rule,

15
00:00:42.635 --> 00:00:45.830
and describe how
the general learning rule

16
00:00:45.830 --> 00:00:49.110
can be used in
non-stationary problems.

17
00:00:49.240 --> 00:00:51.695
The sample average method

18
00:00:51.695 --> 00:00:53.795
can be written in
a recursive manner.

19
00:00:53.795 --> 00:00:57.395
By doing so, we can avoid
storing all the previous data.

20
00:00:57.395 --> 00:00:59.935
Let's see how that works.

21
00:00:59.935 --> 00:01:02.180
To do so, we're going to pull

22
00:01:02.180 --> 00:01:04.430
the current reward
out of the sum.

23
00:01:04.430 --> 00:01:08.430
Now, the sum only goes
until the previous reward.

24
00:01:08.620 --> 00:01:11.270
We write our next value estimate

25
00:01:11.270 --> 00:01:13.460
in terms of our previous
value estimate.

26
00:01:13.460 --> 00:01:15.785
To do so, we multiply and divide

27
00:01:15.785 --> 00:01:18.440
the current sum by N minus one.

28
00:01:18.440 --> 00:01:20.960
By multiplying and dividing
by the same thing,

29
00:01:20.960 --> 00:01:23.855
we are effectively
multiplying by one.

30
00:01:23.855 --> 00:01:27.275
The circle term should
look familiar to you.

31
00:01:27.275 --> 00:01:29.930
This is just our
definition of Q_n,

32
00:01:29.930 --> 00:01:32.135
the current value estimate.

33
00:01:32.135 --> 00:01:35.405
We can simplify this equation
a little further.

34
00:01:35.405 --> 00:01:39.225
We distribute Q_n
to get this form.

35
00:01:39.225 --> 00:01:42.830
Finally, we pull nQ_n out of

36
00:01:42.830 --> 00:01:46.520
the sum and multiply
by one over n. Now,

37
00:01:46.520 --> 00:01:50.550
we have an incremental update
rule for estimating values.

38
00:01:50.800 --> 00:01:53.630
This equation is of
a form that will

39
00:01:53.630 --> 00:01:55.985
show up many times
throughout the course.

40
00:01:55.985 --> 00:01:58.490
Let's define some of these terms.

41
00:01:58.490 --> 00:02:00.920
The error in the estimate
is the difference

42
00:02:00.920 --> 00:02:03.755
between the old estimate
and the new target.

43
00:02:03.755 --> 00:02:06.500
Taking a step towards
that new target will create

44
00:02:06.500 --> 00:02:09.690
a new estimate that
reduces our error.

45
00:02:10.600 --> 00:02:14.240
Here, the new reward
is our target.

46
00:02:14.240 --> 00:02:16.730
The size of the step
is determined by

47
00:02:16.730 --> 00:02:18.125
our step size parameter

48
00:02:18.125 --> 00:02:20.495
and the error of
our old estimate.

49
00:02:20.495 --> 00:02:22.955
We now have a general rule for

50
00:02:22.955 --> 00:02:25.530
updating the estimate
incrementally.

51
00:02:25.530 --> 00:02:27.725
The step size can
be a function of

52
00:02:27.725 --> 00:02:31.500
n that produces a number
from zero to one.

53
00:02:32.590 --> 00:02:35.840
In the specific case
of the sample average,

54
00:02:35.840 --> 00:02:37.670
the step size is
equal to one over

55
00:02:37.670 --> 00:02:41.540
n. Let's go back to
our friendly doctor.

56
00:02:41.540 --> 00:02:43.430
What if one of
the treatments was more

57
00:02:43.430 --> 00:02:45.550
effective under
certain conditions?

58
00:02:45.550 --> 00:02:48.395
Specifically, let's
say the treatment B

59
00:02:48.395 --> 00:02:51.005
is more effective during
the winter months.

60
00:02:51.005 --> 00:02:54.725
This is an example of
a non-stationary bandit problem.

61
00:02:54.725 --> 00:02:56.640
These problems are like
the bandit problems

62
00:02:56.640 --> 00:02:57.940
we've discussed before,

63
00:02:57.940 --> 00:03:01.475
except the distribution of
rewards changes with time.

64
00:03:01.475 --> 00:03:03.230
The doctor is unaware of

65
00:03:03.230 --> 00:03:06.420
this change but would
like to adapt to it.

66
00:03:06.910 --> 00:03:10.670
One option is to use
a fixed step size.

67
00:03:10.670 --> 00:03:15.005
If Alpha_n is constant like 0.1,

68
00:03:15.005 --> 00:03:17.120
then the most recent rewards

69
00:03:17.120 --> 00:03:20.340
affect the estimate more
than older rewards.

70
00:03:22.030 --> 00:03:24.140
This graph shows the amount of

71
00:03:24.140 --> 00:03:25.280
weight the most recent award

72
00:03:25.280 --> 00:03:29.105
receives versus the reward
received T time steps ago.

73
00:03:29.105 --> 00:03:32.515
The weighting fades
exponentially with time.

74
00:03:32.515 --> 00:03:35.745
As we move to the right
on the x-axis,

75
00:03:35.745 --> 00:03:38.250
we go further back in time.

76
00:03:38.250 --> 00:03:40.135
See why this is true,

77
00:03:40.135 --> 00:03:41.555
let's take a look again

78
00:03:41.555 --> 00:03:43.865
at the incremental
update equation.

79
00:03:43.865 --> 00:03:47.990
Let's first distribute
Alpha and rearrange.

80
00:03:47.990 --> 00:03:50.330
We can write the next value as

81
00:03:50.330 --> 00:03:53.330
a weighted sum of the reward
and the last value.

82
00:03:53.330 --> 00:03:56.220
Notice the recursive form.

83
00:03:56.290 --> 00:03:58.910
We can substitute
the definition of

84
00:03:58.910 --> 00:04:02.320
Q_n into this equation
and we get this.

85
00:04:02.320 --> 00:04:06.010
Then we can distribute
1 minus Alpha,

86
00:04:06.010 --> 00:04:08.270
and we get an equation that
shows the relationship

87
00:04:08.270 --> 00:04:11.700
between R_n and R_n minus one.

88
00:04:11.770 --> 00:04:15.670
We can unroll this recursive
relationship further.

89
00:04:15.670 --> 00:04:18.160
We continually replace Q with

90
00:04:18.160 --> 00:04:20.140
its definition until
we get all the

91
00:04:20.140 --> 00:04:23.140
way back to
the initial value Q_1.

92
00:04:23.140 --> 00:04:27.020
Finally, let's clean
up this long equation.

93
00:04:27.600 --> 00:04:31.060
We write it as
our initial value of

94
00:04:31.060 --> 00:04:34.970
Q plus a weighted sum of
the rewards over time.

95
00:04:34.970 --> 00:04:37.515
What does this equation tell us?

96
00:04:37.515 --> 00:04:40.195
It relates our current
estimate of the value,

97
00:04:40.195 --> 00:04:42.850
Q_n plus one to Q one,

98
00:04:42.850 --> 00:04:45.115
and all the observed rewards.

99
00:04:45.115 --> 00:04:48.190
The first term tells us
that the contribution of

100
00:04:48.190 --> 00:04:51.625
Q_1 decreases
exponentially with time.

101
00:04:51.625 --> 00:04:54.910
The second term tells us
the rewards further back in

102
00:04:54.910 --> 00:04:58.285
time contribute exponentially
less to the sum.

103
00:04:58.285 --> 00:05:01.460
Taken all together, we
see that the influence of

104
00:05:01.460 --> 00:05:03.005
our initialization of Q

105
00:05:03.005 --> 00:05:05.810
goes to zero with
more and more data.

106
00:05:05.810 --> 00:05:07.910
The most recent
rewards contribute

107
00:05:07.910 --> 00:05:10.965
most to our current estimate.

108
00:05:10.965 --> 00:05:13.640
In this video, we derived

109
00:05:13.640 --> 00:05:17.200
an incremental update for
the sample average method.

110
00:05:17.200 --> 00:05:20.210
We generalized the
incremental update rule into

111
00:05:20.210 --> 00:05:22.970
a form that will be revisited
throughout the course.

112
00:05:22.970 --> 00:05:25.880
Finally, we discussed
how a constant step size

113
00:05:25.880 --> 00:05:29.640
might help solve
non-stationary bandit problems.