WEBVTT

1
00:00:00.000 --> 00:00:06.950
[MUSIC]

2
00:00:06.950 --> 00:00:09.957
This week you were introduced to
the k-armed bandit problem, and

3
00:00:09.957 --> 00:00:12.760
several fundamental concepts and
reinforcement learning.

4
00:00:13.970 --> 00:00:15.990
We started by describing
the problem setting.

5
00:00:17.580 --> 00:00:20.370
We have an agent taking actions and

6
00:00:20.370 --> 00:00:22.670
receiving rewards based
on the action selected.

7
00:00:23.960 --> 00:00:26.050
We defined the value of each action,

8
00:00:26.050 --> 00:00:29.510
as the expected reward received,
when taking that action.

9
00:00:31.430 --> 00:00:35.170
The value function q star
is unknown to the agent.

10
00:00:35.170 --> 00:00:38.670
We then introduced the Sample-Average
Method for estimating q star.

11
00:00:39.800 --> 00:00:44.080
It uses the sum of rewards
received when action a is taken,

12
00:00:44.080 --> 00:00:47.000
divided by the number of
times a has been taken.

13
00:00:48.650 --> 00:00:52.550
We derived an Incremental update rule
by finding a recursive definition of

14
00:00:52.550 --> 00:00:54.350
the sample average.

15
00:00:54.350 --> 00:00:59.510
By doing so, we need only store the number
of times each action has been taken,

16
00:00:59.510 --> 00:01:01.220
and the previous estimate.

17
00:01:03.020 --> 00:01:07.290
We then show that we can make
the StepSize more generic, and

18
00:01:07.290 --> 00:01:09.920
in fact, by using a constant StepSize,

19
00:01:09.920 --> 00:01:14.520
we could more effectively solve the bandit
problems that change over time.

20
00:01:14.520 --> 00:01:17.830
Next, we ran into the issue of
Exploration versus Exploitation.

21
00:01:18.880 --> 00:01:20.950
We defined Exploration as the chance for

22
00:01:20.950 --> 00:01:25.780
the agent to improve its knowledge, some
actions may be better than we realize, we

23
00:01:25.780 --> 00:01:30.050
have to try the actions we may not think
are the best to improve our estimates.

24
00:01:31.420 --> 00:01:35.224
We defined Exploitation as the agent
taking the action it currently thinks is

25
00:01:35.224 --> 00:01:38.104
best, in the hopes that it
will generate the most reward.

26
00:01:40.113 --> 00:01:44.495
[SOUND] The agent cannot explore and
exploit simultaneously, so

27
00:01:44.495 --> 00:01:48.076
how do we choose when to explore and
when to exploit?

28
00:01:48.076 --> 00:01:52.446
To answer this question, we introduce
Epsilon-Greedy Action Selection.

29
00:01:52.446 --> 00:01:56.370
Epsilon-Greedy explores Epsilon
percentage of the time, and

30
00:01:56.370 --> 00:01:58.790
exploits 1 minus Epsilon
percentage of the time.

31
00:02:00.470 --> 00:02:01.920
When Epsilon-Greedy exploits,

32
00:02:01.920 --> 00:02:05.410
it chooses the action which maximizes
the current value estimate.

33
00:02:07.270 --> 00:02:10.880
When Epsilon-Greedy explores,
it chooses an action Uniform randomly.

34
00:02:12.410 --> 00:02:15.559
We also investigated the effects
of Optimistic-Initial Values.

35
00:02:17.950 --> 00:02:20.570
If the initial values
are larger than Q star,

36
00:02:20.570 --> 00:02:24.000
the agent will systematically
explore the actions.

37
00:02:24.000 --> 00:02:27.410
The optimism fades with time and
the agent eventually stops exploring.

38
00:02:28.550 --> 00:02:32.179
Finally, we discussed
Upper-Confidence Bound Action Selection.

39
00:02:32.179 --> 00:02:38.966
UCB mixes exploitation and exploration
through the use of confidence intervals.

40
00:02:38.966 --> 00:02:42.111
UCB uses the strategy of Optimism
in the Face of Uncertainty.

41
00:02:42.111 --> 00:02:46.432
[SOUND] And that's it for bandits.

42
00:02:46.432 --> 00:02:50.641
This week we introduce some of the most
fundamental concepts in reinforcement

43
00:02:50.641 --> 00:02:51.300
learning.

44
00:02:51.300 --> 00:02:53.890
We can largely view the bandit
problem as a subset

45
00:02:53.890 --> 00:02:56.250
of the larger reinforcement
learning problem.

46
00:02:56.250 --> 00:02:59.450
Concepts like maximizing reward and
choosing actions when

47
00:02:59.450 --> 00:03:02.580
faced with uncertainty,
are key to reinforcement learning.

48
00:03:02.580 --> 00:03:05.760
We're excited to begin introducing
the full reinforcement learning problem

49
00:03:05.760 --> 00:03:06.800
next week.

50
00:03:06.800 --> 00:03:09.290
Be sure to read the textbook for
the next week's lectures.