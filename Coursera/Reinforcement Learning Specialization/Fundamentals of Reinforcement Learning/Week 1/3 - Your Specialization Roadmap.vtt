WEBVTT

1
00:00:06.200 --> 00:00:08.970
Reinforcement learning
is a hard topic.

2
00:00:08.970 --> 00:00:10.560
It seems every week I hear about

3
00:00:10.560 --> 00:00:12.060
a new algorithm or
new application

4
00:00:12.060 --> 00:00:13.335
of reinforcement learning.

5
00:00:13.335 --> 00:00:15.270
On the other hand,
the fundamentals

6
00:00:15.270 --> 00:00:17.025
of RL are much the same.

7
00:00:17.025 --> 00:00:19.080
State of the art learning
systems are often

8
00:00:19.080 --> 00:00:21.660
constructed from
a few well-known ideas,

9
00:00:21.660 --> 00:00:23.520
which are actually
not that complicated.

10
00:00:23.520 --> 00:00:25.305
Take DQN for example,

11
00:00:25.305 --> 00:00:26.670
this learning system combines

12
00:00:26.670 --> 00:00:29.099
Q learning, Epsilon-greedy
action selection,

13
00:00:29.099 --> 00:00:30.945
neural network
function approximation,

14
00:00:30.945 --> 00:00:32.750
and a few other ideas to

15
00:00:32.750 --> 00:00:35.690
achieve superhuman scores
in Atari games.

16
00:00:35.690 --> 00:00:37.100
In this one learning system,

17
00:00:37.100 --> 00:00:39.320
we see many of the most
common building blocks

18
00:00:39.320 --> 00:00:41.195
of RL systems.

19
00:00:41.195 --> 00:00:43.730
This course will closely
follow Sutton and

20
00:00:43.730 --> 00:00:45.185
Martha's new addition of

21
00:00:45.185 --> 00:00:47.315
reinforcement learning
and introduction.

22
00:00:47.315 --> 00:00:50.040
The first edition trained
two generations of

23
00:00:50.040 --> 00:00:52.970
RL researchers including
Martha and myself.

24
00:00:52.970 --> 00:00:54.470
The RL book and

25
00:00:54.470 --> 00:00:57.380
this specialization adhere
to a simple principle,

26
00:00:57.380 --> 00:01:00.850
introduce each idea in the
simplest setting it arises.

27
00:01:00.850 --> 00:01:02.390
In this spirit, we begin

28
00:01:02.390 --> 00:01:04.460
our study with
multi-arm bandit problems.

29
00:01:04.460 --> 00:01:07.220
Here, we get our first taste
of the complexities

30
00:01:07.220 --> 00:01:10.690
of incremental learning,
exploration, and exploitation.

31
00:01:10.690 --> 00:01:14.030
After that, we move onto
Markov decision processes to

32
00:01:14.030 --> 00:01:15.710
broaden the class of problems

33
00:01:15.710 --> 00:01:17.915
we can solve with
reinforcement learning methods.

34
00:01:17.915 --> 00:01:19.520
Here we will learn
about balancing

35
00:01:19.520 --> 00:01:21.365
short-term and long-term reward.

36
00:01:21.365 --> 00:01:23.720
We will introduce key ideas
like policies and

37
00:01:23.720 --> 00:01:26.910
value functions using
almost all RL systems.

38
00:01:26.910 --> 00:01:28.460
We conclude Course 1 with

39
00:01:28.460 --> 00:01:31.115
classic planning methods
called dynamic programming.

40
00:01:31.115 --> 00:01:32.570
These methods have been used in

41
00:01:32.570 --> 00:01:34.430
large industrial control
problems and can

42
00:01:34.430 --> 00:01:36.350
compute optimal policies given

43
00:01:36.350 --> 00:01:38.455
a complete model of the world.

44
00:01:38.455 --> 00:01:41.300
In Course 2, we built
on these ideas and

45
00:01:41.300 --> 00:01:42.740
design algorithms for learning

46
00:01:42.740 --> 00:01:44.270
without a model of the world.

47
00:01:44.270 --> 00:01:46.460
We study three classes
of methods designed

48
00:01:46.460 --> 00:01:48.725
for learning from trial
and error interaction.

49
00:01:48.725 --> 00:01:51.050
We start with Monte
Carlo methods and then

50
00:01:51.050 --> 00:01:52.430
move on to temporal
difference learning,

51
00:01:52.430 --> 00:01:53.680
including Q learning.

52
00:01:53.680 --> 00:01:56.030
We conclude Course 2
with an investigation of

53
00:01:56.030 --> 00:01:59.105
methods for planning
with learned models.

54
00:01:59.105 --> 00:02:02.240
In Course 3, we leave
the relative comfort of

55
00:02:02.240 --> 00:02:04.400
small finite MDPs and

56
00:02:04.400 --> 00:02:06.925
investigate RL with
function approximation.

57
00:02:06.925 --> 00:02:08.310
Here we will see that

58
00:02:08.310 --> 00:02:10.320
the main concepts
from Courses 1 and

59
00:02:10.320 --> 00:02:12.110
2 transferred to problems with

60
00:02:12.110 --> 00:02:14.225
larger infinite state spaces.

61
00:02:14.225 --> 00:02:16.205
We will cover
feature construction,

62
00:02:16.205 --> 00:02:17.555
neural network learning,

63
00:02:17.555 --> 00:02:19.370
policy gradient methods, and

64
00:02:19.370 --> 00:02:20.690
other particularities of the

65
00:02:20.690 --> 00:02:22.610
function approximation setting.

66
00:02:22.610 --> 00:02:24.440
You'll notice that
this specialization

67
00:02:24.440 --> 00:02:26.120
starts out with basic material,

68
00:02:26.120 --> 00:02:27.680
but by the end of Course 3,

69
00:02:27.680 --> 00:02:29.000
you'll see how much we build on

70
00:02:29.000 --> 00:02:31.420
simple concepts
introduced earlier on.

71
00:02:31.420 --> 00:02:33.090
You'll finish Course 3 and wonder

72
00:02:33.090 --> 00:02:35.050
how it is you learned so much,

73
00:02:35.050 --> 00:02:38.010
when it all seemed pretty
simple at the time.

74
00:02:38.050 --> 00:02:40.910
The final course in
this specialization brings

75
00:02:40.910 --> 00:02:43.085
everything together in
a Capstone project.

76
00:02:43.085 --> 00:02:44.735
Throughout this specialization,

77
00:02:44.735 --> 00:02:46.310
as in Rich and Andy's book,

78
00:02:46.310 --> 00:02:48.980
we stress a rigorous and
scientific approach to RL.

79
00:02:48.980 --> 00:02:51.050
We conduct numerous experiments

80
00:02:51.050 --> 00:02:53.045
designed to carefully
compare algorithms.

81
00:02:53.045 --> 00:02:55.160
It takes careful
planning and a lot of

82
00:02:55.160 --> 00:02:57.920
hard work to produce
a meaningful empirical results.

83
00:02:57.920 --> 00:03:00.560
In the Capstone, we will walk
you through each step of

84
00:03:00.560 --> 00:03:02.060
this process so that you can

85
00:03:02.060 --> 00:03:03.830
conduct your own
scientific experiment.

86
00:03:03.830 --> 00:03:05.135
We will explore

87
00:03:05.135 --> 00:03:07.580
all the stages from
problem specification,

88
00:03:07.580 --> 00:03:10.070
all the way to
publication quality plots.

89
00:03:10.070 --> 00:03:11.795
This is not just academic.

90
00:03:11.795 --> 00:03:13.370
In real problems,
it's important to

91
00:03:13.370 --> 00:03:15.290
verify and understand
your system.

92
00:03:15.290 --> 00:03:17.360
After that, you should
be ready to test

93
00:03:17.360 --> 00:03:19.400
your own new ideas or tackle

94
00:03:19.400 --> 00:03:22.750
a new exciting application
of RL in your job.

95
00:03:22.750 --> 00:03:25.010
We hope you enjoyed
the show half as

96
00:03:25.010 --> 00:03:27.840
much as we enjoyed
making it for you.