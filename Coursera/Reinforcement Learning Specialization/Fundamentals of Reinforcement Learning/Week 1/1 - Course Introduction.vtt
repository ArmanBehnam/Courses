WEBVTT

1
00:00:04.400 --> 00:00:07.575
Reinforcement learning is
actually not that old.

2
00:00:07.575 --> 00:00:09.600
When I started back
in the mid 2000,

3
00:00:09.600 --> 00:00:11.565
it was a lot different
than it is now.

4
00:00:11.565 --> 00:00:13.560
There were only
a handful of labs around

5
00:00:13.560 --> 00:00:15.840
the world that focused on
reinforcement learning.

6
00:00:15.840 --> 00:00:18.660
Almost nobody was using
RL in the industry,

7
00:00:18.660 --> 00:00:20.745
and the practical successes of RL

8
00:00:20.745 --> 00:00:23.145
were limited to a handful
of applications.

9
00:00:23.145 --> 00:00:25.010
Reinforcement learning
could fly a helicopter

10
00:00:25.010 --> 00:00:27.310
upside down better
than any human could.

11
00:00:27.310 --> 00:00:28.650
RL methods could beat the

12
00:00:28.650 --> 00:00:30.430
world's best backgammon players,

13
00:00:30.430 --> 00:00:32.825
and schedule elevators
pretty well.

14
00:00:32.825 --> 00:00:34.760
But RL was not the best way to

15
00:00:34.760 --> 00:00:36.515
get robots to do useful things,

16
00:00:36.515 --> 00:00:38.840
and the idea that
a Q-learning agent could play

17
00:00:38.840 --> 00:00:42.860
video games better than people
was almost unimaginable.

18
00:00:42.860 --> 00:00:44.825
Yet we held out hope.

19
00:00:44.825 --> 00:00:46.970
Hope because we thought
reinforce learning was

20
00:00:46.970 --> 00:00:49.355
the best way to make
progress towards AI.

21
00:00:49.355 --> 00:00:51.050
The promise of RL was almost

22
00:00:51.050 --> 00:00:53.630
intoxicating that a
learning agent can figure

23
00:00:53.630 --> 00:00:55.670
out how the world works by simply

24
00:00:55.670 --> 00:00:58.070
trying things and
seeing what happens.

25
00:00:58.070 --> 00:01:01.310
After all, that's what we
think people and animals do.

26
00:01:01.310 --> 00:01:03.740
So why not simulate
that in a computer.

27
00:01:03.740 --> 00:01:06.605
If we could do it, then
it would be useful in

28
00:01:06.605 --> 00:01:10.040
almost any application and
as a nice side effect,

29
00:01:10.040 --> 00:01:11.480
we might learn
a good deal about how

30
00:01:11.480 --> 00:01:13.700
our own minds work too.

31
00:01:13.700 --> 00:01:15.680
You might wonder what's

32
00:01:15.680 --> 00:01:17.495
the difference between
supervised learning,

33
00:01:17.495 --> 00:01:20.090
unsupervised learning, and
reinforcement learning?

34
00:01:20.090 --> 00:01:22.040
The differences are quite simple.

35
00:01:22.040 --> 00:01:24.860
In supervised learning we
assume the learner has

36
00:01:24.860 --> 00:01:28.535
access to labeled examples
giving the correct answer.

37
00:01:28.535 --> 00:01:30.920
In RL, the reward gives the agent

38
00:01:30.920 --> 00:01:34.205
some idea of how good or bad
its recent actions were.

39
00:01:34.205 --> 00:01:36.560
You can think of supervised
learning as requiring

40
00:01:36.560 --> 00:01:38.120
a teacher that helps you

41
00:01:38.120 --> 00:01:39.980
by telling you
the correct answer.

42
00:01:39.980 --> 00:01:41.510
A reward on the other hand,

43
00:01:41.510 --> 00:01:43.820
is like having someone
who can identify what

44
00:01:43.820 --> 00:01:45.320
good behavior looks like

45
00:01:45.320 --> 00:01:47.785
but can't tell you
exactly how to do it.

46
00:01:47.785 --> 00:01:50.060
Unsupervised learning
sounds like it could be

47
00:01:50.060 --> 00:01:52.760
related but really has
a very different goal.

48
00:01:52.760 --> 00:01:54.440
Unsupervised learning is about

49
00:01:54.440 --> 00:01:56.765
extracting underlying
structure in data.

50
00:01:56.765 --> 00:01:58.820
It's about the data
representation.

51
00:01:58.820 --> 00:02:01.370
It can be used to
construct representations

52
00:02:01.370 --> 00:02:04.055
that make a supervised
or RL system better.

53
00:02:04.055 --> 00:02:06.740
In fact, as you'll see
later in this course,

54
00:02:06.740 --> 00:02:08.960
techniques from both
supervised learning and

55
00:02:08.960 --> 00:02:10.640
unsupervised learning can be used

56
00:02:10.640 --> 00:02:13.600
within RL to aid generalization.

57
00:02:13.600 --> 00:02:16.385
In RL, we focus on
the problem of learning

58
00:02:16.385 --> 00:02:18.740
while interacting with
an ever changing world.

59
00:02:18.740 --> 00:02:21.095
We don't expect our agents
to simply compute

60
00:02:21.095 --> 00:02:22.730
a good behavior and then execute

61
00:02:22.730 --> 00:02:24.995
that behavior in
an open loop fashion.

62
00:02:24.995 --> 00:02:27.470
We expect our agents
to get things wrong,

63
00:02:27.470 --> 00:02:29.855
to refine their
understanding as they go.

64
00:02:29.855 --> 00:02:31.925
The world is not a static place.

65
00:02:31.925 --> 00:02:33.875
We get injured, the
weather changes,

66
00:02:33.875 --> 00:02:37.160
and we encounter new situations
and our goals change.

67
00:02:37.160 --> 00:02:39.289
An agent that
immediately integrates

68
00:02:39.289 --> 00:02:41.090
its most recent
experience should do

69
00:02:41.090 --> 00:02:43.400
well especially
compared with ones that

70
00:02:43.400 --> 00:02:46.370
attempt to perfectly memorize
how the world works.

71
00:02:46.370 --> 00:02:48.800
The idea of learning
online is extremely

72
00:02:48.800 --> 00:02:51.665
powerful and is
a defining feature of RL.

73
00:02:51.665 --> 00:02:55.385
The way we introduce concepts
is dictated by this fact.

74
00:02:55.385 --> 00:02:57.440
For example, we
introduced the ideas

75
00:02:57.440 --> 00:02:59.629
of bandits and exploration first,

76
00:02:59.629 --> 00:03:02.390
and get ideas from
supervised learning later.

77
00:03:02.390 --> 00:03:04.220
This might feel
backwards for you if

78
00:03:04.220 --> 00:03:05.980
you have a background
in machine learning.

79
00:03:05.980 --> 00:03:07.100
But as you'll see,

80
00:03:07.100 --> 00:03:08.810
the hard part is learning online

81
00:03:08.810 --> 00:03:11.330
rather than just
learning from data.

82
00:03:11.330 --> 00:03:13.625
Getting comfortable
with the online setting

83
00:03:13.625 --> 00:03:16.070
requires a new perspective.

84
00:03:16.070 --> 00:03:18.500
Today, the field of
reinforce learning

85
00:03:18.500 --> 00:03:21.020
feels like it's changing
at a breakneck pace.

86
00:03:21.020 --> 00:03:22.940
There are almost
weekly posts about

87
00:03:22.940 --> 00:03:25.370
new algorithmic developments
and improvements in

88
00:03:25.370 --> 00:03:28.055
state of the art performance
in benchmark domains.

89
00:03:28.055 --> 00:03:30.695
Search companies,
online retailers,

90
00:03:30.695 --> 00:03:32.540
and hardware manufacturers
are exploring

91
00:03:32.540 --> 00:03:35.375
RL solutions for their
day to day operations.

92
00:03:35.375 --> 00:03:37.160
They are convinced
that online learning

93
00:03:37.160 --> 00:03:38.470
will make them more efficient,

94
00:03:38.470 --> 00:03:40.040
save them money, and keep

95
00:03:40.040 --> 00:03:42.170
humans at a dangerous situations.

96
00:03:42.170 --> 00:03:44.855
I would never have guessed
that there would be companies

97
00:03:44.855 --> 00:03:46.400
dedicated to the study and

98
00:03:46.400 --> 00:03:48.125
application of
reinforce learning.

99
00:03:48.125 --> 00:03:50.440
It's really quite amazing.

100
00:03:50.440 --> 00:03:53.155
However, the more
quickly things change,

101
00:03:53.155 --> 00:03:55.670
the more important it is to
focus on the fundamentals.

102
00:03:55.670 --> 00:03:57.950
Some of the ideas in RL
can be traced as far

103
00:03:57.950 --> 00:04:00.500
back as Pavlov and
his drooling dogs.

104
00:04:00.500 --> 00:04:04.160
Take almost any modern RL system
and take a closer look.

105
00:04:04.160 --> 00:04:06.320
It's built from ideas
and algorithms that

106
00:04:06.320 --> 00:04:09.095
pre-date this specialization
by a decade or two.

107
00:04:09.095 --> 00:04:13.220
For example, at its heart
DQN combines Q learning,

108
00:04:13.220 --> 00:04:15.455
neural networks, and
experienced replay.

109
00:04:15.455 --> 00:04:17.450
Getting good performance
out of these systems,

110
00:04:17.450 --> 00:04:19.970
however, requires
significant innovation.

111
00:04:19.970 --> 00:04:22.535
We would never downplay
the importance of that work.

112
00:04:22.535 --> 00:04:24.440
In fact, we want you to come to

113
00:04:24.440 --> 00:04:27.680
understand these complex
enlarged learning systems.

114
00:04:27.680 --> 00:04:31.575
To do so you first need
to understand the basics.

115
00:04:31.575 --> 00:04:35.405
That's where this course
starts, with the fundamentals.

116
00:04:35.405 --> 00:04:38.000
In this specialization,
we will cover most of

117
00:04:38.000 --> 00:04:41.090
the main ideas used
in modern RL systems.

118
00:04:41.090 --> 00:04:43.190
By the end, you'll implement

119
00:04:43.190 --> 00:04:44.780
a neural network
learning system to

120
00:04:44.780 --> 00:04:47.075
solve an infinite
state control task.

121
00:04:47.075 --> 00:04:49.490
But we will start
with small problems.

122
00:04:49.490 --> 00:04:52.384
We'll spend time discussing
the fundamental challenges

123
00:04:52.384 --> 00:04:53.960
in reinforced learning and

124
00:04:53.960 --> 00:04:55.805
our best ideas for solving them.

125
00:04:55.805 --> 00:04:58.280
The problem of sequential
decision making represents

126
00:04:58.280 --> 00:05:01.025
one of the greatest prizes
of our generation.

127
00:05:01.025 --> 00:05:02.660
It seems appropriate to take

128
00:05:02.660 --> 00:05:04.955
our time and get
the details right.

129
00:05:04.955 --> 00:05:07.730
We begin our study
with a special case of

130
00:05:07.730 --> 00:05:09.350
the reinforcement
learning problem

131
00:05:09.350 --> 00:05:11.105
called multi-arm bandits.

132
00:05:11.105 --> 00:05:13.670
The agent must decide
which choice generates

133
00:05:13.670 --> 00:05:16.685
the best outcome or
reward on average.

134
00:05:16.685 --> 00:05:18.020
This simpler instance of

135
00:05:18.020 --> 00:05:20.510
the full RL problem
is perhaps the best

136
00:05:20.510 --> 00:05:22.430
setting to understand and solve

137
00:05:22.430 --> 00:05:24.905
challenges fundamental to RL.

138
00:05:24.905 --> 00:05:26.720
This module provides

139
00:05:26.720 --> 00:05:28.703
an introduction to
estimating values,

140
00:05:28.703 --> 00:05:30.245
incremental learning,

141
00:05:30.245 --> 00:05:34.460
exploration, non-stationarity,
and parameter tuning.

142
00:05:34.460 --> 00:05:37.460
We continue to develop and
combine these ideas in

143
00:05:37.460 --> 00:05:40.025
different ways over
the next four courses.

144
00:05:40.025 --> 00:05:42.060
Let's get started.