WEBVTT

1
00:00:06.020 --> 00:00:09.435
This week, we introduce MDPs.

2
00:00:09.435 --> 00:00:12.525
When you first heard about
MDPs, you might have thought,

3
00:00:12.525 --> 00:00:14.430
how can this be
the problem formulation

4
00:00:14.430 --> 00:00:16.335
used for all of
reinforcement learning?

5
00:00:16.335 --> 00:00:18.750
It's too simple. By now,

6
00:00:18.750 --> 00:00:20.220
maybe you're starting to see that

7
00:00:20.220 --> 00:00:22.380
it can represent many problems.

8
00:00:22.380 --> 00:00:25.200
To recap, MDPs formalize

9
00:00:25.200 --> 00:00:28.335
the problem of an agent
interacting with an environment.

10
00:00:28.335 --> 00:00:29.895
The agent and environment

11
00:00:29.895 --> 00:00:31.980
interact at discrete time steps.

12
00:00:31.980 --> 00:00:33.950
At each time, the agent

13
00:00:33.950 --> 00:00:36.605
observes the current state
of the environment.

14
00:00:36.605 --> 00:00:38.195
Based on this state,

15
00:00:38.195 --> 00:00:40.280
the agent selects an action.

16
00:00:40.280 --> 00:00:42.770
After that, the
environment transitions

17
00:00:42.770 --> 00:00:45.625
to a new state and
emits a reward.

18
00:00:45.625 --> 00:00:48.000
Remember, the
agent's choices have

19
00:00:48.000 --> 00:00:49.815
long-term consequences.

20
00:00:49.815 --> 00:00:51.830
The action it selects influences

21
00:00:51.830 --> 00:00:54.590
future states and rewards.

22
00:00:54.590 --> 00:00:57.050
The goal of reinforced
learning is to

23
00:00:57.050 --> 00:00:59.990
maximize total future reward.

24
00:00:59.990 --> 00:01:02.270
This often means balancing

25
00:01:02.270 --> 00:01:03.590
immediate reward with

26
00:01:03.590 --> 00:01:06.275
the long-term
consequences of actions.

27
00:01:06.275 --> 00:01:09.649
We formalize this goal
with the expected return,

28
00:01:09.649 --> 00:01:13.940
which is the expected discounted
sum of future rewards.

29
00:01:13.940 --> 00:01:16.715
By discounting with
Gamma less than one,

30
00:01:16.715 --> 00:01:20.640
we can guarantee
the return remains finite.

31
00:01:20.690 --> 00:01:24.740
The precise value of Gamma
defines how much we care

32
00:01:24.740 --> 00:01:28.535
about short-term rewards
versus long-term rewards.

33
00:01:28.535 --> 00:01:30.290
We discussed a number of

34
00:01:30.290 --> 00:01:32.360
examples of problems
that can be naturally

35
00:01:32.360 --> 00:01:36.320
formulated as either
episodic or continuing MDPs.

36
00:01:36.320 --> 00:01:38.510
The MDP formalism can be used to

37
00:01:38.510 --> 00:01:41.105
model many real-world problems.

38
00:01:41.105 --> 00:01:44.275
The first step in applying
reinforcement learning

39
00:01:44.275 --> 00:01:48.815
will always be to formulate
the problem as an MDP.

40
00:01:48.815 --> 00:01:52.920
That's all for this week.
See you next time.