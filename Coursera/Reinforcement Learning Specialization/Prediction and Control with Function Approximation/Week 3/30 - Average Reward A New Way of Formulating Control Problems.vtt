WEBVTT

1
00:00:04.760 --> 00:00:06.795
The need to balance

2
00:00:06.795 --> 00:00:09.300
exploration and
exploitation is one of

3
00:00:09.300 --> 00:00:11.040
the defining characteristics of

4
00:00:11.040 --> 00:00:13.305
the sequential
decision-making problem.

5
00:00:13.305 --> 00:00:15.780
We've talked about
several simple ways to promote

6
00:00:15.780 --> 00:00:17.250
exploration in bandits

7
00:00:17.250 --> 00:00:19.140
and tabular
reinforcement learning.

8
00:00:19.140 --> 00:00:21.150
What about function
approximation?

9
00:00:21.150 --> 00:00:24.105
Is there anything special
about exploration there?

10
00:00:24.105 --> 00:00:25.890
Today we'll find out.

11
00:00:25.890 --> 00:00:27.960
By the end of this video,

12
00:00:27.960 --> 00:00:29.370
you'll be able to describe

13
00:00:29.370 --> 00:00:31.230
how optimistic initial values and

14
00:00:31.230 --> 00:00:34.305
Epsilon greedy can be used
with function approximation.

15
00:00:34.305 --> 00:00:37.215
Let's do a brief refresher
on how we use

16
00:00:37.215 --> 00:00:40.285
optimistic initial values
in the tabular setting.

17
00:00:40.285 --> 00:00:42.080
We initialize our values to be

18
00:00:42.080 --> 00:00:43.925
greater than the true values.

19
00:00:43.925 --> 00:00:45.740
This is like the agent imagining

20
00:00:45.740 --> 00:00:47.420
that it can get more reward by

21
00:00:47.420 --> 00:00:51.335
taking that action than it
actually can in reality.

22
00:00:51.335 --> 00:00:54.620
Typically, initializing
the value function this way

23
00:00:54.620 --> 00:00:56.660
causes the agent
to systematically

24
00:00:56.660 --> 00:00:59.060
explore the state action space.

25
00:00:59.060 --> 00:01:01.820
As the agents values
become more accurate,

26
00:01:01.820 --> 00:01:05.060
they are impacted less and
less by this initialization.

27
00:01:05.060 --> 00:01:07.010
This is straightforward
to implement in

28
00:01:07.010 --> 00:01:09.215
a tabular setting
where the update to

29
00:01:09.215 --> 00:01:10.610
each state action pair is

30
00:01:10.610 --> 00:01:14.120
independent of all the
other state action pairs.

31
00:01:14.120 --> 00:01:16.085
In function approximation,

32
00:01:16.085 --> 00:01:18.410
optimistic initial
values corresponds to

33
00:01:18.410 --> 00:01:20.270
initializing
the weights such that

34
00:01:20.270 --> 00:01:22.515
the resulting values
are optimistic.

35
00:01:22.515 --> 00:01:25.755
In some cases this is
straightforward, for example,

36
00:01:25.755 --> 00:01:27.265
when the features are binary,

37
00:01:27.265 --> 00:01:29.180
we simply initialize
each weight to

38
00:01:29.180 --> 00:01:31.150
be the largest possible return.

39
00:01:31.150 --> 00:01:33.200
Then, as long as each state

40
00:01:33.200 --> 00:01:35.315
has at least one feature active,

41
00:01:35.315 --> 00:01:39.125
the value will be optimistic
and likely overly so.

42
00:01:39.125 --> 00:01:41.090
In many cases however,

43
00:01:41.090 --> 00:01:43.730
it is difficult to
initialize optimistically.

44
00:01:43.730 --> 00:01:47.030
For example, in a neural network
the relationship

45
00:01:47.030 --> 00:01:48.620
between the final values and

46
00:01:48.620 --> 00:01:51.020
the features can be
quite complicated.

47
00:01:51.020 --> 00:01:54.920
Imagine a network composed of
tanh activation functions.

48
00:01:54.920 --> 00:01:57.244
The network could
output negative values

49
00:01:57.244 --> 00:01:59.675
even with positive
initial weights.

50
00:01:59.675 --> 00:02:01.940
But this isn't the whole story.

51
00:02:01.940 --> 00:02:04.685
Depending on how
our features generalize,

52
00:02:04.685 --> 00:02:06.440
optimistic initial values may

53
00:02:06.440 --> 00:02:07.820
not result in the same kind of

54
00:02:07.820 --> 00:02:11.305
systematic exploration we
see in the tabular case.

55
00:02:11.305 --> 00:02:13.225
Consider an extreme example,

56
00:02:13.225 --> 00:02:16.940
where we have only one feature
that is always one.

57
00:02:16.940 --> 00:02:19.670
We can initialize
optimistically but

58
00:02:19.670 --> 00:02:23.195
every update will change
the value for all states.

59
00:02:23.195 --> 00:02:26.440
This means that before some
states are even visited,

60
00:02:26.440 --> 00:02:28.535
the value will already
have decreased

61
00:02:28.535 --> 00:02:31.595
such that it is no
longer optimistic.

62
00:02:31.595 --> 00:02:34.460
To facilitate
systematic exploration,

63
00:02:34.460 --> 00:02:37.910
changes to the value function
need to be more localized.

64
00:02:37.910 --> 00:02:40.530
For example, function
approximation with

65
00:02:40.530 --> 00:02:44.120
tile coding can produce
such localized updates.

66
00:02:44.120 --> 00:02:47.330
Neural networks and also
provide local updates,

67
00:02:47.330 --> 00:02:51.365
but neural networks may also
generalize aggressively.

68
00:02:51.365 --> 00:02:54.475
In practice without
special consideration,

69
00:02:54.475 --> 00:02:55.970
a neural network will lose

70
00:02:55.970 --> 00:02:58.880
his optimism relatively quickly.

71
00:02:58.880 --> 00:03:02.480
Epsilon greedy is generally
applicable and easy to

72
00:03:02.480 --> 00:03:06.215
use even in cases with non-linear
function approximation.

73
00:03:06.215 --> 00:03:08.405
The only thing
Epsilon greedy needs

74
00:03:08.405 --> 00:03:09.920
are the action value estimates,

75
00:03:09.920 --> 00:03:13.610
independent of how they are
initialized or approximated.

76
00:03:13.610 --> 00:03:15.380
However, Epsilon greedy is not

77
00:03:15.380 --> 00:03:17.120
a directed exploration method.

78
00:03:17.120 --> 00:03:19.220
It relies on
randomness to discover

79
00:03:19.220 --> 00:03:21.080
better actions near states

80
00:03:21.080 --> 00:03:22.970
followed by the current policy.

81
00:03:22.970 --> 00:03:24.920
It is therefore not
as systematic as

82
00:03:24.920 --> 00:03:27.650
exploration methods
that rely on optimism.

83
00:03:27.650 --> 00:03:30.170
Improving exploration in
the function approximation

84
00:03:30.170 --> 00:03:32.480
setting remains
an open research question.

85
00:03:32.480 --> 00:03:33.920
So in this course,

86
00:03:33.920 --> 00:03:36.670
we'll stick with
this simple strategy.

87
00:03:36.670 --> 00:03:39.080
In this video, we
talked about how there

88
00:03:39.080 --> 00:03:40.865
are many subtleties
when combining

89
00:03:40.865 --> 00:03:42.470
optimistic initial values and

90
00:03:42.470 --> 00:03:44.390
function approximation and how

91
00:03:44.390 --> 00:03:45.830
Epsilon greedy can
be combined with

92
00:03:45.830 --> 00:03:49.570
any function approximation.
See you next time.