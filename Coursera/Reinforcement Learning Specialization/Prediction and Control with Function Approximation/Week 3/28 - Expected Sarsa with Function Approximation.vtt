WEBVTT

1
00:00:04.880 --> 00:00:07.290
So far we've talked about using

2
00:00:07.290 --> 00:00:08.910
Sarsa with Function
Approximation.

3
00:00:08.910 --> 00:00:10.440
Let's continue
our journey through

4
00:00:10.440 --> 00:00:14.130
approximate control methods
and look at expected Sarsa.

5
00:00:14.130 --> 00:00:16.035
By the end of this video,

6
00:00:16.035 --> 00:00:17.880
you'll be able to
explain the update for

7
00:00:17.880 --> 00:00:20.115
expected Sarsa with
function approximation,

8
00:00:20.115 --> 00:00:22.050
and explain the update
for Q-learning

9
00:00:22.050 --> 00:00:24.180
with function approximation.

10
00:00:24.180 --> 00:00:26.910
Let's see how we
can turn Sarsa into

11
00:00:26.910 --> 00:00:29.985
expected Sarsa when using
function approximation.

12
00:00:29.985 --> 00:00:32.535
Recall the update
equation for Sarsa.

13
00:00:32.535 --> 00:00:34.620
Sarsa's update target includes

14
00:00:34.620 --> 00:00:37.645
the action value for
the next state in action.

15
00:00:37.645 --> 00:00:40.770
Now, recall that
expected Sarsa instead uses

16
00:00:40.770 --> 00:00:43.820
the expectation over
its target policy.

17
00:00:43.820 --> 00:00:45.250
To compute the expectation,

18
00:00:45.250 --> 00:00:47.420
we simply sum over
the action values

19
00:00:47.420 --> 00:00:50.900
weighted by their probability
under the target policy.

20
00:00:50.900 --> 00:00:53.210
The same expectation can be

21
00:00:53.210 --> 00:00:56.255
computed even when we use
function approximation.

22
00:00:56.255 --> 00:00:58.705
Let's see how to do this.

23
00:00:58.705 --> 00:01:00.810
First, recall the update for

24
00:01:00.810 --> 00:01:02.840
Sarsa with function
approximation.

25
00:01:02.840 --> 00:01:05.300
It looks similar to the
tabular setting except

26
00:01:05.300 --> 00:01:06.380
the action value estimates are

27
00:01:06.380 --> 00:01:07.870
parameterized by
the weight factor,

28
00:01:07.870 --> 00:01:10.700
W. We also have

29
00:01:10.700 --> 00:01:11.930
a gradient term to distribute

30
00:01:11.930 --> 00:01:14.570
the error to the weights
appropriately.

31
00:01:14.570 --> 00:01:17.150
Expected Sarsa with
function approximation

32
00:01:17.150 --> 00:01:18.620
follows a similar structure.

33
00:01:18.620 --> 00:01:20.480
We compute the action values from

34
00:01:20.480 --> 00:01:23.575
our weight vector for
every action in the next state.

35
00:01:23.575 --> 00:01:27.560
Then we compute this expectation
under the target policy.

36
00:01:27.560 --> 00:01:29.660
That's all we have
to do to change

37
00:01:29.660 --> 00:01:31.745
Sarsa into expected Sarsa.

38
00:01:31.745 --> 00:01:33.590
Now, how do we do
this for Q-learning

39
00:01:33.590 --> 00:01:35.015
with function approximation?

40
00:01:35.015 --> 00:01:36.410
Fortunately, Q-learning is

41
00:01:36.410 --> 00:01:38.350
a special case of expected Sarsa.

42
00:01:38.350 --> 00:01:40.730
In Q-learning, the target
policy is greedy with

43
00:01:40.730 --> 00:01:43.595
respect to the approximate
action values.

44
00:01:43.595 --> 00:01:46.370
Computing the expectation
under greedy policy

45
00:01:46.370 --> 00:01:49.375
is the same as computing
the maximum action value.

46
00:01:49.375 --> 00:01:51.120
So the Q-learning update with

47
00:01:51.120 --> 00:01:54.260
function approximation is
actually quite straightforward.

48
00:01:54.260 --> 00:01:57.815
We just use the max in
place of the expectation.

49
00:01:57.815 --> 00:02:00.170
That's it for this short video.

50
00:02:00.170 --> 00:02:01.625
Today, we introduced

51
00:02:01.625 --> 00:02:03.980
expected Sarsa with
function approximation,

52
00:02:03.980 --> 00:02:05.720
and we also showed
how to extend it to

53
00:02:05.720 --> 00:02:07.970
Q-learning with
function approximation.

54
00:02:07.970 --> 00:02:09.800
Next time we'll think
about how to do

55
00:02:09.800 --> 00:02:11.780
exploration with
function approximation.

56
00:02:11.780 --> 00:02:14.610
Until then, don't get lost.