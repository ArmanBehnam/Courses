WEBVTT

1
00:00:04.730 --> 00:00:08.490
This week, we discussed methods
for representing large,

2
00:00:08.490 --> 00:00:10.755
impossibly continuous
state spaces.

3
00:00:10.755 --> 00:00:13.290
Ways to construct features.

4
00:00:13.290 --> 00:00:15.060
A representation is

5
00:00:15.060 --> 00:00:17.400
an agent's internal
encoding of the state,

6
00:00:17.400 --> 00:00:19.020
the agent constructs features to

7
00:00:19.020 --> 00:00:20.940
summarize the current input.

8
00:00:20.940 --> 00:00:22.590
Whenever we are talking about

9
00:00:22.590 --> 00:00:24.330
features and
representation learning,

10
00:00:24.330 --> 00:00:26.715
we are in the land of
function approximation.

11
00:00:26.715 --> 00:00:30.210
This brings us to the left side
of our course-map.

12
00:00:30.210 --> 00:00:32.835
First, we introduce
course coding,

13
00:00:32.835 --> 00:00:35.250
course coding is related
to state aggregation,

14
00:00:35.250 --> 00:00:37.605
it groups together
neighboring states where

15
00:00:37.605 --> 00:00:40.370
each grouping can have
an arbitrary shape,

16
00:00:40.370 --> 00:00:42.390
one particular example of

17
00:00:42.390 --> 00:00:43.880
a two-dimensional
course coding is

18
00:00:43.880 --> 00:00:46.505
represented by
these overlapping circles.

19
00:00:46.505 --> 00:00:48.830
Each circle is a feature that is

20
00:00:48.830 --> 00:00:50.915
one when the state is
inside the circle,

21
00:00:50.915 --> 00:00:53.750
and zero when the state
is outside the circle.

22
00:00:53.750 --> 00:00:56.030
Next, we discuss
a particular type

23
00:00:56.030 --> 00:00:57.920
of course coding
called tile coding,

24
00:00:57.920 --> 00:00:59.600
Tile coding generates features

25
00:00:59.600 --> 00:01:01.825
using a set of overlapping grid,

26
00:01:01.825 --> 00:01:03.975
each grid is called a tiling.

27
00:01:03.975 --> 00:01:05.270
A tiling itself has

28
00:01:05.270 --> 00:01:07.685
no overlap or space
between the squares,

29
00:01:07.685 --> 00:01:10.475
only one feature can
be active at a time.

30
00:01:10.475 --> 00:01:13.790
By stacking multiple
offset tilings,

31
00:01:13.790 --> 00:01:16.370
we can discriminate
between different states.

32
00:01:16.370 --> 00:01:18.470
The shape, size, and number of

33
00:01:18.470 --> 00:01:20.855
tilings help us balance
generalization,

34
00:01:20.855 --> 00:01:23.945
discrimination, and
computational efficiency.

35
00:01:23.945 --> 00:01:26.195
We then discuss a way to learn

36
00:01:26.195 --> 00:01:28.760
the representation online
with neural networks.

37
00:01:28.760 --> 00:01:30.485
With course coding techniques,

38
00:01:30.485 --> 00:01:32.890
the representation is
fixed before learning.

39
00:01:32.890 --> 00:01:35.330
A feed-forward
neural network uses

40
00:01:35.330 --> 00:01:38.090
a series of layers to
produce a representation.

41
00:01:38.090 --> 00:01:40.990
In each layer,
multiple neurons received

42
00:01:40.990 --> 00:01:43.925
the same input and
produce distinct outputs.

43
00:01:43.925 --> 00:01:45.440
These outputs are then fed to

44
00:01:45.440 --> 00:01:48.065
the next layer and
the process repeats.

45
00:01:48.065 --> 00:01:51.545
Each neuron computes its output
by taking a weighted sum

46
00:01:51.545 --> 00:01:53.090
of the inputs and passing

47
00:01:53.090 --> 00:01:55.520
the sum through
an activation function.

48
00:01:55.520 --> 00:01:57.395
To train a neural network,

49
00:01:57.395 --> 00:01:59.960
we use an iterative process
called gradient descent.

50
00:01:59.960 --> 00:02:01.520
We pass the inputs into

51
00:02:01.520 --> 00:02:04.175
the network to
produce predictions,

52
00:02:04.175 --> 00:02:06.230
then we compare
those predictions to

53
00:02:06.230 --> 00:02:09.360
the outputs and compute
our loss function.

54
00:02:09.560 --> 00:02:12.470
Finally, we compute
the derivative of

55
00:02:12.470 --> 00:02:13.760
the loss function and apply

56
00:02:13.760 --> 00:02:15.710
our learning rule to the weights.

57
00:02:15.710 --> 00:02:18.770
Next week, we'll talk
about learning to maximize

58
00:02:18.770 --> 00:02:22.200
reward with function
approximation, see you then.