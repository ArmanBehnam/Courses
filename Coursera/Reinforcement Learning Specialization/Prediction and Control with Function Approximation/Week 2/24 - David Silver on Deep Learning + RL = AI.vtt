WEBVTT

1
00:00:00.000 --> 00:00:07.054
[MUSIC]

2
00:00:07.054 --> 00:00:12.506
Hi, I'm David Silver, and I'm going to
talk about deep reinforcement learning.

3
00:00:12.506 --> 00:00:15.300
So let's remind ourselves why we
care about reinforcement learning.

4
00:00:16.600 --> 00:00:20.276
Reinforcement learning is a very general
purpose framework that lets us think

5
00:00:20.276 --> 00:00:22.958
about all kinds of different
decision making problems.

6
00:00:22.958 --> 00:00:27.447
So it really provides a way to think about
any problem where there's an agent, and

7
00:00:27.447 --> 00:00:29.869
the agent gets to take
actions in the world.

8
00:00:29.869 --> 00:00:33.689
And importantly, that agent's actions
actually affect the world, and

9
00:00:33.689 --> 00:00:38.013
the world in turn changes as a result and
gives some observations back to the agent.

10
00:00:38.013 --> 00:00:41.096
And the agent gets to choose
the actions that maximize

11
00:00:41.096 --> 00:00:45.566
its performance in the world that's
measured by some kind of reward signal.

12
00:00:45.566 --> 00:00:50.368
So this is such a general problem, it can
be applied to all kinds of things, from

13
00:00:50.368 --> 00:00:55.320
robots to games to financial transactions,
whatever you think of reinforcement

14
00:00:55.320 --> 00:01:00.008
learning, it's this problem framework
that lets us do all of those things.

15
00:01:00.008 --> 00:01:04.754
So deep learning is a way to, it's
actually a general-purpose framework to

16
00:01:04.754 --> 00:01:07.317
help us solve a wide variety of problems.

17
00:01:07.317 --> 00:01:10.810
It's a way to think about representation
learning in a very general way.

18
00:01:10.810 --> 00:01:15.581
In particular, if we give it an objective,
then deep learning gives a way to build up

19
00:01:15.581 --> 00:01:18.878
a representation that actually
achieve that objective,

20
00:01:18.878 --> 00:01:22.734
a way to start with these inputs
that kind of filter into the system.

21
00:01:22.734 --> 00:01:26.743
And build up some kind of features that
help to understand those inputs in

22
00:01:26.743 --> 00:01:30.633
a way that helps to solve the problem
using minimal domain knowledge.

23
00:01:30.633 --> 00:01:32.360
You don't have to put a lot
of knowledge into the system.

24
00:01:32.360 --> 00:01:36.665
You just pass something in, like an image
of something like a cat or a dog,

25
00:01:36.665 --> 00:01:40.902
and the system builds up features to
understand patches of this image and

26
00:01:40.902 --> 00:01:42.924
then more complicated features.

27
00:01:42.924 --> 00:01:44.722
And eventually comes up with a decision,
for

28
00:01:44.722 --> 00:01:46.812
example, to say whether this
thing is a cat or a dog.

29
00:01:48.887 --> 00:01:50.957
So if we put these two things together,

30
00:01:50.957 --> 00:01:54.173
deep reinforcement learning is
the combination of the two.

31
00:01:54.173 --> 00:01:57.968
Really, we're trying to seek an agent
that can solve any human level task.

32
00:01:57.968 --> 00:01:59.402
That's our ultimate goal here,

33
00:01:59.402 --> 00:02:02.330
that's why many of us are interested
in artificial intelligence.

34
00:02:02.330 --> 00:02:04.722
And if we want to achieve that goal,
well, how can we do it?

35
00:02:04.722 --> 00:02:08.538
Well, the problem can be defined
by reinforcement learning, but

36
00:02:08.538 --> 00:02:12.574
deep learning maybe gives us
the mechanism that helps us to get there.

37
00:02:12.574 --> 00:02:16.966
So RL is like the problem and
deep learning is more like the solution.

38
00:02:16.966 --> 00:02:18.247
So here's a big conjecture.

39
00:02:18.247 --> 00:02:23.331
And a big conjecture is that RL + DL,
if we put these two things together,

40
00:02:23.331 --> 00:02:28.251
it gives us a recipe that can carry us
maybe most of the way towards our big

41
00:02:28.251 --> 00:02:33.089
goal, our big ambitions of being
able to solve any of these problems.

42
00:02:33.089 --> 00:02:36.875
So let's just dive in a little bit
to understand what deep learning is

43
00:02:36.875 --> 00:02:37.540
all about.

44
00:02:37.540 --> 00:02:43.042
So the main piece of deep learning is
this thing called a neural network.

45
00:02:43.042 --> 00:02:46.000
And a deep neural network really
is just something that's,

46
00:02:46.000 --> 00:02:49.712
you can think of as a multi-layered
function, a compositional function,

47
00:02:49.712 --> 00:02:51.627
a function of a function of a function.

48
00:02:51.627 --> 00:02:55.442
And so if we just unpack that a little
bit, it's made up of a few pieces.

49
00:02:55.442 --> 00:02:58.371
We start off with some input
that goes into the system,

50
00:02:58.371 --> 00:03:02.201
that might be your image describing
whether this thing is a cat or a dog.

51
00:03:02.201 --> 00:03:06.723
And that gets processed by a bunch of
different functions to give you these kind

52
00:03:06.723 --> 00:03:08.653
of internal states or features.

53
00:03:08.653 --> 00:03:12.592
And each of those things takes weights,
these w's, that kind of tell you what that

54
00:03:12.592 --> 00:03:15.649
function should actually,
do the parameters of the function.

55
00:03:15.649 --> 00:03:18.505
And then that gets processed by
a series of different functions.

56
00:03:18.505 --> 00:03:22.018
Until eventually this thing,
this deep neural network outputs,

57
00:03:22.018 --> 00:03:25.843
after all of these different layers
of computation, some kind of output

58
00:03:25.843 --> 00:03:29.685
describing what the system actually
thinks, like is this a cat or a dog.

59
00:03:29.685 --> 00:03:33.127
And as a result of that, there's one
final step, which is an important step,

60
00:03:33.127 --> 00:03:36.480
which defines the objective of the system,
what it's really trying to do.

61
00:03:36.480 --> 00:03:37.587
And that we call the loss.

62
00:03:37.587 --> 00:03:41.991
That's a number saying how well
this system did, typically a scale,

63
00:03:41.991 --> 00:03:47.081
one if you classified this correctly and
zero if you got it wrong, as an example.

64
00:03:47.081 --> 00:03:50.719
Now, the special thing about these deep
neural networks is that you can actually

65
00:03:50.719 --> 00:03:53.300
not just compute these things
in the forward direction.

66
00:03:53.300 --> 00:03:57.773
But you can actually reverse the flow of
all of that information to work out which

67
00:03:57.773 --> 00:04:00.864
of these parameters led to
that loss being good or bad.

68
00:04:00.864 --> 00:04:04.956
We can understand, if I was to tweak these
parameters in a particular direction,

69
00:04:04.956 --> 00:04:08.814
how would those changing parameters
actually lead to the output changing and

70
00:04:08.814 --> 00:04:10.847
lead to that causing a particular loss?

71
00:04:10.847 --> 00:04:14.477
Would it lead to me predicting this
thing being a cat more or a dog more?

72
00:04:14.477 --> 00:04:18.288
And the way we do that is just by
reversing the direction of flow and

73
00:04:18.288 --> 00:04:22.863
using the chain rule of calculus to
compute the gradient of this whole system,

74
00:04:22.863 --> 00:04:25.178
and that is known as backpropagation.

75
00:04:25.178 --> 00:04:29.119
So the main idea of training a deep neural
network is actually just to use this

76
00:04:29.119 --> 00:04:31.043
gradient to adjust the parameters.

77
00:04:31.043 --> 00:04:34.707
So we compute the gradient of the loss
with respect to the parameters.

78
00:04:34.707 --> 00:04:38.768
And that you can think of as kind of
pointing us in the direction that kind of

79
00:04:38.768 --> 00:04:43.420
goes down the hill in terms of, think of
this big landscape that's describing, for

80
00:04:43.420 --> 00:04:47.439
each point in the landscape is one
potential choice of your parameters.

81
00:04:47.439 --> 00:04:50.959
And the gradient kind of tells you which
direction is going to take you down

82
00:04:50.959 --> 00:04:51.839
that landscape,

83
00:04:51.839 --> 00:04:55.372
where the height of that landscape is
really telling you how good it is.

84
00:04:55.372 --> 00:04:59.232
And downhill might be better,
be your lower error.

85
00:04:59.232 --> 00:05:02.841
We want to move down this landscape by
following the gradient down to the point

86
00:05:02.841 --> 00:05:04.127
that has the lowest error.

87
00:05:04.127 --> 00:05:07.468
And so we just move the parameters
in that direction, and

88
00:05:07.468 --> 00:05:11.644
that really is the fundamental idea
behind training a neural network,

89
00:05:11.644 --> 00:05:14.096
that's the whole idea of deep learning.

90
00:05:14.096 --> 00:05:18.156
Okay, so let's get back to RL again to
understand how we can actually put these

91
00:05:18.156 --> 00:05:21.123
pieces together to really try and
do something amazing.

92
00:05:21.123 --> 00:05:22.972
So if we think about RL agents,

93
00:05:22.972 --> 00:05:26.677
it's really useful to understand
the anatomy of an RL agent.

94
00:05:26.677 --> 00:05:30.908
So the way I think about this is that
there's these major components that may or

95
00:05:30.908 --> 00:05:32.638
may not be inside our agent, and

96
00:05:32.638 --> 00:05:36.313
that really defines the kind of
category of what type of agent it is.

97
00:05:36.313 --> 00:05:40.554
So it might have a policy inside it
that determines that agent's behavior,

98
00:05:40.554 --> 00:05:43.880
in which case we say this is
a policy-based approach to RL.

99
00:05:43.880 --> 00:05:47.400
It may have a value function inside it,
which kind of describes the agent's

100
00:05:47.400 --> 00:05:50.878
predictions about how much reward it
will get or potentially other terms.

101
00:05:50.878 --> 00:05:54.264
And in that case,
we call it a value-based RL agent.

102
00:05:54.264 --> 00:05:58.240
And it might have a model inside it,
something saying how the agent thinks

103
00:05:58.240 --> 00:06:01.587
the environment works,
what it thinks is going to happen next.

104
00:06:01.587 --> 00:06:06.806
And so if it has a model inside it,
we say it's a model-based approach to RL.

105
00:06:06.806 --> 00:06:11.114
We can apply the combination of deep
learning and reinforcement learning by

106
00:06:11.114 --> 00:06:15.095
essentially using deep neural
networks as a function approximator.

107
00:06:15.095 --> 00:06:18.752
And that function approximator can be
used to represent any one of these three

108
00:06:18.752 --> 00:06:21.352
kind of major components
of reinforcement learning.

109
00:06:21.352 --> 00:06:25.238
It could be used to represent the policy,
it could be used to represent the value

110
00:06:25.238 --> 00:06:27.678
function, or
it could be to represent the model.

111
00:06:27.678 --> 00:06:29.206
Now, to train this system,

112
00:06:29.206 --> 00:06:32.400
the main idea is really that we
have to pick a loss function.

113
00:06:32.400 --> 00:06:36.158
We have to pick an objective that
describes what we really want in terms

114
00:06:36.158 --> 00:06:40.436
of one of these three kinds of families
of reinforcement learning algorithms.

115
00:06:40.436 --> 00:06:44.231
So if we're doing policy-based
reinforcement learning, then, for example,

116
00:06:44.231 --> 00:06:46.433
we might choose to follow
the policy gradient.

117
00:06:46.433 --> 00:06:49.104
If we're doing value-based
reinforcement learning,

118
00:06:49.104 --> 00:06:52.617
we might choose to minimize the TD error,
the temporal difference error.

119
00:06:52.617 --> 00:06:55.126
And if we were doing model-based
reinforcement learning,

120
00:06:55.126 --> 00:06:57.741
we might choose to minimize
the next-step prediction error.

121
00:06:57.741 --> 00:06:58.575
And so what do we do?

122
00:06:58.575 --> 00:06:59.462
What's the algorithm?

123
00:06:59.462 --> 00:07:00.613
Well, the algorithm is very simple.

124
00:07:00.613 --> 00:07:04.515
Just like in deep learning, all we have to
do is to optimize that loss function by

125
00:07:04.515 --> 00:07:08.829
gradient descent, following this landscape
down and down and down and down, until our

126
00:07:08.829 --> 00:07:12.996
loss gets lower and lower and lower, and
we're doing better and better and better.

127
00:07:12.996 --> 00:07:15.970
So if we put those things together,
it gives us a recipe for

128
00:07:15.970 --> 00:07:19.689
achieving all kinds of interesting
things in reinforcement learning.

129
00:07:19.689 --> 00:07:23.739
And some of those have actually led to
some of the exciting success stories

130
00:07:23.739 --> 00:07:25.643
that we've seen in recent years.

131
00:07:25.643 --> 00:07:30.359
So for example, it's possible to use
a deep reinforcement learning approach

132
00:07:30.359 --> 00:07:32.867
to try and play games to very high levels.

133
00:07:32.867 --> 00:07:38.087
So we've seen agents that were able
to defeat the human world champions

134
00:07:38.087 --> 00:07:43.230
in the game of Go, and indeed in
other games, like chess and so forth.

135
00:07:43.230 --> 00:07:45.472
Or taking this into the modern era,

136
00:07:45.472 --> 00:07:48.846
you might want to play against
in video games as well.

137
00:07:48.846 --> 00:07:53.620
So we have deep reinforcement learning
agents that are able to play 50 or

138
00:07:53.620 --> 00:07:57.316
more completely different
games from the Atari system.

139
00:07:57.316 --> 00:08:02.275
And recently we've had a system which was
able to play the very challenging game

140
00:08:02.275 --> 00:08:03.244
of StarCraft.

141
00:08:03.244 --> 00:08:06.983
This is one of the most challenging
video games for humans.

142
00:08:06.983 --> 00:08:09.545
But it's not just games
that these things can do,

143
00:08:09.545 --> 00:08:12.540
they can have impact on all
kinds of applications as well.

144
00:08:12.540 --> 00:08:16.677
So for example, we've seen deep
reinforcement learning agents that

145
00:08:16.677 --> 00:08:20.477
can learn to manipulate objects
with a great deal of dexterity.

146
00:08:20.477 --> 00:08:24.919
We've seen deep reinforcement learning
agents which were able to solve

147
00:08:24.919 --> 00:08:29.303
chemical synthesis problems by
an approach known as retrosynthesis.

148
00:08:29.303 --> 00:08:33.401
Or we've seen deep reinforcement
learning agents that actually go out and

149
00:08:33.401 --> 00:08:34.766
trade in the real world,

150
00:08:34.766 --> 00:08:39.007
like actually algorithmically deciding
what to do on these trading floors.

151
00:08:39.007 --> 00:08:43.596
So in conclusion, when we combine
the problem of reinforcement learning with

152
00:08:43.596 --> 00:08:48.252
the solution approach of deep learning,
and we use these deep learning methods

153
00:08:48.252 --> 00:08:52.232
to provide us with a powerful tool kit for
function approximation.

154
00:08:52.232 --> 00:08:55.861
We actually have a very flexible and
powerful approach that can take us a long

155
00:08:55.861 --> 00:08:59.000
way towards solving many of
the problems that we care about in AI.