WEBVTT

1
00:00:00.000 --> 00:00:05.423
[MUSIC]

2
00:00:05.423 --> 00:00:09.730
Previously we looked at how value
function estimation can be framed as

3
00:00:09.730 --> 00:00:11.192
supervised learning.

4
00:00:11.192 --> 00:00:14.708
We made this precise by
specifying an objective.

5
00:00:14.708 --> 00:00:18.829
Now we want to figure out how to
find solutions to this objective.

6
00:00:18.829 --> 00:00:21.645
Today we'll talk about
a general strategy for

7
00:00:21.645 --> 00:00:24.760
minimizing objectives
called gradient descent.

8
00:00:24.760 --> 00:00:28.957
In the following videos,
we'll cover specific algorithms for

9
00:00:28.957 --> 00:00:31.919
the objectives in reinforcement learning.

10
00:00:31.919 --> 00:00:35.374
After watching this video,
you will be able to,

11
00:00:35.374 --> 00:00:38.662
understand the idea of gradient descent,
and

12
00:00:38.662 --> 00:00:43.568
understand that gradient descent
converges to stationary points.

13
00:00:43.568 --> 00:00:46.204
We want to minimize the mean
squared value error,

14
00:00:46.204 --> 00:00:49.484
to make our value estimates close
to the true value function.

15
00:00:49.484 --> 00:00:54.496
Recall that our value estimate is
given by a function of the state,

16
00:00:54.496 --> 00:00:58.534
parameterised by a set of
real-valued weights, W.

17
00:00:58.534 --> 00:01:02.451
These ways determine how the value's
computed for each state,

18
00:01:02.451 --> 00:01:06.741
changing the weights will modify
the value estimate for many states.

19
00:01:06.741 --> 00:01:10.595
We have to think about how to change
the weights to minimize the overall

20
00:01:10.595 --> 00:01:11.391
value error.

21
00:01:11.391 --> 00:01:15.194
To do that,
we'll need a little bit of calculus.

22
00:01:15.194 --> 00:01:19.492
Hopefully, you remember the idea
of a derivative from calculus.

23
00:01:19.492 --> 00:01:24.425
Here we've plotted a function f,
with scalar parameter W.

24
00:01:24.425 --> 00:01:30.779
The derivative tells us how to locally
change W to increase or decrease f.

25
00:01:30.779 --> 00:01:35.034
The sign of the derivative of
f at a particular point W,

26
00:01:35.034 --> 00:01:39.022
indicates the direction to
change W to increase f.

27
00:01:39.022 --> 00:01:43.215
The magnitude of the derivative
indicates the slope of

28
00:01:43.215 --> 00:01:45.499
the function f at the point W.

29
00:01:45.499 --> 00:01:49.908
That is,
how quickly f will change as we vary W.

30
00:01:49.908 --> 00:01:52.174
Here the derivative is positive.

31
00:01:52.174 --> 00:01:56.593
If we move W in this direction,
we increase f.

32
00:01:56.593 --> 00:01:59.823
At this other point,
the derivative is negative,

33
00:01:59.823 --> 00:02:02.611
it is pointing in
a direction to decrease W.

34
00:02:02.611 --> 00:02:07.080
But this direction still
indicates how to increase f.

35
00:02:07.080 --> 00:02:10.945
If we move W in this direction,
it increases f.

36
00:02:10.945 --> 00:02:17.008
If we were to move it in the negative
of this direction, it would decrease f.

37
00:02:17.008 --> 00:02:21.866
If f is privatized by more than
one variable, then W is a vector.

38
00:02:21.866 --> 00:02:26.654
In this case, we need to introduce
the idea of a gradient to

39
00:02:26.654 --> 00:02:30.587
describe how f changes
as the vector W changes.

40
00:02:30.587 --> 00:02:34.099
The gradient is a vector
of partial derivatives,

41
00:02:34.099 --> 00:02:39.377
indicating how a local change in each
component of W affects the function.

42
00:02:39.377 --> 00:02:44.086
Notice that the gradient has to be
the same size as the weight vector.

43
00:02:44.086 --> 00:02:48.304
The sign of each component of
the gradient specifies the direction to

44
00:02:48.304 --> 00:02:52.167
change the associated component of W,
in order to increase f.

45
00:02:52.167 --> 00:02:55.167
The direction is either positive or
negative.

46
00:02:55.167 --> 00:02:59.978
The magnitude of the component
specifies how quickly f changes,

47
00:02:59.978 --> 00:03:02.172
as W moves in that direction.

48
00:03:02.172 --> 00:03:06.464
If the function is very steep in one
dimension, the magnitude will be high.

49
00:03:06.464 --> 00:03:12.180
If it is very flat the magnitude of the
gradient in that dimension will be low.

50
00:03:12.180 --> 00:03:16.558
Regardless of the magnitude, the gradient
gives the direction of steepest descent.

51
00:03:16.558 --> 00:03:23.298
It provides the direction to change W, so
that locally f is maximally increased.

52
00:03:23.298 --> 00:03:28.187
To give a specific example, let's derive
the gradient of a linear value function.

53
00:03:28.187 --> 00:03:33.122
Remember, a linear value approximation
is just an inner product of the weights

54
00:03:33.122 --> 00:03:35.563
with the feature vector for the state.

55
00:03:35.563 --> 00:03:39.331
The partial derivative of that
with respect to a single weight,

56
00:03:39.331 --> 00:03:42.277
is just the feature
associated with that weight.

57
00:03:42.277 --> 00:03:46.081
Remember, the features themselves
do not depend on the weights.

58
00:03:46.081 --> 00:03:50.679
This means the gradient of a linear
value approximation is simply

59
00:03:50.679 --> 00:03:53.194
the feature vector for that state.

60
00:03:53.194 --> 00:03:55.996
Our objective is
a function of the weights.

61
00:03:55.996 --> 00:04:00.979
For example, the mean squared value error
is a function of the weights because it

62
00:04:00.979 --> 00:04:04.732
is a function of V hat, and
V hat is a function of the weights.

63
00:04:04.732 --> 00:04:09.478
Our goal is to adjust the weights
to make the objective small.

64
00:04:09.478 --> 00:04:12.202
Here we have a hypothetical
plot of our objective.

65
00:04:12.202 --> 00:04:13.674
To keep things simple,

66
00:04:13.674 --> 00:04:17.433
let's say our function is
parametrized by a single weight.

67
00:04:17.433 --> 00:04:20.283
The x axis corresponds to this weight, and

68
00:04:20.283 --> 00:04:23.775
the y-axis to the objective value for
this weight.

69
00:04:23.775 --> 00:04:25.581
Because we have a single weight,

70
00:04:25.581 --> 00:04:28.492
this means our gradient is
just a scalar derivative.

71
00:04:28.492 --> 00:04:30.963
If we want to decrease
our objective function,

72
00:04:30.963 --> 00:04:34.935
we should move the weights in the
direction of the negative of the gradient.

73
00:04:34.935 --> 00:04:38.568
This is the idea of gradient descent.

74
00:04:38.568 --> 00:04:42.451
Here's the gradient descent update
rule that captures this intuition.

75
00:04:42.451 --> 00:04:46.818
We make small changes in the direction
that will most reduce the objective.

76
00:04:46.818 --> 00:04:50.912
We use a step size parameter,
alpha, to control how far we move,

77
00:04:50.912 --> 00:04:53.970
as otherwise there is
a risk of stepping too far.

78
00:04:53.970 --> 00:04:58.717
This is because moving in this direction
is only guaranteed to decrease

79
00:04:58.717 --> 00:05:00.388
the objective locally.

80
00:05:00.388 --> 00:05:04.879
By doing such updates repeatedly
with a sufficiently small alpha,

81
00:05:04.879 --> 00:05:09.935
we will eventually converge to a
stationary point where the gradient is 0.

82
00:05:09.935 --> 00:05:12.117
Likely, this will be a local minimum.

83
00:05:12.117 --> 00:05:17.190
This means these weights are better than
all weights in the immediate vicinity,

84
00:05:17.190 --> 00:05:19.289
but may not be the best possible.

85
00:05:19.289 --> 00:05:23.363
Other possible stationary points
are local maxima and saddle points.

86
00:05:23.363 --> 00:05:27.981
These are unstable solutions the
stochasticity inherent in our algorithms

87
00:05:27.981 --> 00:05:32.683
is usually enough to prevent getting
stuck at these poor stationary points.

88
00:05:32.683 --> 00:05:35.343
On the other hand a local
minima is stable, so

89
00:05:35.343 --> 00:05:38.781
even a stochastic algorithm
will tend to converge there.

90
00:05:38.781 --> 00:05:44.013
For this reason we usually talk about our
algorithms converging to local minima.

91
00:05:44.013 --> 00:05:47.997
In some cases, gradient descent is
guaranteed to converge to the global

92
00:05:47.997 --> 00:05:52.392
minimum, which is the best possible
setting of the weights for the objective.

93
00:05:52.392 --> 00:05:54.317
For example, this is true for

94
00:05:54.317 --> 00:05:58.717
the mean squared value error with
linear function approximation.

95
00:05:58.717 --> 00:06:03.543
For more complex function approximate
errors such as a neural network,

96
00:06:03.543 --> 00:06:06.718
a stationary point may
not be a global minimum.

97
00:06:06.718 --> 00:06:11.385
Note that a global minimum does not
necessarily correspond to the true

98
00:06:11.385 --> 00:06:12.582
value function.

99
00:06:12.582 --> 00:06:16.777
It is limited by our choice of
function parameterization, and

100
00:06:16.777 --> 00:06:19.240
depends on our choice of objective.

101
00:06:19.240 --> 00:06:23.896
Imagine a feature vector which just
contains a single element that is

102
00:06:23.896 --> 00:06:26.957
always one,
no matter what state you are in.

103
00:06:26.957 --> 00:06:31.419
The approximate value function that
minimizes mean squared value error,

104
00:06:31.419 --> 00:06:34.762
will converge to the average
value over all the states.

105
00:06:34.762 --> 00:06:37.298
This is not a very good value function.

106
00:06:37.298 --> 00:06:40.956
However, this is still
the best we can do under that

107
00:06:40.956 --> 00:06:45.138
parameterization in terms of
the value error objective.

108
00:06:45.138 --> 00:06:46.765
That's it for this video.

109
00:06:46.765 --> 00:06:51.746
You should now understand how gradient
descent can be used to find stationary

110
00:06:51.746 --> 00:06:57.120
points of objectives, and that these
solutions are not always globally optimal.

111
00:06:57.120 --> 00:07:00.598
Next time we'll talk about how to use
gradient descent specifically for

112
00:07:00.598 --> 00:07:01.173
our error.

113
00:07:01.173 --> 00:07:01.700
See you then.