WEBVTT

1
00:00:05.360 --> 00:00:07.560
My name is Doina Precup.

2
00:00:07.560 --> 00:00:10.110
I'm a professor at McGill
University and I also

3
00:00:10.110 --> 00:00:13.485
work DeepMind as a
research scientist.

4
00:00:13.485 --> 00:00:16.140
A lot of my research
interests have to

5
00:00:16.140 --> 00:00:18.975
do with abstraction in
reinforcement learning.

6
00:00:18.975 --> 00:00:21.090
Reinforcement
learning agents have

7
00:00:21.090 --> 00:00:23.280
to acquire two
kinds of knowledge.

8
00:00:23.280 --> 00:00:25.860
One, is what we would call
procedural knowledge,

9
00:00:25.860 --> 00:00:27.450
how to do things, and the policy

10
00:00:27.450 --> 00:00:29.220
is the most basic
example of this.

11
00:00:29.220 --> 00:00:30.870
But actually, there's lots of

12
00:00:30.870 --> 00:00:33.720
other pieces of knowledge
we might want to have,

13
00:00:33.720 --> 00:00:34.770
like how to interact with

14
00:00:34.770 --> 00:00:36.300
particular objects or how to

15
00:00:36.300 --> 00:00:38.385
travel between different places.

16
00:00:38.385 --> 00:00:40.740
So I will talk about
this in terms of

17
00:00:40.740 --> 00:00:44.305
skills or options or sometimes
goal-driven behavior.

18
00:00:44.305 --> 00:00:47.330
We also would like to have
predictive knowledge or

19
00:00:47.330 --> 00:00:49.790
empirical knowledge which means,

20
00:00:49.790 --> 00:00:53.690
what might happen conditioned
on an agent's behavior?

21
00:00:53.690 --> 00:00:56.630
This is value functions but

22
00:00:56.630 --> 00:01:00.305
also other things like
for example models.

23
00:01:00.305 --> 00:01:03.940
Now, there is certain
characteristics we would

24
00:01:03.940 --> 00:01:07.100
like to have for these kinds
of pieces of knowledge.

25
00:01:07.100 --> 00:01:09.800
One is we would like this
knowledge to be learnable,

26
00:01:09.800 --> 00:01:12.095
so that we can
acquire it from data,

27
00:01:12.095 --> 00:01:14.540
we would also like
it to be expressive.

28
00:01:14.540 --> 00:01:17.180
We would like for
the agent to be able

29
00:01:17.180 --> 00:01:19.130
to know about

30
00:01:19.130 --> 00:01:22.010
many different things and
different circumstances,

31
00:01:22.010 --> 00:01:23.570
and we would like
the agent to be able

32
00:01:23.570 --> 00:01:24.950
to compose pieces of knowledge

33
00:01:24.950 --> 00:01:28.160
that it already has
into larger pieces.

34
00:01:28.160 --> 00:01:30.650
Those are the main things

35
00:01:30.650 --> 00:01:33.415
for our knowledge representation.

36
00:01:33.415 --> 00:01:36.645
Knowledge representation has to

37
00:01:36.645 --> 00:01:39.330
generalize both in terms of

38
00:01:39.330 --> 00:01:43.910
the agent's behavior over time
scales as well as in terms

39
00:01:43.910 --> 00:01:46.040
of the agents capacity to reason

40
00:01:46.040 --> 00:01:48.575
about the world about
its perceptions.

41
00:01:48.575 --> 00:01:50.180
So that's where state

42
00:01:50.180 --> 00:01:53.380
abstraction function
approximation come in.

43
00:01:53.380 --> 00:01:55.760
I will talk a little bit more

44
00:01:55.760 --> 00:01:57.980
about generalization in terms

45
00:01:57.980 --> 00:02:02.180
of procedural knowledge over
the different timescales.

46
00:02:02.180 --> 00:02:05.480
Now, why might we want to
have something like this?

47
00:02:05.480 --> 00:02:09.625
Well, a lot of our agents
when they are "born",

48
00:02:09.625 --> 00:02:11.240
they have a very limited set of

49
00:02:11.240 --> 00:02:13.580
actions and these
actions always last one

50
00:02:13.580 --> 00:02:15.365
time step because we want to fit

51
00:02:15.365 --> 00:02:17.920
with the markup decision
process framework.

52
00:02:17.920 --> 00:02:20.520
But in realistic circumstances,

53
00:02:20.520 --> 00:02:22.425
agents might need to

54
00:02:22.425 --> 00:02:25.095
keep a course of action
for many time steps.

55
00:02:25.095 --> 00:02:26.780
For example, if
you're thinking of

56
00:02:26.780 --> 00:02:28.975
going from Montreal to Edmonton,

57
00:02:28.975 --> 00:02:31.050
you don't want to think in
terms of muscle twitches,

58
00:02:31.050 --> 00:02:32.810
you'd like to think in terms of

59
00:02:32.810 --> 00:02:35.050
taking a cab to the
airport and then fly to

60
00:02:35.050 --> 00:02:36.530
Edmonton and taking a cab to

61
00:02:36.530 --> 00:02:39.920
the university and maybe
grabbing dinner in the meantime.

62
00:02:39.920 --> 00:02:44.175
These are periods of
time that are extended,

63
00:02:44.175 --> 00:02:45.480
they are not the same.

64
00:02:45.480 --> 00:02:46.670
But the agent should be able to

65
00:02:46.670 --> 00:02:48.290
reason at this level because

66
00:02:48.290 --> 00:02:51.590
that means its plan will
now be much shorter,

67
00:02:51.590 --> 00:02:53.360
only four steps instead of

68
00:02:53.360 --> 00:02:55.505
many thousands of
steps potentially,

69
00:02:55.505 --> 00:02:57.440
and also the plan might

70
00:02:57.440 --> 00:02:59.720
be reusable in other
circumstances.

71
00:02:59.720 --> 00:03:01.520
For example, if we
want to travel to

72
00:03:01.520 --> 00:03:04.235
Edmonton or to Toronto
or some other city,

73
00:03:04.235 --> 00:03:09.190
the agent would still go through
the same kinds of steps.

74
00:03:09.190 --> 00:03:12.995
How do we formalize this idea of

75
00:03:12.995 --> 00:03:15.230
abstraction at the temporal level

76
00:03:15.230 --> 00:03:16.370
or at the behavioral level?

77
00:03:16.370 --> 00:03:19.780
One way to do this is through
this notion of options.

78
00:03:19.780 --> 00:03:22.320
We're going to think
of an option as

79
00:03:22.320 --> 00:03:24.890
consisting of three components.

80
00:03:24.890 --> 00:03:27.060
One is an initiation set,

81
00:03:27.060 --> 00:03:29.000
that means the set of
circumstances under

82
00:03:29.000 --> 00:03:31.160
which an option
could be initiated.

83
00:03:31.160 --> 00:03:32.795
For example, we can only

84
00:03:32.795 --> 00:03:34.790
fly somewhere if we
are at the airport.

85
00:03:34.790 --> 00:03:36.785
So being at the airport
is an initiation set.

86
00:03:36.785 --> 00:03:40.265
A second component is
an internal policy.

87
00:03:40.265 --> 00:03:41.750
That means, a way of picking

88
00:03:41.750 --> 00:03:44.725
preventive actions while
the option is executing.

89
00:03:44.725 --> 00:03:48.030
For example, if we're
walking to the gate,

90
00:03:48.030 --> 00:03:49.850
the walking motion is

91
00:03:49.850 --> 00:03:54.520
a policy and that will
take control and execute.

92
00:03:54.520 --> 00:03:57.230
This is the usual probability of

93
00:03:57.230 --> 00:04:00.500
taking preventive actions
conditioned on the agent state.

94
00:04:00.500 --> 00:04:03.205
Then, we have a
termination condition.

95
00:04:03.205 --> 00:04:05.625
Typically, this is
also stochastic.

96
00:04:05.625 --> 00:04:08.000
It basically tells us what
is the probability of

97
00:04:08.000 --> 00:04:11.495
the agent finishing the behavior
in any particular state.

98
00:04:11.495 --> 00:04:13.610
So if we're walking to the gate,

99
00:04:13.610 --> 00:04:16.805
we might finish this behavior
because we've reached

100
00:04:16.805 --> 00:04:19.025
the gate of the airplane

101
00:04:19.025 --> 00:04:21.410
but we might also
finish the behavior.

102
00:04:21.410 --> 00:04:23.480
For example, if
there is an alarm,

103
00:04:23.480 --> 00:04:25.480
maybe we want to go
outside the building,

104
00:04:25.480 --> 00:04:28.320
so switch the behavior
to something else.

105
00:04:29.330 --> 00:04:33.150
If we think in terms
of these components,

106
00:04:33.150 --> 00:04:38.690
actually agents have now
transient behavior that they can

107
00:04:38.690 --> 00:04:41.540
execute and we can also think of

108
00:04:41.540 --> 00:04:44.765
models that are paired up with
these chance of behavior.

109
00:04:44.765 --> 00:04:47.990
Each option will have
a model and its model

110
00:04:47.990 --> 00:04:50.540
consists of the expected reward

111
00:04:50.540 --> 00:04:52.520
that the injured might
get along the way,

112
00:04:52.520 --> 00:04:54.875
and essentially a transition

113
00:04:54.875 --> 00:04:56.990
probability distribution

114
00:04:56.990 --> 00:04:59.215
of the states where
it might end up.

115
00:04:59.215 --> 00:05:02.830
Because options take
different amounts of time,

116
00:05:02.830 --> 00:05:06.544
instead of just thinking of
transitions to next states,

117
00:05:06.544 --> 00:05:09.155
we are in fact going
to think of transition

118
00:05:09.155 --> 00:05:12.160
to a next time as well.

119
00:05:12.160 --> 00:05:14.625
So this will be a
joint distribution,

120
00:05:14.625 --> 00:05:18.555
in fact, over states
and over times.

121
00:05:18.555 --> 00:05:22.680
But once an agent is
endowed with options,

122
00:05:22.680 --> 00:05:26.255
it essentially reasons in the
same way that it would in

123
00:05:26.255 --> 00:05:28.250
MDP except we have

124
00:05:28.250 --> 00:05:30.845
what's now called the
semi-Markov decision process.

125
00:05:30.845 --> 00:05:33.050
The Markov property
still holds but

126
00:05:33.050 --> 00:05:37.045
the duration of a
behavior is not fixed,

127
00:05:37.045 --> 00:05:40.175
it's actually based on
probability distribution.

128
00:05:40.175 --> 00:05:44.000
However, all of the planning
and learning algorithms that

129
00:05:44.000 --> 00:05:47.790
you already know and love
such as valid duration,

130
00:05:47.790 --> 00:05:50.090
temporal difference
learning, all the

131
00:05:50.090 --> 00:05:52.460
other things that are talked
about in this course,

132
00:05:52.460 --> 00:05:55.010
work in almost the same way in

133
00:05:55.010 --> 00:05:58.470
semi-Markov decision
processes as they do in MDPs.

134
00:05:58.470 --> 00:06:01.340
So we can bring to bear
all of the ideas from

135
00:06:01.340 --> 00:06:06.270
these algorithms in order
to learn about options.

136
00:06:06.940 --> 00:06:13.550
One of the interesting
questions is how to do

137
00:06:13.550 --> 00:06:16.760
temporal obstruction at
the same time as doing

138
00:06:16.760 --> 00:06:20.160
state obstruction or doing
function approximation.

139
00:06:20.160 --> 00:06:22.570
Ideally, we would like to
do both of these things.

140
00:06:22.570 --> 00:06:27.335
So we would like to reason
about the airport rather than

141
00:06:27.335 --> 00:06:30.680
specific centimeters in space
just like we would like to

142
00:06:30.680 --> 00:06:32.210
think about the
behavior of going to

143
00:06:32.210 --> 00:06:34.130
the airport instead
of muscle twitches.

144
00:06:34.130 --> 00:06:37.310
However, these two are actually

145
00:06:37.310 --> 00:06:40.400
distinct and we would

146
00:06:40.400 --> 00:06:43.985
like them to work
together in some sense.

147
00:06:43.985 --> 00:06:45.305
But how to get

148
00:06:45.305 --> 00:06:47.570
good temporal obstruction and

149
00:06:47.570 --> 00:06:49.960
state obstruction
at the same time,

150
00:06:49.960 --> 00:06:51.680
and in a way in which
they communicate

151
00:06:51.680 --> 00:06:52.775
well with each other

152
00:06:52.775 --> 00:06:56.195
is actually still an
open research problem.

153
00:06:56.195 --> 00:06:59.300
Similarly, one interesting
open restricts

154
00:06:59.300 --> 00:07:03.245
problem is figuring out
where options come from.

155
00:07:03.245 --> 00:07:08.140
So who tells us what is a
good termination condition?

156
00:07:08.140 --> 00:07:11.630
Well, this is something
that we would like to find

157
00:07:11.630 --> 00:07:16.230
out that we don't quite
have a handle on it.