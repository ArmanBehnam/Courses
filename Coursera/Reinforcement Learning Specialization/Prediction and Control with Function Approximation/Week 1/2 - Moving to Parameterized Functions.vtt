WEBVTT

1
00:00:04.520 --> 00:00:08.055
Up until now, we've learned
about tabular methods.

2
00:00:08.055 --> 00:00:09.360
Methods that store table

3
00:00:09.360 --> 00:00:11.490
containing separately
learn values,

4
00:00:11.490 --> 00:00:13.740
for each possible state.

5
00:00:13.740 --> 00:00:15.870
In real-world problems,

6
00:00:15.870 --> 00:00:18.540
these tables will become
intractably large.

7
00:00:18.540 --> 00:00:21.540
Imagine a robot that sees
the world through a camera.

8
00:00:21.540 --> 00:00:23.040
We clearly cannot store

9
00:00:23.040 --> 00:00:25.980
a table entry for
every possible image.

10
00:00:25.980 --> 00:00:29.100
Luckily, this is not
the only possibility.

11
00:00:29.100 --> 00:00:31.140
Today we'll talk about
more general ways

12
00:00:31.140 --> 00:00:33.300
to approximate value function.

13
00:00:33.300 --> 00:00:35.265
By the end of this video,

14
00:00:35.265 --> 00:00:37.700
you'll be able to
understand how we

15
00:00:37.700 --> 00:00:40.900
can use parameterized functions
to approximate values,

16
00:00:40.900 --> 00:00:45.150
explain linear function
approximation,

17
00:00:45.150 --> 00:00:47.690
recognize that
the tabular case is

18
00:00:47.690 --> 00:00:51.515
a special case of linear
value function approximation,

19
00:00:51.515 --> 00:00:54.050
and understand that
there are many ways to

20
00:00:54.050 --> 00:00:57.270
parameterize an approximate
value function.

21
00:00:57.740 --> 00:01:00.530
In previous courses,
we spent a lot of

22
00:01:00.530 --> 00:01:04.320
time thinking about how to
estimate value functions.

23
00:01:04.570 --> 00:01:08.210
So far, our value approximations

24
00:01:08.210 --> 00:01:10.255
have always had the same form.

25
00:01:10.255 --> 00:01:14.225
For each state, we store
separate value in a table.

26
00:01:14.225 --> 00:01:16.460
We look at values,
and modify them in

27
00:01:16.460 --> 00:01:19.205
the table as learning progresses.

28
00:01:19.205 --> 00:01:21.320
But this is not the only way

29
00:01:21.320 --> 00:01:23.760
to approximate a value function.

30
00:01:23.900 --> 00:01:27.860
In principle, we can use
any function that takes a state,

31
00:01:27.860 --> 00:01:30.020
and produces a real number.

32
00:01:30.020 --> 00:01:32.795
For example in a grid like this,

33
00:01:32.795 --> 00:01:34.595
our value function approximation

34
00:01:34.595 --> 00:01:36.860
could take the X and Y position,

35
00:01:36.860 --> 00:01:40.470
and add them together to
produce a value estimate.

36
00:01:40.610 --> 00:01:43.595
This was just to get
your intuitions going.

37
00:01:43.595 --> 00:01:46.280
We wouldn't want to use
this as a value function,

38
00:01:46.280 --> 00:01:49.115
because we can't modify
the approximation.

39
00:01:49.115 --> 00:01:53.010
So we have no way to do learning.

40
00:01:53.560 --> 00:01:55.880
This is where the idea of

41
00:01:55.880 --> 00:01:58.040
a parameterized
function comes in.

42
00:01:58.040 --> 00:02:00.965
We incorporate a set of
real valued weights,

43
00:02:00.965 --> 00:02:03.710
which we can adjust to
change the function.

44
00:02:03.710 --> 00:02:06.515
For example in this grid,

45
00:02:06.515 --> 00:02:09.905
instead of using
a fixed sum of X and Y,

46
00:02:09.905 --> 00:02:13.690
we could use a function
of the form w-1 times X,

47
00:02:13.690 --> 00:02:16.280
plus w-2 times Y.

48
00:02:16.280 --> 00:02:20.540
The weights w-1 and w-2
parameterize our function.

49
00:02:20.540 --> 00:02:21.800
They allow us to change

50
00:02:21.800 --> 00:02:24.320
the output the
function generates.

51
00:02:24.320 --> 00:02:26.420
To indicate that this function

52
00:02:26.420 --> 00:02:28.685
approximates
the true value function,

53
00:02:28.685 --> 00:02:31.325
we use the notation v hat.

54
00:02:31.325 --> 00:02:33.305
W is a vector containing

55
00:02:33.305 --> 00:02:35.900
all the weights that
parameterize the approximation.

56
00:02:35.900 --> 00:02:39.095
We do not have to store
a whole table of values.

57
00:02:39.095 --> 00:02:41.390
We only have to
store two weights to

58
00:02:41.390 --> 00:02:43.985
represent our value
function approximation.

59
00:02:43.985 --> 00:02:47.000
You can imagine how
this compact representation

60
00:02:47.000 --> 00:02:49.895
is useful for large state spaces.

61
00:02:49.895 --> 00:02:52.970
The tabular
representation modifying

62
00:02:52.970 --> 00:02:54.665
the value estimates
for one state,

63
00:02:54.665 --> 00:02:56.990
leaves the others unchanged.

64
00:02:56.990 --> 00:03:01.055
This is no longer the case
with a parameterized function.

65
00:03:01.055 --> 00:03:03.800
Watch what happens to
the value estimates of

66
00:03:03.800 --> 00:03:06.830
each state as we
change the weight w-1.

67
00:03:06.830 --> 00:03:10.099
If we change either w-1 or w-2,

68
00:03:10.099 --> 00:03:13.190
we will change the value
estimate for every state.

69
00:03:13.190 --> 00:03:16.025
Now, our learning outcomes
will modify the weights,

70
00:03:16.025 --> 00:03:18.960
instead of the individual
state values.

71
00:03:20.090 --> 00:03:23.405
The example we just
discussed is a special case

72
00:03:23.405 --> 00:03:26.540
called linear value
function approximation.

73
00:03:26.540 --> 00:03:29.030
In this case, the value
of each state is

74
00:03:29.030 --> 00:03:32.060
represented by a linear
function of the weights.

75
00:03:32.060 --> 00:03:34.790
This simply means that
the value of each state,

76
00:03:34.790 --> 00:03:37.520
is computed as the sum of
the weights multiplied by

77
00:03:37.520 --> 00:03:41.465
some fixed attributes of
the state called features.

78
00:03:41.465 --> 00:03:44.105
We can express this compactly.

79
00:03:44.105 --> 00:03:46.790
We write that the approximate
value is given by

80
00:03:46.790 --> 00:03:48.725
the inner product of
this feature vector,

81
00:03:48.725 --> 00:03:50.240
and the weight vector.

82
00:03:50.240 --> 00:03:54.960
We will use bold X of S to
denote the feature vector.

83
00:03:55.540 --> 00:03:57.950
Our choice of feature impacts

84
00:03:57.950 --> 00:04:00.575
the kinds of value
functions we can represent.

85
00:04:00.575 --> 00:04:02.430
For example, consider

86
00:04:02.430 --> 00:04:04.790
the approximation we've
discussed so far.

87
00:04:04.790 --> 00:04:06.730
With it, we can only

88
00:04:06.730 --> 00:04:08.470
represent value
functions which changed

89
00:04:08.470 --> 00:04:12.500
linearly as a function
of the features X and Y.

90
00:04:12.660 --> 00:04:15.610
Consider the case where
the true value function

91
00:04:15.610 --> 00:04:17.905
is given by the numbers
on this grid.

92
00:04:17.905 --> 00:04:21.935
We cannot represent this as
a linear function of X and Y.

93
00:04:21.935 --> 00:04:24.210
To get these outer
values correct,

94
00:04:24.210 --> 00:04:27.620
w-1 and w-2 both need to be zero.

95
00:04:27.620 --> 00:04:30.550
However that would mean
these inner values are not

96
00:04:30.550 --> 00:04:34.585
correct because at least one
of w-1 and w-2,

97
00:04:34.585 --> 00:04:38.360
must be non-zero for
the sum that equal five.

98
00:04:38.360 --> 00:04:42.690
But we don't have to use
X and Y as features.

99
00:04:42.690 --> 00:04:45.130
Linear function
approximation relies

100
00:04:45.130 --> 00:04:46.690
on having good features.

101
00:04:46.690 --> 00:04:50.560
Here X and Y are not
good features for this problem.

102
00:04:50.560 --> 00:04:52.300
But as you'll see later,

103
00:04:52.300 --> 00:04:56.060
there are many powerful methods
to construct features.

104
00:04:56.820 --> 00:05:00.460
Linear function approximation
is actually very general.

105
00:05:00.460 --> 00:05:02.830
In fact, even a tabular
representation is

106
00:05:02.830 --> 00:05:05.710
a special case of
linear function approximation.

107
00:05:05.710 --> 00:05:07.810
To build a linear
value function to

108
00:05:07.810 --> 00:05:10.510
represent this tabular
value function,

109
00:05:10.510 --> 00:05:12.490
we need to define the features.

110
00:05:12.490 --> 00:05:14.230
Let's choose our features to be

111
00:05:14.230 --> 00:05:17.080
indicator functions
for particular states.

112
00:05:17.080 --> 00:05:19.250
For state S_i,

113
00:05:19.250 --> 00:05:23.245
feature i is one and
the remaining features are zero.

114
00:05:23.245 --> 00:05:27.210
We have 16 features,
one for each state.

115
00:05:27.210 --> 00:05:29.150
Let's compute
the approximate values

116
00:05:29.150 --> 00:05:31.010
for a state with these features.

117
00:05:31.010 --> 00:05:34.345
Since all the features are
zero except for one of them,

118
00:05:34.345 --> 00:05:35.720
the inner product is equal to

119
00:05:35.720 --> 00:05:38.495
the single weight
associated with that state.

120
00:05:38.495 --> 00:05:40.220
Since every state's value is

121
00:05:40.220 --> 00:05:41.885
represented by a separate weight,

122
00:05:41.885 --> 00:05:45.450
this is equivalent to
a tabular value function.

123
00:05:45.610 --> 00:05:48.340
These parameterized
values are general,

124
00:05:48.340 --> 00:05:51.185
and we can consider lots of
different types of functions.

125
00:05:51.185 --> 00:05:52.850
Neural networks are an example of

126
00:05:52.850 --> 00:05:54.650
a non-linear function of state.

127
00:05:54.650 --> 00:05:56.450
The output of the network is

128
00:05:56.450 --> 00:05:59.045
our approximate value
for a given state.

129
00:05:59.045 --> 00:06:02.930
The state is passed to
the network as the input.

130
00:06:02.930 --> 00:06:05.750
All the connections in
the network correspond

131
00:06:05.750 --> 00:06:07.715
to real valued weights.

132
00:06:07.715 --> 00:06:10.609
When data is passed
through a connection,

133
00:06:10.609 --> 00:06:12.830
the weight is multiplied
by its input.

134
00:06:12.830 --> 00:06:15.560
This process transforms
the input state through

135
00:06:15.560 --> 00:06:16.670
a sequence of layers to

136
00:06:16.670 --> 00:06:18.605
finally produce
the value estimate.

137
00:06:18.605 --> 00:06:22.200
We will take a deep dive
into neural networks later.

138
00:06:23.170 --> 00:06:25.565
That's it for this video.

139
00:06:25.565 --> 00:06:27.050
You should now be
comfortable with

140
00:06:27.050 --> 00:06:28.830
the idea of
parameterized functions,

141
00:06:28.830 --> 00:06:29.900
and how they might be used

142
00:06:29.900 --> 00:06:31.895
to approximate value functions.

143
00:06:31.895 --> 00:06:34.580
In the coming weeks, we
will discuss how to update

144
00:06:34.580 --> 00:06:36.260
these approximate value functions

145
00:06:36.260 --> 00:06:39.030
while the agent interacts
with the world.