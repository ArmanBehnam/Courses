WEBVTT

1
00:00:00.740 --> 00:00:03.420
So in Week 3 of Course 4,

2
00:00:03.420 --> 00:00:07.005
you're going to cover many
different applications of NLP.

3
00:00:07.005 --> 00:00:10.380
One thing you are going to
look at is question answering.

4
00:00:10.380 --> 00:00:12.870
Given the question
and some contexts,

5
00:00:12.870 --> 00:00:14.820
can you tell us what the answer

6
00:00:14.820 --> 00:00:17.295
is going to be
inside that context.

7
00:00:17.295 --> 00:00:20.295
Another thing you're going to
cover is transfer learning.

8
00:00:20.295 --> 00:00:21.615
For example,

9
00:00:21.615 --> 00:00:25.170
knowing some information by

10
00:00:25.170 --> 00:00:27.310
training something
in a specific task.

11
00:00:27.310 --> 00:00:29.010
How can you make use of

12
00:00:29.010 --> 00:00:32.415
that information and apply
it to a different task?

13
00:00:32.415 --> 00:00:34.215
You're going to look at BERT,

14
00:00:34.215 --> 00:00:35.520
which is known as

15
00:00:35.520 --> 00:00:37.950
the Bidirectional
Encoder Representation,

16
00:00:37.950 --> 00:00:40.005
which makes use of transformers.

17
00:00:40.005 --> 00:00:42.250
You'll see how you can use

18
00:00:42.250 --> 00:00:45.840
bidirectionality to
improve performance.

19
00:00:45.840 --> 00:00:48.435
Then you're going to
look at the T5 model.

20
00:00:48.435 --> 00:00:50.150
Basically what this model does,

21
00:00:50.150 --> 00:00:53.989
you can see here it has
several possible inputs.

22
00:00:53.989 --> 00:00:55.280
So it could be question,

23
00:00:55.280 --> 00:00:56.465
you get an answer.

24
00:00:56.465 --> 00:01:00.260
It could be review and you
will get the rating over here.

25
00:01:00.260 --> 00:01:03.160
It's all being fed
into one model.

26
00:01:03.160 --> 00:01:06.180
Let's look at question answering.

27
00:01:06.180 --> 00:01:10.175
Over here you have context-based
question-answering,

28
00:01:10.175 --> 00:01:14.555
meaning you take in a
question and the contexts.

29
00:01:14.555 --> 00:01:16.190
It tells you where the answer

30
00:01:16.190 --> 00:01:18.600
is inside that's
context over here.

31
00:01:18.600 --> 00:01:21.740
So this is the highlighted
stuff, which is the answer.

32
00:01:21.740 --> 00:01:23.930
Then you have closed
book question answering,

33
00:01:23.930 --> 00:01:26.540
which only takes the
question and it returns

34
00:01:26.540 --> 00:01:29.180
the answer without having
access to a context,

35
00:01:29.180 --> 00:01:31.820
so it comes up with
its own answer.

36
00:01:31.820 --> 00:01:33.680
Previously we've seen

37
00:01:33.680 --> 00:01:36.545
how innovations in
model architecture,

38
00:01:36.545 --> 00:01:39.170
improved performance,
and we've also seen

39
00:01:39.170 --> 00:01:42.380
how data preparation could help.

40
00:01:42.380 --> 00:01:45.860
But over here, you're going
to see that innovations in

41
00:01:45.860 --> 00:01:47.240
the way that training is being

42
00:01:47.240 --> 00:01:49.610
done also improves performance.

43
00:01:49.610 --> 00:01:51.200
In which case, you will see how

44
00:01:51.200 --> 00:01:55.245
transfer learning will
improve performance.

45
00:01:55.245 --> 00:01:57.170
This is the classical training

46
00:01:57.170 --> 00:01:58.565
that you're used to seeing.

47
00:01:58.565 --> 00:02:00.140
You have a course review,

48
00:02:00.140 --> 00:02:01.790
this goes through a model and

49
00:02:01.790 --> 00:02:03.790
let's you predict the rating.

50
00:02:03.790 --> 00:02:06.140
Then you just predict
the rating the

51
00:02:06.140 --> 00:02:08.450
same way as you've
always been doing.

52
00:02:08.450 --> 00:02:09.640
So nothing changed here,

53
00:02:09.640 --> 00:02:11.270
this is just an overview of

54
00:02:11.270 --> 00:02:13.615
the classical training
that you're used to.

55
00:02:13.615 --> 00:02:15.290
Now in transfer learning,

56
00:02:15.290 --> 00:02:16.705
let's look at this example.

57
00:02:16.705 --> 00:02:19.504
Let's say that you
have movie reviews

58
00:02:19.504 --> 00:02:20.990
and then you feed them into

59
00:02:20.990 --> 00:02:23.270
your model and you
predict a rating.

60
00:02:23.270 --> 00:02:25.775
Over here you have
the pretrain task,

61
00:02:25.775 --> 00:02:27.675
which is on movie reviews.

62
00:02:27.675 --> 00:02:29.930
Now when training, you're going

63
00:02:29.930 --> 00:02:32.460
to take the existing model or

64
00:02:32.460 --> 00:02:35.270
movie reviews and then
you're going to find units

65
00:02:35.270 --> 00:02:38.585
or train it again
on course reviews.

66
00:02:38.585 --> 00:02:40.970
You'll predict the
rating for that review.

67
00:02:40.970 --> 00:02:43.040
So as you can see over here,

68
00:02:43.040 --> 00:02:45.725
instead of initializing
the weights from scratch,

69
00:02:45.725 --> 00:02:47.570
you start with the
weights that you got

70
00:02:47.570 --> 00:02:49.760
from the movie reviews and you

71
00:02:49.760 --> 00:02:51.785
use them as a starter point

72
00:02:51.785 --> 00:02:54.155
when training for
the course reviews.

73
00:02:54.155 --> 00:02:56.960
At the end you do some
inference over here.

74
00:02:56.960 --> 00:02:59.400
You do the inference the same
way you're used to doing,

75
00:02:59.400 --> 00:03:01.025
you just take the course review,

76
00:03:01.025 --> 00:03:02.645
you feed this into your model,

77
00:03:02.645 --> 00:03:04.760
and you get your prediction.

78
00:03:04.760 --> 00:03:07.220
So you can also use
transfer learning

79
00:03:07.220 --> 00:03:09.560
on different tasks
is another example,

80
00:03:09.560 --> 00:03:12.380
where you feed in the ratings and

81
00:03:12.380 --> 00:03:16.150
some review and it gives you
sentiment classification.

82
00:03:16.150 --> 00:03:18.200
Then you can train it on

83
00:03:18.200 --> 00:03:20.510
a downstream tasks like
question-answering,

84
00:03:20.510 --> 00:03:22.640
where you take the
initial weights over here

85
00:03:22.640 --> 00:03:25.040
and you train it on
question answering.

86
00:03:25.040 --> 00:03:26.495
So when is pay day?

87
00:03:26.495 --> 00:03:30.040
The model answer's March 14th.

88
00:03:30.040 --> 00:03:33.555
Then you can ask them
model the same question.

89
00:03:33.555 --> 00:03:35.970
When's my birthday over here?

90
00:03:35.970 --> 00:03:37.905
It does not know the answer.

91
00:03:37.905 --> 00:03:40.940
But this is just
another example of how

92
00:03:40.940 --> 00:03:44.485
you can use transfer
learning on different tasks.

93
00:03:44.485 --> 00:03:46.500
Now we're going to look at BERT,

94
00:03:46.500 --> 00:03:49.320
which makes use of
bi-directional context.

95
00:03:49.320 --> 00:03:52.365
In this case, you have learning
from deep learning AI,

96
00:03:52.365 --> 00:03:55.025
it's like watching the
sunset with my best friend.

97
00:03:55.025 --> 00:03:58.485
Over here the context is
everything that's come before.

98
00:03:58.485 --> 00:03:59.960
Then let's say you're
trying to predict

99
00:03:59.960 --> 00:04:02.105
the next word, deep-learning AI.

100
00:04:02.105 --> 00:04:05.750
Now when doing bi-directional
representations,

101
00:04:05.750 --> 00:04:09.080
you'll be looking at the
context from this side and

102
00:04:09.080 --> 00:04:13.080
from this side to
predict the middle word.

103
00:04:13.080 --> 00:04:19.010
This is one of the main
takeaways for bidirectionality.

104
00:04:19.010 --> 00:04:22.355
Now let's look at single
task versus multitask.

105
00:04:22.355 --> 00:04:25.685
Over here you have a single
model which takes in

106
00:04:25.685 --> 00:04:29.520
a review and then
predicts a rating.

107
00:04:29.520 --> 00:04:31.985
Over here you have another
model which takes in

108
00:04:31.985 --> 00:04:34.040
a question and
predicts an answer.

109
00:04:34.040 --> 00:04:35.210
This is a single task,

110
00:04:35.210 --> 00:04:37.055
each like one module per task.

111
00:04:37.055 --> 00:04:40.690
Now, what you can do here with T5

112
00:04:40.690 --> 00:04:42.845
is it is the same
model that's being

113
00:04:42.845 --> 00:04:45.305
used to take the review,

114
00:04:45.305 --> 00:04:47.870
predict the rating, and then take

115
00:04:47.870 --> 00:04:51.340
the question and
predict the answer.

116
00:04:51.340 --> 00:04:54.595
So instead of having
two independent models,

117
00:04:54.595 --> 00:04:56.830
you end up having one model.

118
00:04:56.830 --> 00:04:59.275
Let's look at T5.

119
00:04:59.275 --> 00:05:01.690
Over here, the main takeaway

120
00:05:01.690 --> 00:05:03.445
is that the more data you have,

121
00:05:03.445 --> 00:05:06.165
generally the better
performance there is.

122
00:05:06.165 --> 00:05:10.150
For example, the English
Wikipedia dataset

123
00:05:10.150 --> 00:05:11.815
is around 13 gigabytes

124
00:05:11.815 --> 00:05:14.780
compared to the C4

125
00:05:14.780 --> 00:05:18.310
Colossal Clean Crawled.Corpus
is about a 800 gigabytes,

126
00:05:18.310 --> 00:05:20.680
which is what T5 was trained on.

127
00:05:20.680 --> 00:05:23.095
This is just to give you,

128
00:05:23.095 --> 00:05:27.220
how much larger the
C4 data sets is,

129
00:05:27.220 --> 00:05:29.960
when compared to the
English Wikipedia.

130
00:05:29.960 --> 00:05:33.610
What are the desirable goals
for transfer learning?

131
00:05:33.610 --> 00:05:35.300
First of all, you want to reduce

132
00:05:35.300 --> 00:05:36.440
training time because you

133
00:05:36.440 --> 00:05:38.450
already have a pre-trained model.

134
00:05:38.450 --> 00:05:41.450
Hopefully, once you
use transfer learning,

135
00:05:41.450 --> 00:05:43.310
you'll get faster convergence.

136
00:05:43.310 --> 00:05:45.470
It will also improve
predictions because you'll

137
00:05:45.470 --> 00:05:47.780
learn a few things from
different tasks that

138
00:05:47.780 --> 00:05:49.880
might be helpful and useful for

139
00:05:49.880 --> 00:05:52.520
your current predictions on
the task you're training on.

140
00:05:52.520 --> 00:05:56.975
Finally, you might
require or need less data

141
00:05:56.975 --> 00:05:59.135
because your model
has already learned

142
00:05:59.135 --> 00:06:01.750
a lot from other tasks.

143
00:06:01.750 --> 00:06:03.560
So if you have a smaller dataset,

144
00:06:03.560 --> 00:06:06.870
then transfer learning
might help you.