WEBVTT

1
00:00:00.430 --> 00:00:03.480
Hi again,
I'll now be talking about the T5 model.

2
00:00:03.480 --> 00:00:06.296
The T5 model could be used
on several NLP tasks.

3
00:00:06.296 --> 00:00:09.581
And it's also uses a similar
training strategy to birds.

4
00:00:09.581 --> 00:00:13.949
Concretely it makes use of transfer
learning and mask language modeling.

5
00:00:13.949 --> 00:00:16.961
The T5 model also uses
transformers when training.

6
00:00:16.961 --> 00:00:22.090
So let's take a look at how you can use
this model in your own applications.

7
00:00:22.090 --> 00:00:24.990
So we're going to understand how T5 works.

8
00:00:24.990 --> 00:00:29.648
Then you're going to recognize
the different types of attention used.

9
00:00:29.648 --> 00:00:32.474
And team overview of
the model architecture.

10
00:00:32.474 --> 00:00:36.412
So T5 Transformers known as
the text to text transformer, and

11
00:00:36.412 --> 00:00:38.577
you can use it in classification.

12
00:00:38.577 --> 00:00:42.393
You can use it for question answering,
to answer a question.

13
00:00:42.393 --> 00:00:44.988
You can use it for machine translation.

14
00:00:44.988 --> 00:00:49.262
You can use it for summarization and
you can use it for sentiments.

15
00:00:49.262 --> 00:00:52.865
And there are other applications
that you can use T5 for, but

16
00:00:52.865 --> 00:00:54.741
we'll focus on these four now.

17
00:00:54.741 --> 00:00:59.569
So the model architecture and the way the
pre-training is done first of all is you

18
00:00:59.569 --> 00:01:03.971
have an original text, something like
this where you have, thank you for

19
00:01:03.971 --> 00:01:06.980
inviting me to your party last week.

20
00:01:06.980 --> 00:01:10.601
So you mask this certain words so
for inviting me last,

21
00:01:10.601 --> 00:01:15.182
and then you replace it with these
tokens like brackets X brackets Y.

22
00:01:15.182 --> 00:01:18.146
So brackets X corresponding to for
inviting and

23
00:01:18.146 --> 00:01:20.378
brackets Y corresponding to last.

24
00:01:20.378 --> 00:01:25.738
And your targets are going to be brackets
X for inviting, brackets Y last and

25
00:01:25.738 --> 00:01:30.953
these tokens or these brackets like
they keep going in increments order.

26
00:01:30.953 --> 00:01:35.494
So then it's bracket Z, then maybe
brackets A brackets B and so forth.

27
00:01:35.494 --> 00:01:39.240
So each bracket corresponds
to a certain target.

28
00:01:39.240 --> 00:01:43.896
The model architecture and
different transformer architecture

29
00:01:43.896 --> 00:01:48.813
variance that we're going to consider for
the attention part here.

30
00:01:48.813 --> 00:01:53.004
So we start with the basic
encoder-decoder representation.

31
00:01:53.004 --> 00:01:58.445
So you can see over here you have fully
visible attention in the encoder and

32
00:01:58.445 --> 00:02:01.358
then causal attention in the decoder.

33
00:02:01.358 --> 00:02:06.208
And then you have the general
encoder-decoder representation just as

34
00:02:06.208 --> 00:02:07.024
notation.

35
00:02:07.024 --> 00:02:10.750
So light gray lines
correspond to causal masking.

36
00:02:10.750 --> 00:02:15.720
And dark gray lines correspond
to the fully visible masking.

37
00:02:15.720 --> 00:02:21.371
So on the left as I said again, it's
the standard encoder-decoder architecture.

38
00:02:21.371 --> 00:02:23.980
In the middle over here what we have,

39
00:02:23.980 --> 00:02:29.632
we have the language model which consists
of a single transformer layer stack.

40
00:02:29.632 --> 00:02:34.378
And it's being fed the concatenation
of the inputs and the target.

41
00:02:34.378 --> 00:02:39.306
So it uses causal masking throughout
as you can see because they're

42
00:02:39.306 --> 00:02:40.522
all gray lines.

43
00:02:40.522 --> 00:02:44.230
And you have X1 going inside over here,
get at X2,

44
00:02:44.230 --> 00:02:47.870
X2 goes into the model X3 and so forth.

45
00:02:47.870 --> 00:02:49.716
Now over here to the right,

46
00:02:49.716 --> 00:02:54.216
we have prefix language model which
corresponds to allowing fully

47
00:02:54.216 --> 00:02:58.977
visible masking over the inputs as
you can see here in the dark arrows.

48
00:02:58.977 --> 00:03:01.400
And then causal masking in the rest.

49
00:03:02.870 --> 00:03:07.850
So as you can see over here,
it's doing causal masking.

50
00:03:07.850 --> 00:03:11.809
So the model architecture,
it uses encoder/decoder stack.

51
00:03:11.809 --> 00:03:15.060
It has 12 transformer blocks each.

52
00:03:15.060 --> 00:03:20.049
So you can think of it as a dozen eggs and
then 220 million parametres.

53
00:03:20.049 --> 00:03:23.847
So in summary, you've seen
prefix language model attention.

54
00:03:23.847 --> 00:03:26.357
You've seen the model architecture for T5.

55
00:03:26.357 --> 00:03:30.338
And you've seen how the pre-training
is done similar to birds, but

56
00:03:30.338 --> 00:03:32.750
we just use mask language modeling here.

57
00:03:33.820 --> 00:03:36.565
You now have an overview
of the transformer 5 model.

58
00:03:36.565 --> 00:03:40.692
You know how to train it, and you've seen
that you can use it on multiple tasks.

59
00:03:40.692 --> 00:03:44.922
In the next video I'll be talking about
a few training strategies for this model.

60
00:03:44.922 --> 00:03:46.270
See you in the next video.