WEBVTT

1
00:00:00.000 --> 00:00:01.725
Good to see you again.

2
00:00:01.725 --> 00:00:03.570
When running large deep models,

3
00:00:03.570 --> 00:00:05.130
you'll often run out of memory as

4
00:00:05.130 --> 00:00:07.935
each layer keeps allocating
it for a long time.

5
00:00:07.935 --> 00:00:10.320
I'll show you how this
can be solved using

6
00:00:10.320 --> 00:00:13.365
reversible layers, let's dive in.

7
00:00:13.365 --> 00:00:16.410
The transformer
network precedes by

8
00:00:16.410 --> 00:00:19.290
repeatedly adding residuals
to the hidden states.

9
00:00:19.290 --> 00:00:21.045
To run it in reverse,

10
00:00:21.045 --> 00:00:24.120
you can subtract the residuals
in the opposite order,

11
00:00:24.120 --> 00:00:26.250
starting with the
outputs of the model.

12
00:00:26.250 --> 00:00:27.810
But in order to save memory

13
00:00:27.810 --> 00:00:30.330
otherwise used to
store the residuals,

14
00:00:30.330 --> 00:00:33.420
you need to be able to
recompute them quickly instead,

15
00:00:33.420 --> 00:00:35.040
and this is where reversible

16
00:00:35.040 --> 00:00:36.960
residual connections come in.

17
00:00:36.960 --> 00:00:39.330
The key idea is that you start

18
00:00:39.330 --> 00:00:41.730
with two copies of
the model inputs,

19
00:00:41.730 --> 00:00:45.185
then at each layer you
only update one of them.

20
00:00:45.185 --> 00:00:47.780
The activations that
you don't update

21
00:00:47.780 --> 00:00:50.765
will be the ones used to
compute the residuals,

22
00:00:50.765 --> 00:00:52.940
where this configuration you

23
00:00:52.940 --> 00:00:55.085
can now run the
network in reverse.

24
00:00:55.085 --> 00:00:59.135
Layer 1 is attention and
layer 2 is feedforward.

25
00:00:59.135 --> 00:01:03.050
The activations in the
model are now twice as big,

26
00:01:03.050 --> 00:01:04.460
but you don't have to worry

27
00:01:04.460 --> 00:01:06.820
about caching for
the backwards pass.

28
00:01:06.820 --> 00:01:09.545
The standard
transformer equations

29
00:01:09.545 --> 00:01:14.710
give Y_a is equal to x
plus attention of X,

30
00:01:14.710 --> 00:01:20.130
and Y_b is equal to Y_a
plus feedforward of Y_a.

31
00:01:20.130 --> 00:01:22.665
This is the normal
residual connection.

32
00:01:22.665 --> 00:01:24.650
In the reversible case,

33
00:01:24.650 --> 00:01:29.110
you will have Y_1 equals
x_1 plus attention of x_2,

34
00:01:29.110 --> 00:01:33.550
and Y_2 equals x_2 plus
feedforward of Y_1.

35
00:01:33.550 --> 00:01:35.600
Then to save the memory,

36
00:01:35.600 --> 00:01:40.075
you can reconstruct the inputs
x_1 and x_2 as follows,

37
00:01:40.075 --> 00:01:43.770
x_1 equals Y_1 minus
attention of x_2

38
00:01:43.770 --> 00:01:48.765
and x_2 equals Y_2 minus
feedforward of Y_1.

39
00:01:48.765 --> 00:01:50.180
Feel free to pause here for

40
00:01:50.180 --> 00:01:53.800
a moment to make sure you
understand what's happening.

41
00:01:53.800 --> 00:01:57.530
But basically, I'm
computing using one side of

42
00:01:57.530 --> 00:01:59.675
the network plus feedforward

43
00:01:59.675 --> 00:02:02.080
of the other side to compute Y_2.

44
00:02:02.080 --> 00:02:04.650
Similarly, one side of

45
00:02:04.650 --> 00:02:06.725
the network plus attention

46
00:02:06.725 --> 00:02:09.050
of the other side to compute Y_1.

47
00:02:09.050 --> 00:02:10.640
By doing it this way,

48
00:02:10.640 --> 00:02:12.755
you can use the last row formulas

49
00:02:12.755 --> 00:02:15.695
to recompute x_1 and x_2.

50
00:02:15.695 --> 00:02:19.340
Let me show you how these
new formulas fits into

51
00:02:19.340 --> 00:02:21.530
this reversible
layers illustration

52
00:02:21.530 --> 00:02:23.300
I've already shown you.

53
00:02:23.300 --> 00:02:27.785
To do so, I'm going to rotate
the diagram onto its side,

54
00:02:27.785 --> 00:02:30.800
something like this,
so that information

55
00:02:30.800 --> 00:02:34.490
is flowing from left to
right in a forward pass.

56
00:02:34.490 --> 00:02:38.015
Restating the equations
I showed you earlier,

57
00:02:38.015 --> 00:02:40.460
the first step is to calculate

58
00:02:40.460 --> 00:02:45.525
Y_1 equals x_1 plus
the attention of x_2.

59
00:02:45.525 --> 00:02:47.420
Then after you've done this,

60
00:02:47.420 --> 00:02:48.800
the second step is to

61
00:02:48.800 --> 00:02:51.830
calculate Y_2 with
the second formula,

62
00:02:51.830 --> 00:02:54.770
which has a dependency on Y_1,

63
00:02:54.770 --> 00:02:57.200
and requires a few extra parts in

64
00:02:57.200 --> 00:03:00.835
the illustration that weren't
used in the first step,

65
00:03:00.835 --> 00:03:04.710
so Y_2 equals x_2 plus

66
00:03:04.710 --> 00:03:08.310
feedforward of Y_1 from the
first step, and that's it.

67
00:03:08.310 --> 00:03:11.060
But notice that Y_1 dependency,

68
00:03:11.060 --> 00:03:14.630
the key takeaway is to
recognize the two parts to

69
00:03:14.630 --> 00:03:18.080
the operation and how the
second builds on the first.

70
00:03:18.080 --> 00:03:19.835
First you find Y_1,

71
00:03:19.835 --> 00:03:22.270
then you use that to find Y_2.

72
00:03:22.270 --> 00:03:25.810
That's a forward pass for
irreversible residual block.

73
00:03:25.810 --> 00:03:28.265
It's combined standard attention

74
00:03:28.265 --> 00:03:30.410
and feedforward residual layers

75
00:03:30.410 --> 00:03:32.210
from a regular transformer

76
00:03:32.210 --> 00:03:35.080
into a single reversible
residual block,

77
00:03:35.080 --> 00:03:36.860
and there is nothing
to be saved in

78
00:03:36.860 --> 00:03:39.350
memory except the Y_1 and

79
00:03:39.350 --> 00:03:41.840
Y_2 of the output layer instead

80
00:03:41.840 --> 00:03:44.600
of activations for
every individual layer.

81
00:03:44.600 --> 00:03:48.300
Memory saved. Now,
I'll show you how to

82
00:03:48.300 --> 00:03:50.190
recompute x_1 and x_2

83
00:03:50.190 --> 00:03:53.415
from Y_1 and Y_2 for
a backward pass.

84
00:03:53.415 --> 00:03:56.030
First thing to notice is that I'm

85
00:03:56.030 --> 00:03:58.840
going to calculate
x_2 before x_1,

86
00:03:58.840 --> 00:04:01.490
the reason for which
I'll explain soon.

87
00:04:01.490 --> 00:04:03.260
First, I'll reverse some of

88
00:04:03.260 --> 00:04:05.404
the arrows in the illustration.

89
00:04:05.404 --> 00:04:07.280
The reverse direction is to

90
00:04:07.280 --> 00:04:09.755
indicate the information
is flowing backwards.

91
00:04:09.755 --> 00:04:11.660
Notice I also changed

92
00:04:11.660 --> 00:04:13.700
the plus signs in
the orange circles

93
00:04:13.700 --> 00:04:15.515
to be minuses to indicate

94
00:04:15.515 --> 00:04:18.170
subtraction will now
be taking place.

95
00:04:18.170 --> 00:04:21.125
The first step is
to calculate x_2

96
00:04:21.125 --> 00:04:25.795
equals Y_2 minus
feedforward of Y_1,

97
00:04:25.795 --> 00:04:28.260
and great work, you
just calculated

98
00:04:28.260 --> 00:04:32.125
an input x_2 from the two
outputs of the forward pass.

99
00:04:32.125 --> 00:04:35.090
The second step is
to calculate x_1.

100
00:04:35.090 --> 00:04:37.695
The formula for x_1 has

101
00:04:37.695 --> 00:04:41.295
a dependency on x_2 that
you just calculated,

102
00:04:41.295 --> 00:04:43.845
similar to the Y_2 dependency on

103
00:04:43.845 --> 00:04:46.650
Y_1 in the forward
pass formulas above,

104
00:04:46.650 --> 00:04:50.040
so x_1 equals Y_1

105
00:04:50.040 --> 00:04:52.395
minus the attention of x_2

106
00:04:52.395 --> 00:04:55.060
that you already calculated
in the first step.

107
00:04:55.060 --> 00:04:57.155
You now know how to compute

108
00:04:57.155 --> 00:04:59.630
a backward pass for
a residual layer and

109
00:04:59.630 --> 00:05:01.970
a transformer model
without the need for

110
00:05:01.970 --> 00:05:05.255
saving memory hungry activations
in the forward pass.

111
00:05:05.255 --> 00:05:06.950
I showed you how to do this

112
00:05:06.950 --> 00:05:09.320
using reversible residual blocks.

113
00:05:09.320 --> 00:05:12.095
You'll be seeing this
again in the assignment.

114
00:05:12.095 --> 00:05:13.910
Comparing performance of

115
00:05:13.910 --> 00:05:15.995
regular and reversible
transformers

116
00:05:15.995 --> 00:05:17.420
on machine translation,

117
00:05:17.420 --> 00:05:19.610
you find roughly the
same BLEU scores,

118
00:05:19.610 --> 00:05:22.505
and language modeling
produces similar results.

119
00:05:22.505 --> 00:05:26.190
In fact, reversibility is
a very general technique,

120
00:05:26.190 --> 00:05:29.300
this can be applied anywhere
you use a transformer model.

121
00:05:29.300 --> 00:05:31.490
While the comparison
shown here suggests

122
00:05:31.490 --> 00:05:34.600
reversing produces
slight out-performance,

123
00:05:34.600 --> 00:05:36.080
it's really because there's

124
00:05:36.080 --> 00:05:38.135
been some hyperparameter tuning

125
00:05:38.135 --> 00:05:39.830
in the three years since

126
00:05:39.830 --> 00:05:42.305
the original transformer
paper was published.

127
00:05:42.305 --> 00:05:44.750
You now understand
reversible layers and

128
00:05:44.750 --> 00:05:47.410
how they solved the memory
issue during training.

129
00:05:47.410 --> 00:05:50.690
Next, I'll put them together
with the LSH attention,

130
00:05:50.690 --> 00:05:52.970
this will give us a
variant of transformer

131
00:05:52.970 --> 00:05:55.205
that confinement on
very long sequences.

132
00:05:55.205 --> 00:05:57.720
Let's go to the next video.