WEBVTT

1
00:00:00.680 --> 00:00:04.620
Welcome, you're an expert
on transformers by now.

2
00:00:04.620 --> 00:00:07.731
In this week I will push
the transformer model even further,

3
00:00:07.731 --> 00:00:10.480
to make it work on really long sequences.

4
00:00:10.480 --> 00:00:15.260
First, let's see examples of tasks
where this is needed, let's dive in.

5
00:00:15.260 --> 00:00:19.212
>> Tasks with long sequences in
NLP include many applications,

6
00:00:19.212 --> 00:00:22.331
and are becoming
increasingly sophisticated.

7
00:00:22.331 --> 00:00:26.520
Well known use cases include
writing books and storytelling, or

8
00:00:26.520 --> 00:00:30.722
building intelligent agents for
conversations like chatbots.

9
00:00:30.722 --> 00:00:35.145
In fact, it's becoming increasingly
difficult to tell if the book

10
00:00:35.145 --> 00:00:38.014
here reading was written by a human or
AI, or

11
00:00:38.014 --> 00:00:42.295
if the conversation you're having
is rael or with the computer.

12
00:00:42.295 --> 00:00:46.730
The models behind these long
sequence applications presents

13
00:00:46.730 --> 00:00:51.599
considerable challenges,
largely due to their size in training.

14
00:00:51.599 --> 00:00:54.740
Many are based on the GPT-3
transformer model,

15
00:00:54.740 --> 00:01:00.324
which is just a larger version of GPT-2
that you already know from previous weeks.

16
00:01:00.324 --> 00:01:05.877
But these models can take industrial scale
compute and cost a lot of money to train.

17
00:01:05.877 --> 00:01:10.869
This week I will show you some of the
bottlenecks in these larger transformer

18
00:01:10.869 --> 00:01:15.238
models, and solutions you can use
to make them trainable for you.

19
00:01:15.238 --> 00:01:20.688
I'll teach you about the re former model
known as the reversible transformer,

20
00:01:20.688 --> 00:01:24.278
I'll explain why it's important and
how it works.

21
00:01:24.278 --> 00:01:27.348
Then you will use this new
knowledge to build and

22
00:01:27.348 --> 00:01:31.207
train a real working chatbot
in this week's assignments.

23
00:01:31.207 --> 00:01:35.782
You'll be able to ask it anything
you like on almost any topic at all.

24
00:01:35.782 --> 00:01:40.232
Processing long text sequences is
at the core of building chatbots.

25
00:01:40.232 --> 00:01:44.927
A chatbot model needs to use all
the previous pieces of the conversation as

26
00:01:44.927 --> 00:01:50.380
inputs for the next reply, this can make
for some really big context Windows.

27
00:01:50.380 --> 00:01:52.615
But what exactly is a chatbot, and

28
00:01:52.615 --> 00:01:56.778
how does it relate to say context
based question and answer, and

29
00:01:56.778 --> 00:02:01.885
closed loop based question and answer
that you learned in the previous weeks.

30
00:02:01.885 --> 00:02:05.504
To recap the context based Q and
A needs both a question and

31
00:02:05.504 --> 00:02:09.360
relevant text from where it's
going to retrieve an answer.

32
00:02:10.580 --> 00:02:12.470
Closed loop Q and A, however,

33
00:02:12.470 --> 00:02:17.069
doesn't need extra text to go along
with a question or prompt from a human.

34
00:02:17.069 --> 00:02:21.764
All the knowledge is stored in the weights
of the model itself during training.

35
00:02:21.764 --> 00:02:26.221
And this is how your chat bot will work,
and what you learn to do this week.

36
00:02:26.221 --> 00:02:31.307
So you'll use all the knowledge of NLP you
built over the previous three courses,

37
00:02:31.307 --> 00:02:36.174
and the power of transformers over long
sequences to build your own chatbots in

38
00:02:36.174 --> 00:02:37.964
this week's assignments.

39
00:02:37.964 --> 00:02:39.100
It's going to be fun.

40
00:02:40.380 --> 00:02:44.853
>> Now you've seen a number of tasks where
the sequence to generate is quite long,

41
00:02:44.853 --> 00:02:48.310
much longer than just one or
a few sentences.

42
00:02:48.310 --> 00:02:51.233
Could we just apply
the transformer to these tasks?

43
00:02:51.233 --> 00:02:55.430
Next I'll show you why it's not that
simple, let's go to the next video.