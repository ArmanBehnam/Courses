WEBVTT

1
00:00:00.250 --> 00:00:01.840
Welcome back.

2
00:00:01.840 --> 00:00:06.080
The main operation in transformer
is the dot product attention.

3
00:00:06.080 --> 00:00:09.522
You've already seen attention in
the first week of this course.

4
00:00:09.522 --> 00:00:10.335
In this video,

5
00:00:10.335 --> 00:00:14.592
I'll remind you how it works before we
move to more complex attention variants.

6
00:00:14.592 --> 00:00:16.420
Let's dive in.

7
00:00:16.420 --> 00:00:20.508
First I'll introduce the concept
of attention using an example of

8
00:00:20.508 --> 00:00:21.825
a translation task.

9
00:00:21.825 --> 00:00:26.330
Then you'll see some details
about the math behind attention.

10
00:00:26.330 --> 00:00:30.250
To understand what's attention, does
think of the following translation task.

11
00:00:30.250 --> 00:00:33.450
You need to translate
an English sentence to German.

12
00:00:33.450 --> 00:00:37.267
You've seen in previous courses how
to make good word embeddings, so

13
00:00:37.267 --> 00:00:42.280
that's vectors for English and German
words that mean the same or similar.

14
00:00:42.280 --> 00:00:45.796
For instance,
let's consider the example sentence.

15
00:00:45.796 --> 00:00:50.290
I am happy which is translated
into the German equivalent.

16
00:00:50.290 --> 00:00:54.506
So when you have a German word you want
to look at the English sentence and

17
00:00:54.506 --> 00:00:56.685
find all places with similar words.

18
00:00:56.685 --> 00:00:59.620
This is what attention does for you.

19
00:00:59.620 --> 00:01:04.700
In an attention layer, the German
word vectors are called queries.

20
00:01:04.700 --> 00:01:09.424
Because they initiate the look up,
a query is matched against all keys and

21
00:01:09.424 --> 00:01:13.233
each guy gets probability that
it's a match for the query.

22
00:01:13.233 --> 00:01:18.223
For instance, to translate the sentence,
I'm happy to German, you can compare

23
00:01:18.223 --> 00:01:22.657
the word I from English with
the following word I-c-h over here, Ich.

24
00:01:22.657 --> 00:01:26.060
Then you can compare the word
am with the word Ich and

25
00:01:26.060 --> 00:01:29.873
finally you can compare the word
happy with the word Ich.

26
00:01:29.873 --> 00:01:34.302
These queries by keys matrix is
often called attention weights and

27
00:01:34.302 --> 00:01:37.632
it says how much each key
is similar to each query.

28
00:01:37.632 --> 00:01:42.735
For example, the vector of the word Ich
maybe similar to the vector for I and

29
00:01:42.735 --> 00:01:47.515
get zero points its probability,
while it's not very similar to am or

30
00:01:47.515 --> 00:01:52.820
happy, but still a little bit and
get 0.1 probability for both.

31
00:01:52.820 --> 00:01:58.440
In the end, the query gets the sum of the
key vectors waited by the probabilities.

32
00:01:58.440 --> 00:01:59.465
In the example,

33
00:01:59.465 --> 00:02:04.816
the weighted sum of these vectors is 0.8
times the embedding of the word I plus 0.1

34
00:02:04.816 --> 00:02:10.780
times the embedding of the word am plus
0.1 times the embedding of the word happy.

35
00:02:10.780 --> 00:02:13.751
Before you dive into
the concept of attention,

36
00:02:13.751 --> 00:02:18.428
you need to first define three main
matrices, capital Q, which stands for

37
00:02:18.428 --> 00:02:24.240
queries, capital K which stands for Keys
and Capital V, which stands for values.

38
00:02:24.240 --> 00:02:26.563
So understand how to
create these matrices.

39
00:02:26.563 --> 00:02:30.200
Let's consider the phrase in English,
I am happy.

40
00:02:30.200 --> 00:02:35.125
First, the word I is embedded,
to obtain a vector representation that

41
00:02:35.125 --> 00:02:38.162
holds continuous values
which is unique for

42
00:02:38.162 --> 00:02:41.540
every single word as seen
earlier in the course.

43
00:02:41.540 --> 00:02:44.281
By feeding three distinct linear layers,

44
00:02:44.281 --> 00:02:49.340
you get three different vectors for
queries, keys and values.

45
00:02:49.340 --> 00:02:54.500
Then you can do the same for
the word am to output a second vector.

46
00:02:54.500 --> 00:02:59.372
And finally the word happy to get
a third vector and form the queries,

47
00:02:59.372 --> 00:03:01.950
keys and values matrix.

48
00:03:01.950 --> 00:03:05.586
From both the capital Q matrix and
the capital K matrix and

49
00:03:05.586 --> 00:03:09.452
attention model calculates weights or
scores representing

50
00:03:09.452 --> 00:03:13.970
the relative importance of the keys for
a specific query.

51
00:03:13.970 --> 00:03:17.667
These attention weights can be
understood as alignments course

52
00:03:17.667 --> 00:03:20.480
as they come from a dos product.

53
00:03:20.480 --> 00:03:24.006
Additionally, to turn these
weights into probabilities,

54
00:03:24.006 --> 00:03:25.950
a softmax function is required.

55
00:03:27.540 --> 00:03:31.123
Finally, multiplying these
probabilities with the values,

56
00:03:31.123 --> 00:03:36.590
you will then get a weighted sequence,
which is the attention results itself.

57
00:03:36.590 --> 00:03:40.114
Now that you have reviewed
the general idea behind attention,

58
00:03:40.114 --> 00:03:42.380
let me go through the math.

59
00:03:42.380 --> 00:03:45.934
The input to attention are queries,
keys and values.

60
00:03:45.934 --> 00:03:48.950
Often values are the same as keys.

61
00:03:48.950 --> 00:03:54.540
Queries are vectors of dimension T and
there is a number LQ of them.

62
00:03:54.540 --> 00:04:00.130
Keys are vectors of the same dimension
D and there is a number LK of them.

63
00:04:00.130 --> 00:04:03.090
Think of keys as the embeddings
of English words.

64
00:04:04.110 --> 00:04:08.501
And the queries as
the embeddings of German words.

65
00:04:08.501 --> 00:04:13.096
Then D is the dimensionality
of word embeddings.

66
00:04:13.096 --> 00:04:20.039
LQ, the length of the English sentence and
LQ, number of words of the German senses.

67
00:04:20.039 --> 00:04:26.650
So queries are a tensor Q of shape LQ
by D and keys have a shape LK by D.

68
00:04:26.650 --> 00:04:31.510
As I said before, a query Q will
assign each key a probability

69
00:04:31.510 --> 00:04:33.905
that the key K is a match for Q.

70
00:04:33.905 --> 00:04:38.717
You measure the similarity by
taking those products of vectors.

71
00:04:38.717 --> 00:04:41.618
So Q and
K are similar if Q dot K is large.

72
00:04:41.618 --> 00:04:45.997
However, the similarity numbers
do not add up to one, so

73
00:04:45.997 --> 00:04:49.690
they cannot be used as probabilities.

74
00:04:49.690 --> 00:04:50.530
To make them so

75
00:04:50.530 --> 00:04:56.100
and to make attention more focused on
the best matching keys, use the softmax.

76
00:04:56.100 --> 00:04:59.557
So you compute the matrix
of query key probabilities,

77
00:04:59.557 --> 00:05:05.660
often called the attention weights,
just as softmax of between Q&K transpose.

78
00:05:05.660 --> 00:05:08.980
This matrix has shaped LQ by LK.

79
00:05:08.980 --> 00:05:14.226
Each query keeper gets a probability
in the final step you take the values,

80
00:05:14.226 --> 00:05:17.734
which is another matrix of
the same shape as keys.

81
00:05:17.734 --> 00:05:22.091
And often the same as keys, and
we want to get a weighted sum,

82
00:05:22.091 --> 00:05:27.663
weighting each value vi by the probability
that the key ki matches the query.

83
00:05:27.663 --> 00:05:32.557
This can be computed very efficiently
just as matrix multiplication,

84
00:05:32.557 --> 00:05:38.140
we multiply attention weights W sub
S by the values V, and that's it.

85
00:05:38.140 --> 00:05:42.919
And attention mechanism calculates the
dynamic or alignment weights representing

86
00:05:42.919 --> 00:05:46.153
the relative importance of
the inputs in this sequence.

87
00:05:46.153 --> 00:05:50.790
Which are the keys for that particular
outputs, which is the query.

88
00:05:50.790 --> 00:05:53.750
Multiplying the dynamic weights or
the alignments

89
00:05:53.750 --> 00:05:58.600
course with the input sequence the values
will then weight the sequence.

90
00:05:58.600 --> 00:06:02.545
A single context vector can then be
calculated using the sum of weighted

91
00:06:02.545 --> 00:06:03.137
vectors.

92
00:06:03.137 --> 00:06:06.427
Now that you've gone through
the math behind attention and

93
00:06:06.427 --> 00:06:11.950
don't worry if you didn't get all of it at
the first time, it's a tricky operation.

94
00:06:11.950 --> 00:06:14.210
Take a moment to appreciate
the final formula.

95
00:06:15.350 --> 00:06:18.252
Matrix Z is our final results,

96
00:06:18.252 --> 00:06:24.290
which is equal to the attention
function of matrices Q, K and V.

97
00:06:25.370 --> 00:06:33.880
Which equals the softmax of the dot
product between Q and K transpose times V.

98
00:06:33.880 --> 00:06:36.822
And in turn it is equal
to W sub A times V.

99
00:06:36.822 --> 00:06:41.722
So the whole attention operation can
be written in this simple equation

100
00:06:41.722 --> 00:06:47.050
which is the softmax of the dot
product between QK transpose times V.

101
00:06:47.050 --> 00:06:49.241
It is important and very simple.

102
00:06:49.241 --> 00:06:50.180
In practice,

103
00:06:50.180 --> 00:06:55.899
this means that you can compute attention
by performing two matrix multiplications.

104
00:06:55.899 --> 00:07:00.930
Q times K transpose and then by V and
once after softmax operation.

105
00:07:00.930 --> 00:07:06.426
Modern hardware such as GPUs and CPUs
are very fast as matrix multiplication,

106
00:07:06.426 --> 00:07:11.100
which makes attention very fast and
such a great operation to use.

107
00:07:11.100 --> 00:07:15.337
Dot-product Attention is the heart and
soul of transformers.

108
00:07:15.337 --> 00:07:20.116
In general terms, the attention
takes as inputs, queries, keys and

109
00:07:20.116 --> 00:07:24.130
values, which are matrices of embeddings.

110
00:07:24.130 --> 00:07:28.484
Dot-product Attention is composed by
just two matrix multiplications and

111
00:07:28.484 --> 00:07:30.530
the softmax function.

112
00:07:30.530 --> 00:07:34.361
Therefore, you should consider
the use of GPUs and CPUs for

113
00:07:34.361 --> 00:07:36.960
speeding up this process.

114
00:07:36.960 --> 00:07:39.991
Now you understand dot
product attention very well.

115
00:07:39.991 --> 00:07:41.675
In the transformer decoder,

116
00:07:41.675 --> 00:07:44.974
we need an extended version
called the causal attention.

117
00:07:44.974 --> 00:07:47.420
I'll teach you about it in the next video.