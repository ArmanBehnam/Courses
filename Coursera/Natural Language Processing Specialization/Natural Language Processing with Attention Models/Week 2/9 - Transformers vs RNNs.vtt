WEBVTT

1
00:00:00.000 --> 00:00:01.890
Welcome. This week I

2
00:00:01.890 --> 00:00:03.960
will teach you about
the transformer model.

3
00:00:03.960 --> 00:00:06.750
It's a purely attention based
model that I developed with

4
00:00:06.750 --> 00:00:09.960
my friends at Google to remedy
some problems with RNNs.

5
00:00:09.960 --> 00:00:12.750
First, let me tell you what
these problems are so you

6
00:00:12.750 --> 00:00:14.550
understand why the
transformer model

7
00:00:14.550 --> 00:00:16.650
is needed. Let's dive in.

8
00:00:16.650 --> 00:00:19.830
First, I'll talk about
some problems related to

9
00:00:19.830 --> 00:00:21.510
recurrent neural networks using

10
00:00:21.510 --> 00:00:23.505
some familiar architectures.

11
00:00:23.505 --> 00:00:25.980
After that, I'll show you why

12
00:00:25.980 --> 00:00:29.475
pure attention models help
to solve those issues.

13
00:00:29.475 --> 00:00:32.100
In neural machine translation,

14
00:00:32.100 --> 00:00:34.575
you use a neural
network architecture

15
00:00:34.575 --> 00:00:37.200
to translate from one
language to another.

16
00:00:37.200 --> 00:00:39.300
In this example, we're going to

17
00:00:39.300 --> 00:00:42.160
translate from English to German.

18
00:00:42.160 --> 00:00:44.570
Using an RNN, you have to

19
00:00:44.570 --> 00:00:47.315
take sequential steps
to encode your input,

20
00:00:47.315 --> 00:00:50.360
and you start from the
beginning of your input making

21
00:00:50.360 --> 00:00:54.530
computations at every step
until you reach the end.

22
00:00:54.530 --> 00:00:57.080
At that point, you decode

23
00:00:57.080 --> 00:01:00.920
the information following a
similar sequential procedure.

24
00:01:00.920 --> 00:01:02.690
As you can see here,

25
00:01:02.690 --> 00:01:04.160
you have to go through every word

26
00:01:04.160 --> 00:01:05.870
in your inputs starting with

27
00:01:05.870 --> 00:01:07.640
the first word followed by

28
00:01:07.640 --> 00:01:10.150
the second word,
one after another.

29
00:01:10.150 --> 00:01:13.540
In sequential matcher in order
to start the translation,

30
00:01:13.540 --> 00:01:16.055
that is done in a
sequential way too.

31
00:01:16.055 --> 00:01:18.590
For that reason, there is not

32
00:01:18.590 --> 00:01:21.140
much room for parallel
computations here.

33
00:01:21.140 --> 00:01:24.004
The more words you have
in the input sequence,

34
00:01:24.004 --> 00:01:27.245
the more time it will take
to process that sentence.

35
00:01:27.245 --> 00:01:29.750
Take a look at a more
general sequence

36
00:01:29.750 --> 00:01:31.660
to sequence architecture.

37
00:01:31.660 --> 00:01:34.565
In this case, to
propagate information

38
00:01:34.565 --> 00:01:37.235
from your first word
to the last output,

39
00:01:37.235 --> 00:01:41.450
you have to go through
T sequential steps.

40
00:01:41.450 --> 00:01:45.530
Where T is an integer that
stands for the number of

41
00:01:45.530 --> 00:01:47.855
time-steps that your
model will go through

42
00:01:47.855 --> 00:01:51.140
to process the inputs of
one example sentence.

43
00:01:51.140 --> 00:01:53.780
If let's say for instance,

44
00:01:53.780 --> 00:01:57.770
you are inputting a sentence
that consist of five words,

45
00:01:57.770 --> 00:01:58.820
then the model will take

46
00:01:58.820 --> 00:02:01.115
five times steps to
encode the sentence,

47
00:02:01.115 --> 00:02:04.340
and in this example,
T equals five.

48
00:02:04.340 --> 00:02:06.740
As you may recall from earlier in

49
00:02:06.740 --> 00:02:09.260
the specialization,
with large sequences,

50
00:02:09.260 --> 00:02:12.335
the information tends to get
lost within the network,

51
00:02:12.335 --> 00:02:14.945
and vanishing gradients problems

52
00:02:14.945 --> 00:02:18.290
arise related to the length
of your input sequences.

53
00:02:18.290 --> 00:02:22.605
LSTMs and GRUs help a
little with these problems.

54
00:02:22.605 --> 00:02:25.250
But even those architectures
stop working well

55
00:02:25.250 --> 00:02:28.265
when they try to process
very long sequences.

56
00:02:28.265 --> 00:02:31.115
To recap, there is a
loss of information,

57
00:02:31.115 --> 00:02:33.380
and then there's the
vanishing gradients problem.

58
00:02:33.380 --> 00:02:35.960
Now transformers are
models specially

59
00:02:35.960 --> 00:02:37.550
designed to tackle some

60
00:02:37.550 --> 00:02:39.140
of the problems
that you just saw.

61
00:02:39.140 --> 00:02:41.050
For instance, let's recall

62
00:02:41.050 --> 00:02:43.850
the conventional
encoder-decoder architecture

63
00:02:43.850 --> 00:02:46.610
which is based on RNNs and used

64
00:02:46.610 --> 00:02:49.950
to compute T sequential steps.

65
00:02:49.950 --> 00:02:53.480
In contrast, transformers
are based on

66
00:02:53.480 --> 00:02:55.340
attention and don't require

67
00:02:55.340 --> 00:02:57.935
any sequential
computation per layer,

68
00:02:57.935 --> 00:03:00.145
only one single step is needed.

69
00:03:00.145 --> 00:03:02.780
Additionally, the gradient steps

70
00:03:02.780 --> 00:03:04.220
that need to be taken from

71
00:03:04.220 --> 00:03:05.720
the last output to

72
00:03:05.720 --> 00:03:08.750
the first input in a
transformer is just one.

73
00:03:08.750 --> 00:03:14.985
For RNNs, the number of steps
is equal to T. Finally,

74
00:03:14.985 --> 00:03:17.130
transformers don't
suffer from vanishing

75
00:03:17.130 --> 00:03:18.440
gradients problems that are

76
00:03:18.440 --> 00:03:21.290
related to the length
of the sequences.

77
00:03:21.290 --> 00:03:25.400
Transformer differs from
sequence to sequence by

78
00:03:25.400 --> 00:03:27.380
using multi-head attention layers

79
00:03:27.380 --> 00:03:29.405
instead of recurrent layers.

80
00:03:29.405 --> 00:03:32.585
To understand the intuition
of multi-head attention,

81
00:03:32.585 --> 00:03:34.885
let's first recall
self-attention.

82
00:03:34.885 --> 00:03:38.310
Self-attention uses
directly its query,

83
00:03:38.310 --> 00:03:41.960
key, and value obtained from
every sequential inputs.

84
00:03:41.960 --> 00:03:43.715
Finally, it's out puts

85
00:03:43.715 --> 00:03:46.750
a same length sequential
output for each input.

86
00:03:46.750 --> 00:03:50.120
Self-attention can
be understood as

87
00:03:50.120 --> 00:03:52.175
an intention model
that incorporates

88
00:03:52.175 --> 00:03:54.470
a dense layer for every input,

89
00:03:54.470 --> 00:03:56.885
queries, keys, and values.

90
00:03:56.885 --> 00:03:59.180
Now you can think
of adding a set of

91
00:03:59.180 --> 00:04:02.810
parallel self-attention layers
which are called heads.

92
00:04:02.810 --> 00:04:05.420
These heads are further

93
00:04:05.420 --> 00:04:09.005
concatenated to produce
a single output.

94
00:04:09.005 --> 00:04:11.840
This model is called
multi-head attention

95
00:04:11.840 --> 00:04:13.490
which emulates the recurrence

96
00:04:13.490 --> 00:04:15.695
sequence effect with attention.

97
00:04:15.695 --> 00:04:18.275
Transformers also incorporates

98
00:04:18.275 --> 00:04:21.500
a positional encoding
stage which encodes

99
00:04:21.500 --> 00:04:24.860
each inputs position
in the sequence since

100
00:04:24.860 --> 00:04:26.510
the words order and position is

101
00:04:26.510 --> 00:04:28.415
very important for any language.

102
00:04:28.415 --> 00:04:31.760
For instance, let's suppose
you want to translate

103
00:04:31.760 --> 00:04:36.250
from German the phrase
ich bin glucklich,

104
00:04:36.250 --> 00:04:39.920
and I apologize for
my pronunciation.

105
00:04:39.920 --> 00:04:42.715
Now to capture the
sequential information,

106
00:04:42.715 --> 00:04:46.175
the transformers use
a positional encoding

107
00:04:46.175 --> 00:04:48.710
to retain the
positional information

108
00:04:48.710 --> 00:04:50.135
of the input sequence.

109
00:04:50.135 --> 00:04:52.240
The positional encoding out puts

110
00:04:52.240 --> 00:04:54.980
values to be added
to the embeddings.

111
00:04:54.980 --> 00:04:58.205
That's where every input word
that is given to the model

112
00:04:58.205 --> 00:04:59.735
you have some of the information

113
00:04:59.735 --> 00:05:01.880
about it's order
and the position.

114
00:05:01.880 --> 00:05:03.530
In this case,

115
00:05:03.530 --> 00:05:09.405
a positional coding vector
for each word I-C-H B-I-N,

116
00:05:09.405 --> 00:05:11.810
and glucklich will have

117
00:05:11.810 --> 00:05:14.045
some information
which will tell us

118
00:05:14.045 --> 00:05:16.535
about their respective positions.

119
00:05:16.535 --> 00:05:17.780
Unlike the recurrent layer,

120
00:05:17.780 --> 00:05:20.300
the multi-head attention
layer computes

121
00:05:20.300 --> 00:05:22.835
the outputs of each
inputs in the sequence

122
00:05:22.835 --> 00:05:26.030
independently then it allows

123
00:05:26.030 --> 00:05:28.300
us to parallelize
the computation.

124
00:05:28.300 --> 00:05:29.840
But it fails to model

125
00:05:29.840 --> 00:05:32.720
the sequential information
for a given sequence.

126
00:05:32.720 --> 00:05:34.895
That is why you
need to incorporate

127
00:05:34.895 --> 00:05:38.495
the positional encoding stage
into the transformer model.

128
00:05:38.495 --> 00:05:40.760
In summary, RNNs have

129
00:05:40.760 --> 00:05:41.840
some problems that's come

130
00:05:41.840 --> 00:05:43.595
from their sequential structure.

131
00:05:43.595 --> 00:05:45.800
With RNNs, it is hard to fully

132
00:05:45.800 --> 00:05:48.755
exploit the advantages
of parallel computing.

133
00:05:48.755 --> 00:05:50.420
For long sequences,

134
00:05:50.420 --> 00:05:53.855
important information might
get lost within the network,

135
00:05:53.855 --> 00:05:56.390
and vanishing gradients
problems arise.

136
00:05:56.390 --> 00:05:59.780
But fortunately recent
research has found ways to

137
00:05:59.780 --> 00:06:03.895
solve for the shortcomings of
RNNs by using transformers.

138
00:06:03.895 --> 00:06:06.560
Transformers are a
great alternative to

139
00:06:06.560 --> 00:06:09.920
RNNs that help overcome
these problems in NLP,

140
00:06:09.920 --> 00:06:12.830
and in many fields that
processed sequence data.

141
00:06:12.830 --> 00:06:15.140
Now you understand
why RNNs can be

142
00:06:15.140 --> 00:06:18.005
slow and can have problems
with big contexts.

143
00:06:18.005 --> 00:06:20.480
These are the cases where
transformer can help.

144
00:06:20.480 --> 00:06:21.950
Next, I will show you

145
00:06:21.950 --> 00:06:25.400
a concrete examples of tasks
where transformer is used.

146
00:06:25.400 --> 00:06:27.810
Let's go to the next video.