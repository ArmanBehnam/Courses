WEBVTT

1
00:00:00.270 --> 00:00:04.014
Transformer is one of the most
versatile deep learning models.

2
00:00:04.014 --> 00:00:08.780
It is successfully applied to a number
of tasks, both in NLP and beyond.

3
00:00:08.780 --> 00:00:11.280
Let me show you a few examples.

4
00:00:11.280 --> 00:00:16.970
First, I'll mention the most popular
applications of Transformers in NLP.

5
00:00:16.970 --> 00:00:21.245
Then you will learn what are the states
of the art Transformer models,

6
00:00:21.245 --> 00:00:26.546
including the so-called Text-to-Text
Transfer Transformer, T5 in shorthand.

7
00:00:26.546 --> 00:00:30.181
Finally, you'll see how useful and
versatile T5 is.

8
00:00:30.181 --> 00:00:35.054
Since Transformers can be in general
applied to any sequential task,

9
00:00:35.054 --> 00:00:38.947
just like RNNs,
it has been widely used throughout NLP.

10
00:00:38.947 --> 00:00:40.545
One very interesting and

11
00:00:40.545 --> 00:00:44.210
popular application is
automatic text summarization.

12
00:00:45.370 --> 00:00:50.410
They are also used for auto-completion,
named entity recognition,

13
00:00:50.410 --> 00:00:54.448
automatic question answering,
machine translation.

14
00:00:54.448 --> 00:01:00.748
Another application is Chat-bots, and many
other NLP tasks like Sentiment Analysis,

15
00:01:00.748 --> 00:01:05.035
Market Intelligence,
Text Classification, and others.

16
00:01:05.035 --> 00:01:08.622
Many variants of Transformers
are used in NLP.

17
00:01:08.622 --> 00:01:14.290
And as usual, researchers give
their models their very own names.

18
00:01:14.290 --> 00:01:19.372
So for example, GPT-2, which stands for
Generative Pre-training for

19
00:01:19.372 --> 00:01:24.970
Transformer, is a Transformer created
by open AI with pre training.

20
00:01:24.970 --> 00:01:29.513
It is so good at generating text that
the news magazine, The Economist,

21
00:01:29.513 --> 00:01:32.713
had a reporter asked
the GPT-2 model questions,

22
00:01:32.713 --> 00:01:35.750
as if they were interviewing a person.

23
00:01:35.750 --> 00:01:39.640
And they published the interview
at the end of 2019.

24
00:01:39.640 --> 00:01:44.784
BERT, which stands for Bidirectional
Encoder Representations from Transformers,

25
00:01:44.784 --> 00:01:47.954
and which was created by
the Google AI Language team,

26
00:01:47.954 --> 00:01:52.930
is another famous transformer used for
learning text representations.

27
00:01:52.930 --> 00:01:56.796
T5, which stands for
Text-to-Text Transfer Transformer, and

28
00:01:56.796 --> 00:01:58.434
was also created by Google.

29
00:01:58.434 --> 00:02:00.266
As a multi task Transformer,

30
00:02:00.266 --> 00:02:05.190
this can do question answering
among the lots of different tasks.

31
00:02:05.190 --> 00:02:08.192
So let's dive a little
deeper into the T5 Model.

32
00:02:08.192 --> 00:02:11.626
A single model can learn to
do multiple different tasks.

33
00:02:11.626 --> 00:02:14.900
This is a pretty significant advancement.

34
00:02:14.900 --> 00:02:19.372
For example, let's say you want to
perform tasks such as translation,

35
00:02:19.372 --> 00:02:22.860
classification, and question answering.

36
00:02:22.860 --> 00:02:27.062
Normally you would design an train
one model to perform translation, and

37
00:02:27.062 --> 00:02:30.925
then design an train a second model
to perform classification, and

38
00:02:30.925 --> 00:02:35.480
then design an train a third model
to perform question answering.

39
00:02:35.480 --> 00:02:36.952
But to Transformers,

40
00:02:36.952 --> 00:02:41.525
you can train a single model that is
able to perform all of these tasks.

41
00:02:41.525 --> 00:02:46.607
For instance, to tell the T5 model that
you wanted to perform a certain task,

42
00:02:46.607 --> 00:02:51.689
you will give the model an input string
of texts that includes both the task that

43
00:02:51.689 --> 00:02:57.260
you wanted to do, as well as the data
that you wanted to perform that task on.

44
00:02:57.260 --> 00:03:02.848
For example, if you want the T5 model to
translate a particular sentence like,

45
00:03:02.848 --> 00:03:05.478
I'm happy, from English to German,

46
00:03:05.478 --> 00:03:10.843
you would give the input string translate
English into German, I'm happy.

47
00:03:10.843 --> 00:03:15.382
And the model would be able to
output the sentence, [FOREIGN],

48
00:03:15.382 --> 00:03:20.390
which is the translation of,
I'm happy into German.

49
00:03:20.390 --> 00:03:24.980
And once again, I apologize for
my German pronunciation.

50
00:03:24.980 --> 00:03:28.960
So as you can see, we started with, I'm
happy, and we got the German equivalent.

51
00:03:30.170 --> 00:03:35.561
So this is an example of classification,
where input sentences are classified

52
00:03:35.561 --> 00:03:40.559
into two classes, acceptable when
they make sense, and unacceptable.

53
00:03:40.559 --> 00:03:44.921
In this example, the input string
starts with Cola sentence,

54
00:03:44.921 --> 00:03:49.779
which the model understands is asking
it to classify the sentence that

55
00:03:49.779 --> 00:03:54.290
follows the command as acceptable or
unacceptable.

56
00:03:54.290 --> 00:03:59.606
In this case, the sentence to classify is,
he bought fruits and period.

57
00:03:59.606 --> 00:04:05.680
For instance, the sentence, he bought
fruits and period, is incomplete.

58
00:04:05.680 --> 00:04:09.150
And then it's classified as unacceptable.

59
00:04:09.150 --> 00:04:13.339
Meanwhile, if we give the T5
model this input, Cola sentence,

60
00:04:13.339 --> 00:04:15.559
he bought fruits and vegetables.

61
00:04:15.559 --> 00:04:21.650
The model classifies, he bought fruits and
vegetables, as an acceptable sentence.

62
00:04:21.650 --> 00:04:26.185
If we give the T5 model the inputs
starting with the word, question followed

63
00:04:26.185 --> 00:04:31.450
by a colon, the model then knows that
this is a question answering example.

64
00:04:31.450 --> 00:04:33.732
In this example, the question is,

65
00:04:33.732 --> 00:04:38.450
which volcano in Tanzania is
the highest mountain in Africa?

66
00:04:38.450 --> 00:04:43.946
And your T5 will ask for the answer to
that question, which is Mount Kilimanjaro.

67
00:04:43.946 --> 00:04:48.809
And remember, that's all of these tasks
are done by the same model with no

68
00:04:48.809 --> 00:04:51.956
modification other than
the input sentences.

69
00:04:51.956 --> 00:04:53.133
How cool is that?

70
00:04:53.133 --> 00:04:58.081
Even more, the T5 also performs tasks
of regression and summarization.

71
00:04:58.081 --> 00:05:03.815
Recall that's a regression model is one
that's outputs a continuous numeric value.

72
00:05:03.815 --> 00:05:06.591
Here you can see an example of regression,

73
00:05:06.591 --> 00:05:10.570
which outputs the similarity
between two sentences.

74
00:05:10.570 --> 00:05:15.235
The start of the input string is Stsb,
which indicates to the model that

75
00:05:15.235 --> 00:05:20.420
it should perform a similarity
measurement between two sentences.

76
00:05:20.420 --> 00:05:25.008
The two sentences are denoted by
the word sentence one and sentence two.

77
00:05:25.008 --> 00:05:27.186
The range of possible outputs for

78
00:05:27.186 --> 00:05:31.008
this model is any numeric value
ranging from zero to five.

79
00:05:31.008 --> 00:05:35.444
Where zero indicates that the sentence
is are not similar at all, and

80
00:05:35.444 --> 00:05:38.900
five indicates that
the sentences are very similar.

81
00:05:38.900 --> 00:05:40.971
Let's consider this example.

82
00:05:40.971 --> 00:05:46.168
When comparing sentence one, cats and
dogs are mammals, with the sentence two,

83
00:05:46.168 --> 00:05:51.760
there are four known forces in nature,
gravity, electromagnetic, weak and strong.

84
00:05:51.760 --> 00:05:54.427
The resulting similarity level is zero,

85
00:05:54.427 --> 00:05:57.559
indicating that the sentences
are not similar.

86
00:05:57.559 --> 00:06:00.250
Now let's consider this another example.

87
00:06:00.250 --> 00:06:03.089
Sentence1, cats and dogs are mammals.

88
00:06:03.089 --> 00:06:06.428
And Sentence2, cats,
dogs and cows are domestic.

89
00:06:06.428 --> 00:06:09.569
In this case,
the similarity level maybe 2.6,

90
00:06:09.569 --> 00:06:12.064
if you use a range between zero and five.

91
00:06:12.064 --> 00:06:15.670
Finally, here you can see
an example of summarization.

92
00:06:15.670 --> 00:06:20.447
It is a long story about all the events
and details on an onslaught of

93
00:06:20.447 --> 00:06:24.969
severe weather in Mississippi,
which is summarized just as,

94
00:06:24.969 --> 00:06:29.093
six people hospitalized after
a storm in Attala County.

95
00:06:29.093 --> 00:06:32.766
This is a demo using T5 for
trivia questions, so

96
00:06:32.766 --> 00:06:37.100
that you can compete
against the Transformer.

97
00:06:37.100 --> 00:06:41.847
What makes this demo interesting,
is that T5 was trained in a closed book

98
00:06:41.847 --> 00:06:45.880
setting without access to
any external knowledge.

99
00:06:45.880 --> 00:06:46.650
So let's take a look at it.

100
00:06:51.430 --> 00:06:57.485
So over here you have, what's country
is the largest oil producer in Africa?

101
00:06:57.485 --> 00:06:58.680
So I said Nigeria.

102
00:07:00.000 --> 00:07:01.350
And we both got it's correct.

103
00:07:02.640 --> 00:07:10.390
What color hair did Charles Dickens'
character David Cooperfield have?

104
00:07:11.460 --> 00:07:12.877
So we both got it wrong.

105
00:07:12.877 --> 00:07:14.708
I said pink, and T5 said brown.

106
00:07:18.920 --> 00:07:25.180
Who wrote the 1961 novel,
The Prime of Miss Jean Brodie?

107
00:07:26.870 --> 00:07:29.357
So, I put Murlel Spark.

108
00:07:30.763 --> 00:07:35.605
Well, I use some help there,
but anyways you

109
00:07:35.605 --> 00:07:41.400
can play against the T5 model and
see who wins.

110
00:07:41.400 --> 00:07:44.480
So let's summarize what
we just saw over here.

111
00:07:44.480 --> 00:07:49.395
In this video you saw what
are the Transformers applications in NLP,

112
00:07:49.395 --> 00:07:53.053
which ranged from translation
to summarization.

113
00:07:53.053 --> 00:07:59.670
And then we covered some state of the art
Transformers like GPT-2, BERTS and T5.

114
00:07:59.670 --> 00:08:03.618
And then I showed you how versatile and
powerful T5 is,

115
00:08:03.618 --> 00:08:08.820
as it can perform multiple tasks
using text representations.

116
00:08:08.820 --> 00:08:12.648
Now you know why we need transformer and
where it can be applied.

117
00:08:12.648 --> 00:08:16.771
Isn't it astounding that one model
can handle such a variety of tasks?

118
00:08:16.771 --> 00:08:20.308
I hope you are now eager to
learn how Transformer works, and

119
00:08:20.308 --> 00:08:22.095
that's what I show you next.

120
00:08:22.095 --> 00:08:23.710
Let's go to the next video.