WEBVTT

1
00:00:00.734 --> 00:00:05.184
Let's now get a little deeper into
how attention works by creating some

2
00:00:05.184 --> 00:00:10.243
components for each input that's are
useful for quick information retrieval.

3
00:00:10.243 --> 00:00:13.565
First, though,
I'll give you some intuition for

4
00:00:13.565 --> 00:00:18.510
information retrieval, then move on
to discussing the components used for

5
00:00:18.510 --> 00:00:23.235
calculating attention weights,
called keys, queries, and values.

6
00:00:23.235 --> 00:00:27.778
Before getting deeper into attention
territory, here's some intuition for

7
00:00:27.778 --> 00:00:31.442
how information retrieval works
in the context of attention.

8
00:00:31.442 --> 00:00:33.493
Say you've lost your keys and

9
00:00:33.493 --> 00:00:38.672
have spent a lot of time going over each
meter of your room looking for them.

10
00:00:38.672 --> 00:00:41.607
This is time-consuming and inefficient.

11
00:00:41.607 --> 00:00:45.008
Wisely, you ask your mom
to help you find them.

12
00:00:45.008 --> 00:00:49.856
She thinks for a moment waiting up all
the possibilities for where the keys could

13
00:00:49.856 --> 00:00:53.776
be based on what she already knows
about where they usually are.

14
00:00:53.776 --> 00:00:57.350
Then, she tells you
the likeliest place to look.

15
00:00:57.350 --> 00:01:00.919
Behold, she's right, there are your keys.

16
00:01:00.919 --> 00:01:03.738
And then nutshell,
this is what attention is doing.

17
00:01:03.738 --> 00:01:08.653
Taking a query, selecting the place
where the highest likelihood to look for

18
00:01:08.653 --> 00:01:10.703
the key, then finding the key.

19
00:01:10.703 --> 00:01:14.614
Now, let's peek under the hood
to see that in action.

20
00:01:14.614 --> 00:01:19.972
The attention mechanism uses encoded
representations of both the input or

21
00:01:19.972 --> 00:01:25.168
the encoder hidden states and
the outputs or the decoder hidden states.

22
00:01:25.168 --> 00:01:28.293
The keys and values are pairs.

23
00:01:28.293 --> 00:01:33.090
Both of dimension N,
where N is the input sequence length and

24
00:01:33.090 --> 00:01:36.205
comes from the encoder hidden states.

25
00:01:36.205 --> 00:01:39.978
Keys and values have their
own respective matrices, but

26
00:01:39.978 --> 00:01:43.681
the matrices have the same shape and
are often the same.

27
00:01:43.681 --> 00:01:47.544
While the queries come from
the decoder hidden states,

28
00:01:47.544 --> 00:01:51.405
both the key value pair and
the query enter the attention

29
00:01:51.405 --> 00:01:55.203
layer from their places on
opposite ends of the model.

30
00:01:55.203 --> 00:02:00.383
And once they're inside, the dot product
of the querying and the key is calculated.

31
00:02:00.383 --> 00:02:05.301
This is essentially a measure
of similarity between them, and

32
00:02:05.301 --> 00:02:10.330
the dot product of similar vectors
tends to have a higher value.

33
00:02:10.330 --> 00:02:15.183
The weighted sum given to each value
is determined by the probability that

34
00:02:15.183 --> 00:02:16.993
the key matches the query.

35
00:02:16.993 --> 00:02:19.336
Probability as you might recall,

36
00:02:19.336 --> 00:02:24.264
can be determined by running the attention
wait through the softmax, so

37
00:02:24.264 --> 00:02:29.137
there transform to fit a distribution
of numbers between zero and one.

38
00:02:29.137 --> 00:02:34.678
Then, the query is mapped to the next
key value pair and so on and so forth.

39
00:02:34.678 --> 00:02:38.285
This so
is called scale dot product attention.

40
00:02:38.285 --> 00:02:42.350
Now, let's visualize how the attention
weights look in matrix form.

41
00:02:42.350 --> 00:02:48.363
You have all the inputs or
words of 1, query Q is columns and

42
00:02:48.363 --> 00:02:52.344
the words of the keys (K) as the rows.

43
00:02:52.344 --> 00:02:57.326
That's value score that you'll remember
as the weighted sum from the previous

44
00:02:57.326 --> 00:03:02.157
slide is indicated here in a lighter shade
of gray to show the highest score and

45
00:03:02.157 --> 00:03:03.589
therefore the match.

46
00:03:03.589 --> 00:03:08.176
I've included this one equation just to
help you get familiar with how it looks,

47
00:03:08.176 --> 00:03:09.255
but don't worry,

48
00:03:09.255 --> 00:03:13.921
you'll be getting much cozier with the
math behind attention in upcoming weeks.

49
00:03:13.921 --> 00:03:18.553
Here's an example that shows where the
model is looking when translating between

50
00:03:18.553 --> 00:03:22.124
English and German,
as you'll be doing in your assignments.

51
00:03:22.124 --> 00:03:24.988
In this rather straightforward example,

52
00:03:24.988 --> 00:03:30.407
attention is first translating the English
word how, into the German word wie.

53
00:03:30.407 --> 00:03:33.798
When attention is translating,
the word are,

54
00:03:33.798 --> 00:03:36.611
is looking at the word sind, and so on.

55
00:03:36.611 --> 00:03:41.513
An important thing to keep in mind is
that the model should be flexible enough

56
00:03:41.513 --> 00:03:45.420
to connect each English word
with its relevant German word,

57
00:03:45.420 --> 00:03:50.500
even if they do not appear in the same
position in their respective sentences.

58
00:03:50.500 --> 00:03:55.102
In other words, it should be flexible
enough to handle differences in

59
00:03:55.102 --> 00:03:58.540
grammar and
word ordering in different languages.

60
00:03:58.540 --> 00:04:03.783
So, even though in this example the word
how, is the first word in the English

61
00:04:03.783 --> 00:04:08.637
sentence and the word wie is also
the first word in the German sentence.

62
00:04:08.637 --> 00:04:13.117
The attention model should still be able
to find a connection between the two

63
00:04:13.117 --> 00:04:14.869
words, even if the grammar.

64
00:04:14.869 --> 00:04:20.107
And one language requires a different word
ordering, then the other language does.

65
00:04:20.107 --> 00:04:23.632
In the matrix,
the lighter square shows where the model

66
00:04:23.632 --> 00:04:27.614
is actually looking when making
the translation of that word.

67
00:04:27.614 --> 00:04:31.391
I'll show you how to build
an attention matrix like this one.

68
00:04:31.391 --> 00:04:35.722
You will learn how to properly
interpret the attention matrix and

69
00:04:35.722 --> 00:04:38.565
how to use it to make
the word predictions.

70
00:04:38.565 --> 00:04:42.797
In a situation like the one I just
mentioned, where the grammar of foreign

71
00:04:42.797 --> 00:04:46.209
language requires a difference
word order than the other,

72
00:04:46.209 --> 00:04:49.707
the attention is so
flexible enough to find the connection.

73
00:04:49.707 --> 00:04:54.774
The first four tokens, the agreements
on the, are pretty straightforward,

74
00:04:54.774 --> 00:04:59.554
but then the grammatical structure
between French and English changes.

75
00:04:59.554 --> 00:05:04.299
Now instead of looking at
the corresponding fifth token to translate

76
00:05:04.299 --> 00:05:10.104
the French word zone, the attention knows
to look further down at the eighth token,

77
00:05:10.104 --> 00:05:15.045
which corresponds to the English
word area, glorious and necessary.

78
00:05:15.045 --> 00:05:19.446
It's pretty amazing,
was a little matrix multiplication can do.

79
00:05:19.446 --> 00:05:24.522
To recap all the new concepts, attention
is a layer of calculations that let

80
00:05:24.522 --> 00:05:29.534
your model focus on the most important
parts of the sequence for each step.

81
00:05:29.534 --> 00:05:34.594
Queries, values, and keys
are representations of the encoder and

82
00:05:34.594 --> 00:05:36.467
decoder hidden states.

83
00:05:36.467 --> 00:05:41.736
And they're used to retrieve information
inside the attention layer by calculating

84
00:05:41.736 --> 00:05:46.283
similarity between the decoder queries and
the encoder key value pairs.

85
00:05:46.283 --> 00:05:51.163
This is them is so flexible, it can even
find matches between languages with very

86
00:05:51.163 --> 00:05:54.272
different grammatical structures or
alphabets.

87
00:05:54.272 --> 00:05:56.885
Congrats on absorbing
all these new concepts.

88
00:05:56.885 --> 00:06:00.777
In the next video, I'll be talking
about neural machine translation and

89
00:06:00.777 --> 00:06:03.488
show you what the setup looks like for
this system.

90
00:06:03.488 --> 00:06:06.008
I'll show you what the data
set looks like and

91
00:06:06.008 --> 00:06:10.400
also be talking about the steps required
for pre-processing your data sets.