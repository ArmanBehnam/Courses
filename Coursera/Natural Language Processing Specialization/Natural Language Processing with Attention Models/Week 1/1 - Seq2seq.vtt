WEBVTT

1
00:00:00.390 --> 00:00:01.675
Good to see you again.

2
00:00:01.675 --> 00:00:04.802
You'll now learn about neural
machine translation, and

3
00:00:04.802 --> 00:00:08.579
you'll see what the architecture
of this neural network looks like.

4
00:00:08.579 --> 00:00:12.156
You also learn which words the neural
network is focusing on when

5
00:00:12.156 --> 00:00:15.280
translating from one language,
to another language.

6
00:00:15.280 --> 00:00:17.130
So let's formalize this task.

7
00:00:18.330 --> 00:00:22.637
To get started on this weeks material,
I'll introduce you to neural machine

8
00:00:22.637 --> 00:00:27.690
translation, along with the model that was
traditionally used for its implementation.

9
00:00:27.690 --> 00:00:29.552
The Seq2Seq model.

10
00:00:29.552 --> 00:00:34.435
Then, I'll talk about some of these models
shortcomings and the solution as a lead

11
00:00:34.435 --> 00:00:38.358
into the model that you'll be
using in this week's assignments.

12
00:00:38.358 --> 00:00:41.230
It's exciting stuff, so let's get started.

13
00:00:42.680 --> 00:00:46.842
In neural machine translation you're
using an encoder and a decoder,

14
00:00:46.842 --> 00:00:50.300
to translate from one language to another.

15
00:00:50.300 --> 00:00:54.299
This week you will be translating
from English to German.

16
00:00:54.299 --> 00:00:59.640
To do this, you'll use a machine
translation system that has LSTMs for

17
00:00:59.640 --> 00:01:02.660
both, encoding and decoding.

18
00:01:02.660 --> 00:01:08.221
The traditional Seq2Seq model was
introduced by Google in 2014,

19
00:01:08.221 --> 00:01:10.824
and was a revelation at the time.

20
00:01:10.824 --> 00:01:15.842
Basically, it works by taking one
sequence of items such as words,

21
00:01:15.842 --> 00:01:18.403
and us putting another sequence.

22
00:01:18.403 --> 00:01:23.276
The way it does this is by mapping
variable length sequences to

23
00:01:23.276 --> 00:01:25.860
a fixed length memory.

24
00:01:25.860 --> 00:01:30.283
This feature is what made this model
a powerhouse for machine translation,

25
00:01:30.283 --> 00:01:31.536
among other things.

26
00:01:31.536 --> 00:01:35.950
The inputs and outputs don't
need to have matching lengths.

27
00:01:35.950 --> 00:01:39.871
You might recall the vanishing
gradients problem from earlier in

28
00:01:39.871 --> 00:01:41.232
the specialization.

29
00:01:41.232 --> 00:01:43.291
And Seq2Seq models LSTMs and

30
00:01:43.291 --> 00:01:48.120
GRUs are typically used to
avoid vanishing gradients.

31
00:01:48.120 --> 00:01:50.662
An encoder decoder looks like this.

32
00:01:50.662 --> 00:01:55.746
It takes in a hidden states and a string
of words, such as a single sentence.

33
00:01:55.746 --> 00:01:59.053
The encoder takes the inputs
one step at a time,

34
00:01:59.053 --> 00:02:04.710
collects information for that piece
of inputs, then moves it forward.

35
00:02:04.710 --> 00:02:08.873
The orange rectangle represents
the encoders final hidden states,

36
00:02:08.873 --> 00:02:13.465
which tries to capture all the information
collected from each input step,

37
00:02:13.465 --> 00:02:15.558
before feeding it to the decoder.

38
00:02:15.558 --> 00:02:19.036
This final hidden state
provides the initial states for

39
00:02:19.036 --> 00:02:22.000
the decoder to begin
predicting the sequence.

40
00:02:23.080 --> 00:02:27.116
One major limitation of
the traditional Seq2Seq model,

41
00:02:27.116 --> 00:02:30.999
is what's referred to as
the information bottleneck.

42
00:02:30.999 --> 00:02:35.746
You might have already begun to imagine
what can happen in the case of a long

43
00:02:35.746 --> 00:02:36.521
sequence.

44
00:02:36.521 --> 00:02:41.843
As individual inputs begin stacking
up inside the encoders final

45
00:02:41.843 --> 00:02:46.971
hidden states, because Seq2Seq
uses a fixed length memory,

46
00:02:46.971 --> 00:02:50.274
longer sequences become problematic.

47
00:02:50.274 --> 00:02:55.127
Another issue surfaces as the later
input steps in the sequence

48
00:02:55.127 --> 00:02:57.331
are given more importance.

49
00:02:57.331 --> 00:03:02.960
Which you can see illustrated here,
with the last word of the sentence today.

50
00:03:02.960 --> 00:03:06.976
All of this results in a model
that's performs incredibly well for

51
00:03:06.976 --> 00:03:08.274
shorter sequences.

52
00:03:08.274 --> 00:03:11.150
And not so well for
longer, more complex ones.

53
00:03:12.220 --> 00:03:17.321
So the power of Seq2Seq which lies
in its ability to lets inputs and

54
00:03:17.321 --> 00:03:19.599
outputs be different sizes,

55
00:03:19.599 --> 00:03:24.170
becomes its weakness when
the input itself is a large size.

56
00:03:24.170 --> 00:03:27.219
Because the encoder hidden
states is of a fixed size,

57
00:03:27.219 --> 00:03:31.100
and longer inputs become bottlenecked
on their way to the decoder.

58
00:03:32.160 --> 00:03:36.592
This results in lower model performance
as sequence size increases,

59
00:03:36.592 --> 00:03:37.951
and that's no good.

60
00:03:37.951 --> 00:03:41.847
So the issue with having one
fixed size encoder hidden states,

61
00:03:41.847 --> 00:03:45.093
is that it struggles to
compress longer sequences.

62
00:03:45.093 --> 00:03:48.643
And ends up throttling itself and
punishing the decoder,

63
00:03:48.643 --> 00:03:51.184
who only wants to make a good prediction.

64
00:03:51.184 --> 00:03:53.470
How can you be nicer to the decoder?

65
00:03:54.600 --> 00:03:58.854
You could hold onto each word vector
with this individual information,

66
00:03:58.854 --> 00:04:02.660
instead of trying to smash
it all into one big vector.

67
00:04:02.660 --> 00:04:06.800
But this model still has obvious
flaws with memory and context.

68
00:04:06.800 --> 00:04:08.600
How could you build a time and

69
00:04:08.600 --> 00:04:13.181
memory efficient model that predicts
accurately from a long sequence?

70
00:04:13.181 --> 00:04:16.786
This becomes possible if
the model has a way to select and

71
00:04:16.786 --> 00:04:19.406
focus on the likeliest words each step.

72
00:04:19.406 --> 00:04:25.540
You can think of this as giving the model
a new layer to process this information.

73
00:04:25.540 --> 00:04:29.279
If you provide the information
specific to each input word,

74
00:04:29.279 --> 00:04:34.790
you give the model a way to focus its
attention in the right place at each step.

75
00:04:34.790 --> 00:04:36.441
And that sweet progress.

76
00:04:36.441 --> 00:04:41.851
Up next you'll get a conceptual idea of
what this new layer is doing and why.

77
00:04:46.433 --> 00:04:49.493
You now have an overview of
neural machine translation, and

78
00:04:49.493 --> 00:04:52.260
you have a rough idea of
what's attention looks like.

79
00:04:52.260 --> 00:04:54.949
You know which words
the model is focusing on when

80
00:04:54.949 --> 00:04:57.910
translating from one language
to another language.