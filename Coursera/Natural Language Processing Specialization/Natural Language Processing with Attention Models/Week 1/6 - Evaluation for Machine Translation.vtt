WEBVTT

1
00:00:00.000 --> 00:00:02.535
The BLEU score, which stands for

2
00:00:02.535 --> 00:00:05.415
a Bilingual Evaluation
Understudy.

3
00:00:05.415 --> 00:00:08.550
It's an algorithm that was
developed to solve some of

4
00:00:08.550 --> 00:00:10.020
the most difficult problems in

5
00:00:10.020 --> 00:00:12.810
NLP, including
Machine Translation.

6
00:00:12.810 --> 00:00:17.310
It's evaluates the quality of
machine-translated text by

7
00:00:17.310 --> 00:00:20.025
comparing a candidate
texts translation

8
00:00:20.025 --> 00:00:22.980
to one or more
reference translations.

9
00:00:22.980 --> 00:00:26.520
Usually, the closer the
BLEU score is to one,

10
00:00:26.520 --> 00:00:28.260
the better your model is.

11
00:00:28.260 --> 00:00:30.765
The closer to zero,
the worse it is.

12
00:00:30.765 --> 00:00:32.550
With that said, what is

13
00:00:32.550 --> 00:00:35.865
BLEU score and why is
this an important metric?

14
00:00:35.865 --> 00:00:37.575
To get a BLEU score,

15
00:00:37.575 --> 00:00:39.980
the candidates and
the references are

16
00:00:39.980 --> 00:00:42.805
usually based on
an average of uni,

17
00:00:42.805 --> 00:00:45.830
bi, try or even
four-gram precision.

18
00:00:45.830 --> 00:00:49.300
To demonstrate, I'll use
uni-grams as an example.

19
00:00:49.300 --> 00:00:50.720
Let's say that you have

20
00:00:50.720 --> 00:00:53.510
a candidate sequence
composed of I,

21
00:00:53.510 --> 00:00:56.465
I, am, I, I.

22
00:00:56.465 --> 00:00:59.450
This is what your model
outputs at this step.

23
00:00:59.450 --> 00:01:02.255
Then you have a
reference sequence one,

24
00:01:02.255 --> 00:01:06.180
which contains the words
Younes, I am hungry.

25
00:01:06.180 --> 00:01:09.395
The second reference
sequence contains the words,

26
00:01:09.395 --> 00:01:11.165
he said, I am hungry.

27
00:01:11.165 --> 00:01:14.210
To get the BLEU score
count how many words

28
00:01:14.210 --> 00:01:17.330
and the candidates also
appear in the references.

29
00:01:17.330 --> 00:01:19.815
The candidate you can see I, I,

30
00:01:19.815 --> 00:01:22.230
I, I appeared four times.

31
00:01:22.230 --> 00:01:25.410
Then am also appeared once.

32
00:01:25.410 --> 00:01:27.930
Each word, I and am,

33
00:01:27.930 --> 00:01:31.185
appears once in Reference
1, and Reference 2.

34
00:01:31.185 --> 00:01:34.125
I appears once in each reference.

35
00:01:34.125 --> 00:01:35.400
You're going to write

36
00:01:35.400 --> 00:01:40.060
M underscore W or max
words is equal to one,

37
00:01:40.060 --> 00:01:41.855
and clip the count as one.

38
00:01:41.855 --> 00:01:43.750
Then sum over

39
00:01:43.750 --> 00:01:46.960
the unique uni-gram
counts in the candidates.

40
00:01:46.960 --> 00:01:49.360
Meaning you sum over
the unique counts in

41
00:01:49.360 --> 00:01:51.250
these candidates
and you divide by

42
00:01:51.250 --> 00:01:53.900
the total number of
words in the candidates.

43
00:01:53.900 --> 00:01:56.175
In this case, what would that be?

44
00:01:56.175 --> 00:02:00.885
Well, for the first one
you have I and am, so two.

45
00:02:00.885 --> 00:02:03.040
Then you divide by
the total number of

46
00:02:03.040 --> 00:02:05.915
words in the candidates,
which is five.

47
00:02:05.915 --> 00:02:10.245
Two out of five. This is your
BLEU score. Makes sense?

48
00:02:10.245 --> 00:02:11.930
Like anything in life.

49
00:02:11.930 --> 00:02:13.370
Using the BLEU score as

50
00:02:13.370 --> 00:02:16.265
an evaluation metrics
has some caveats.

51
00:02:16.265 --> 00:02:18.050
For one, it doesn't

52
00:02:18.050 --> 00:02:20.465
consider the semantic
meaning of the words.

53
00:02:20.465 --> 00:02:23.645
It also doesn't consider the
structure of the sentence.

54
00:02:23.645 --> 00:02:25.735
Imagine getting this translation,

55
00:02:25.735 --> 00:02:28.075
"Ate I was hungry because."

56
00:02:28.075 --> 00:02:29.905
If the reference sentence is,

57
00:02:29.905 --> 00:02:31.775
I ate because I was hungry,

58
00:02:31.775 --> 00:02:34.840
this would actually get
a perfect BLEU score.

59
00:02:34.840 --> 00:02:39.740
BLEU score is the most
widely adapted evaluation

60
00:02:39.740 --> 00:02:41.960
metric for machine translation.

61
00:02:41.960 --> 00:02:43.910
But you should be aware of

62
00:02:43.910 --> 00:02:47.170
these drawbacks before
you begin using it.

63
00:02:47.170 --> 00:02:49.040
I'll introduce you now to

64
00:02:49.040 --> 00:02:51.230
another family of
metrics called ROUGE.

65
00:02:51.230 --> 00:02:54.260
It stands for Recall
Oriented Understudy for

66
00:02:54.260 --> 00:02:57.870
Gisting Evaluation,
which is a mouthful.

67
00:02:57.870 --> 00:03:00.230
But let's you know
right off the bat that

68
00:03:00.230 --> 00:03:02.615
it's more recall-oriented
by default.

69
00:03:02.615 --> 00:03:04.370
This means that it's placing

70
00:03:04.370 --> 00:03:06.470
more importance on how much of

71
00:03:06.470 --> 00:03:08.180
the human created reference

72
00:03:08.180 --> 00:03:10.495
appears in the
machine translation.

73
00:03:10.495 --> 00:03:13.370
ROUGE was originally developed to

74
00:03:13.370 --> 00:03:16.490
evaluate the quality of
machine summarized texts,

75
00:03:16.490 --> 00:03:20.330
but is useful for evaluating
machine translation as well.

76
00:03:20.330 --> 00:03:23.270
It works by comparing
the machine texts or

77
00:03:23.270 --> 00:03:26.135
system texts against
the reference texts,

78
00:03:26.135 --> 00:03:28.720
which is often
created by a human.

79
00:03:28.720 --> 00:03:31.549
The ROUGE score
calculates precision,

80
00:03:31.549 --> 00:03:34.400
and recall for a machine
texts by counting

81
00:03:34.400 --> 00:03:36.440
the n-gram overlap between

82
00:03:36.440 --> 00:03:38.990
the machine texts and
a reference texts.

83
00:03:38.990 --> 00:03:40.910
Recall that's an n-gram,

84
00:03:40.910 --> 00:03:43.640
is a list of words that
appear next to each

85
00:03:43.640 --> 00:03:47.000
other in a sentence
where the order matters.

86
00:03:47.000 --> 00:03:48.760
If you have the word,

87
00:03:48.760 --> 00:03:52.770
"I baked a pie," a uni-gram
can be the word baked,

88
00:03:52.770 --> 00:03:55.960
and then bi-gram can be
the two words a pie.

89
00:03:55.960 --> 00:03:57.530
Next, I'll show you

90
00:03:57.530 --> 00:04:00.170
an example of how this
works with uni-grams.

91
00:04:00.170 --> 00:04:03.530
The ROUGE family of
metrics focuses on

92
00:04:03.530 --> 00:04:05.150
the n-gram overlap between

93
00:04:05.150 --> 00:04:07.655
system translated texts
and the reference.

94
00:04:07.655 --> 00:04:09.725
By system translated text,

95
00:04:09.725 --> 00:04:11.270
I'm referring to a model

96
00:04:11.270 --> 00:04:13.405
that's being trained
to do the prediction.

97
00:04:13.405 --> 00:04:15.860
By reference, I'm referring to

98
00:04:15.860 --> 00:04:17.750
the ideal correct sentence

99
00:04:17.750 --> 00:04:20.390
that I want the model to predict.

100
00:04:20.390 --> 00:04:23.000
I mentioned earlier that ROUGE is

101
00:04:23.000 --> 00:04:25.790
primarily recall-oriented
by default.

102
00:04:25.790 --> 00:04:28.655
What I meant by recall
on a high level,

103
00:04:28.655 --> 00:04:32.734
is that if you look at all of
the words in the reference,

104
00:04:32.734 --> 00:04:35.495
which is the cats had orange fur.

105
00:04:35.495 --> 00:04:37.715
How many of the reference words

106
00:04:37.715 --> 00:04:39.950
gets predicted by the model?

107
00:04:39.950 --> 00:04:43.145
The second part of the
equation is precision,

108
00:04:43.145 --> 00:04:46.010
which you can think of as
answering this question.

109
00:04:46.010 --> 00:04:48.710
Of all the words that
the model predicted,

110
00:04:48.710 --> 00:04:50.450
how many of them are words

111
00:04:50.450 --> 00:04:52.795
that we want the
model to predict.

112
00:04:52.795 --> 00:04:56.770
To calculate the recall for
your model translated text.

113
00:04:56.770 --> 00:04:59.865
For each word in the
true reference sentence,

114
00:04:59.865 --> 00:05:02.300
"The cats had orange fur," counts

115
00:05:02.300 --> 00:05:05.300
how many of them are also
predicted by the model?

116
00:05:05.300 --> 00:05:08.160
The, appears in the
model prediction.

117
00:05:08.160 --> 00:05:11.335
Cats, appears in the
prediction as well, had,

118
00:05:11.335 --> 00:05:15.395
appears, orange also
appears, fur, also appears.

119
00:05:15.395 --> 00:05:17.650
In this case, all five of

120
00:05:17.650 --> 00:05:20.755
the reference words are also
predicted by the model.

121
00:05:20.755 --> 00:05:23.050
For the example system texts,

122
00:05:23.050 --> 00:05:26.890
the cat had many orange fur
and the reference texts,

123
00:05:26.890 --> 00:05:29.080
the cats had orange fur.

124
00:05:29.080 --> 00:05:30.960
For the example system texts,

125
00:05:30.960 --> 00:05:33.910
the cats had many orange fur
and the reference texts,

126
00:05:33.910 --> 00:05:35.355
the cats had orange fur.

127
00:05:35.355 --> 00:05:37.550
You can see that
there are a total of

128
00:05:37.550 --> 00:05:39.680
five overlapping uni-grams and

129
00:05:39.680 --> 00:05:41.635
five total words
in the reference.

130
00:05:41.635 --> 00:05:45.710
This would give you a recall
of one, a high score.

131
00:05:45.710 --> 00:05:48.770
If your model wanted to
have a high recall score,

132
00:05:48.770 --> 00:05:51.365
it could just guess hundreds
of thousands of words,

133
00:05:51.365 --> 00:05:53.255
and it would have a
good chance of guessing

134
00:05:53.255 --> 00:05:55.930
all the words in the
true reference sentence.

135
00:05:55.930 --> 00:05:58.005
But what does that
actually tell you?

136
00:05:58.005 --> 00:06:00.135
This is where precision comes in.

137
00:06:00.135 --> 00:06:02.090
To calculate precision, look at

138
00:06:02.090 --> 00:06:04.430
all the words that are
predicted by your model.

139
00:06:04.430 --> 00:06:07.705
In this example, the cats
had striped orange fur.

140
00:06:07.705 --> 00:06:09.560
How many of these predicted words

141
00:06:09.560 --> 00:06:12.020
actually show up in
the correct sentence?

142
00:06:12.020 --> 00:06:15.305
Which is represented by
the reference sentence?

143
00:06:15.305 --> 00:06:20.720
Over here, the, appears
in the reference,

144
00:06:20.720 --> 00:06:23.525
cat, appears in the reference,

145
00:06:23.525 --> 00:06:26.495
had, appears in the reference.

146
00:06:26.495 --> 00:06:29.585
Striped, was predicted
by the model,

147
00:06:29.585 --> 00:06:33.110
but does not appear in
the reference sentence.

148
00:06:33.110 --> 00:06:38.120
Orange, appears in the
reference, and so does fur.

149
00:06:38.120 --> 00:06:42.530
Out of the six words
predicted by the model,

150
00:06:42.530 --> 00:06:46.345
five of them appear in
the reference sentence.

151
00:06:46.345 --> 00:06:48.320
This means that your model has

152
00:06:48.320 --> 00:06:50.720
a precision of five
divided by six,

153
00:06:50.720 --> 00:06:55.375
or roughly 83 percent of
the words were relevant.

154
00:06:55.375 --> 00:06:57.485
There are a few
considerations to be

155
00:06:57.485 --> 00:06:59.720
aware of when using
a ROUGE score.

156
00:06:59.720 --> 00:07:01.160
For one, it focuses on

157
00:07:01.160 --> 00:07:03.560
comparing n-gram counts
to a yield score,

158
00:07:03.560 --> 00:07:06.950
which doesn't allow for
meaningful evaluation of topics.

159
00:07:06.950 --> 00:07:09.320
What this means is that it can

160
00:07:09.320 --> 00:07:11.780
only count word overlap
as a measure of

161
00:07:11.780 --> 00:07:15.380
similarity and misses
any broader contexts

162
00:07:15.380 --> 00:07:17.435
that words are describing.

163
00:07:17.435 --> 00:07:20.720
For example, if two sentences
being compared were,

164
00:07:20.720 --> 00:07:22.910
I am a fruit-field pastry,

165
00:07:22.910 --> 00:07:24.865
and I'm a jelly donut.

166
00:07:24.865 --> 00:07:27.290
ROUGE would have no way
of understanding that

167
00:07:27.290 --> 00:07:30.205
the two sentences actually
mean the same thing.

168
00:07:30.205 --> 00:07:31.980
Because of this limitation,

169
00:07:31.980 --> 00:07:35.285
it's can take a similar
or synonymous concepts

170
00:07:35.285 --> 00:07:37.805
into consideration when
the score is computed.

171
00:07:37.805 --> 00:07:40.610
A low ROUGE score may not reflect

172
00:07:40.610 --> 00:07:43.690
that a model translated
text actually captured

173
00:07:43.690 --> 00:07:46.170
all the same relevant content as

174
00:07:46.170 --> 00:07:48.380
the reference texts
just because it's

175
00:07:48.380 --> 00:07:50.960
had a large difference
in n-gram overlap.

176
00:07:50.960 --> 00:07:53.570
But ROUGE scores are
still very useful for

177
00:07:53.570 --> 00:07:56.420
evaluation of machine
translations and summaries.

178
00:07:56.420 --> 00:07:59.960
These are just a couple of
caveats to keep in mind as

179
00:07:59.960 --> 00:08:02.110
you start taking your
own ROUGE scores.

180
00:08:02.110 --> 00:08:03.600
Here's a quick recap.

181
00:08:03.600 --> 00:08:06.529
You now are familiar with
the blue square algorithm,

182
00:08:06.529 --> 00:08:08.210
which was created to evaluate

183
00:08:08.210 --> 00:08:10.460
machine translations and how it

184
00:08:10.460 --> 00:08:12.695
operates by comparing
candidate texts

185
00:08:12.695 --> 00:08:15.560
against one or more
reference translations.

186
00:08:15.560 --> 00:08:17.120
By taking the average of

187
00:08:17.120 --> 00:08:19.310
the n-grams in the candidates and

188
00:08:19.310 --> 00:08:21.140
comparing that to the average of

189
00:08:21.140 --> 00:08:23.795
the n-grams in the
reference translations.

190
00:08:23.795 --> 00:08:28.235
Then clipping this average
to fit on a scale of 0-1.

191
00:08:28.235 --> 00:08:30.140
You are now aware of a couple of

192
00:08:30.140 --> 00:08:32.315
drawbacks to using
BLEU score as well,

193
00:08:32.315 --> 00:08:34.490
ROUGE 2.0 is also

194
00:08:34.490 --> 00:08:36.320
a valuable metric for measuring

195
00:08:36.320 --> 00:08:38.300
the relevance of
machine translations.

196
00:08:38.300 --> 00:08:42.380
That works by counting
overlapping uni or bi-grams

197
00:08:42.380 --> 00:08:44.480
in a machine translated text

198
00:08:44.480 --> 00:08:47.090
and comparing them to
a reference texts,

199
00:08:47.090 --> 00:08:50.280
which is usually
created by a human.