WEBVTT

1
00:00:00.000 --> 00:00:01.590
Good to see you again.

2
00:00:01.590 --> 00:00:03.540
You will now explore triplets.

3
00:00:03.540 --> 00:00:06.240
You'll see how you can
build pairs of inputs.

4
00:00:06.240 --> 00:00:08.805
Rather than just classifying
what's an input is,

5
00:00:08.805 --> 00:00:11.130
you're going to build something
that will allow you to

6
00:00:11.130 --> 00:00:13.740
identify the difference
between two inputs.

7
00:00:13.740 --> 00:00:15.405
Let's see how this works.

8
00:00:15.405 --> 00:00:18.705
Here are three questions
where the first one,

9
00:00:18.705 --> 00:00:20.940
how old are you, is the anchor.

10
00:00:20.940 --> 00:00:24.615
The second one is a positive
example, what is your age?

11
00:00:24.615 --> 00:00:28.530
The third one, where are you
from is a negative example.

12
00:00:28.530 --> 00:00:30.690
Having the three
components here is

13
00:00:30.690 --> 00:00:32.865
what gives rise to
the name triplets,

14
00:00:32.865 --> 00:00:35.820
which is to say, an
anchor being used in

15
00:00:35.820 --> 00:00:39.120
conjunction with a positive
and negative pairing.

16
00:00:39.120 --> 00:00:42.110
Accordingly, triplet
loss is the name

17
00:00:42.110 --> 00:00:45.170
for a loss function that
uses three components.

18
00:00:45.170 --> 00:00:48.890
The intuition behind this
simple function is to

19
00:00:48.890 --> 00:00:50.420
minimize the difference between

20
00:00:50.420 --> 00:00:53.270
the similarity of A and N,

21
00:00:53.270 --> 00:00:57.980
and the similarity of A and
P. You already know that,

22
00:00:57.980 --> 00:00:59.750
as the difference gets bigger

23
00:00:59.750 --> 00:01:02.030
or smaller along the x axis,

24
00:01:02.030 --> 00:01:05.755
the loss gets bigger or
smaller along the y axis.

25
00:01:05.755 --> 00:01:09.530
But notice, when the
difference is less than zero,

26
00:01:09.530 --> 00:01:12.395
do you also want the loss
to be less than zero?

27
00:01:12.395 --> 00:01:14.885
Let's think about
this for a moment.

28
00:01:14.885 --> 00:01:18.215
If you gave the model
a positive loss value,

29
00:01:18.215 --> 00:01:21.455
the model uses this to update
its weight to improve.

30
00:01:21.455 --> 00:01:24.520
If you gave the model
a negative loss value,

31
00:01:24.520 --> 00:01:27.140
this is like telling
the model, "Good job.

32
00:01:27.140 --> 00:01:29.960
Please update your weight
to the worst next time."

33
00:01:29.960 --> 00:01:32.120
So you don't actually
want to give

34
00:01:32.120 --> 00:01:34.655
the model a loss value
that's less than zero.

35
00:01:34.655 --> 00:01:38.045
In other words, when the
model is doing a good job,

36
00:01:38.045 --> 00:01:39.440
you don't want it to
undo a [inaudible].

37
00:01:39.440 --> 00:01:41.450
To make sure that

38
00:01:41.450 --> 00:01:43.880
the model doesn't update
itself to do worse,

39
00:01:43.880 --> 00:01:45.650
you can modify the loss so that

40
00:01:45.650 --> 00:01:47.855
whenever the diff
is less than zero,

41
00:01:47.855 --> 00:01:49.775
the loss should just be zero.

42
00:01:49.775 --> 00:01:51.560
When the loss is zero,

43
00:01:51.560 --> 00:01:53.300
we're effectively not asking

44
00:01:53.300 --> 00:01:55.285
the model to update it's weights,

45
00:01:55.285 --> 00:01:56.850
because it is performing as

46
00:01:56.850 --> 00:01:59.595
expected for that
training example.

47
00:01:59.595 --> 00:02:03.680
The loss-function now cannot
take on negative values.

48
00:02:03.680 --> 00:02:06.740
If the difference is
less than or equal to 0,

49
00:02:06.740 --> 00:02:08.285
the loss is 0.

50
00:02:08.285 --> 00:02:10.745
If the difference
is greater than 0,

51
00:02:10.745 --> 00:02:13.315
then the loss is equal
to the difference.

52
00:02:13.315 --> 00:02:15.875
Notice the non-linearity happens

53
00:02:15.875 --> 00:02:18.545
at the origin of this line chart.

54
00:02:18.545 --> 00:02:21.680
But you might also wonder
what's happens when

55
00:02:21.680 --> 00:02:25.015
the model is correct but
only by a tiny bits?

56
00:02:25.015 --> 00:02:26.780
The model is still correct if

57
00:02:26.780 --> 00:02:28.505
the difference is a tiny number,

58
00:02:28.505 --> 00:02:30.035
that is less than zero.

59
00:02:30.035 --> 00:02:31.940
What if you want
the model to still

60
00:02:31.940 --> 00:02:33.500
learn from this example,

61
00:02:33.500 --> 00:02:34.820
and ask it to predict

62
00:02:34.820 --> 00:02:37.780
a wider difference for
this training example?

63
00:02:37.780 --> 00:02:39.650
You can think of shifting

64
00:02:39.650 --> 00:02:42.185
this loss function a
little to the left,

65
00:02:42.185 --> 00:02:45.085
by a margin that we'll
refer to as Alpha.

66
00:02:45.085 --> 00:02:48.375
Let's say we chose
Alpha to be 0.2,

67
00:02:48.375 --> 00:02:51.695
if the difference between
similarities is very small,

68
00:02:51.695 --> 00:02:53.784
like negative 0.1,

69
00:02:53.784 --> 00:02:57.215
then if you add it
to the Alpha of 0.2,

70
00:02:57.215 --> 00:02:59.525
the result is still
greater than 0.

71
00:02:59.525 --> 00:03:03.095
The sum of the diff plus
Alpha can be considered

72
00:03:03.095 --> 00:03:04.820
a positive loss that tells

73
00:03:04.820 --> 00:03:06.955
the model to learn
from this example.

74
00:03:06.955 --> 00:03:09.770
You can see this visually
in the line chart.

75
00:03:09.770 --> 00:03:11.510
The loss function is shifted

76
00:03:11.510 --> 00:03:13.570
to the left by the amount Alpha.

77
00:03:13.570 --> 00:03:16.830
The diff is along
the horizontal axis.

78
00:03:16.830 --> 00:03:18.390
When the difference is less than

79
00:03:18.390 --> 00:03:20.825
zero but small in magnitude,

80
00:03:20.825 --> 00:03:22.975
the loss is greater than zero.

81
00:03:22.975 --> 00:03:26.770
So if the difference is smaller
in magnitude than Alpha,

82
00:03:26.770 --> 00:03:28.660
then there is still a loss.

83
00:03:28.660 --> 00:03:31.210
This loss tells the
model that it can still

84
00:03:31.210 --> 00:03:34.475
improve and learn from
this training example.

85
00:03:34.475 --> 00:03:37.885
Triplet loss, as the difference
with a margin Alpha,

86
00:03:37.885 --> 00:03:39.880
is what you will implement in

87
00:03:39.880 --> 00:03:43.420
the assignments which
you will code like this,

88
00:03:43.420 --> 00:03:45.850
which is the triplet
loss function for A,

89
00:03:45.850 --> 00:03:49.440
P and N. A small
detail worth noting.

90
00:03:49.440 --> 00:03:53.020
In these explanations,
I've been using similarity

91
00:03:53.020 --> 00:03:54.580
because that's what will be

92
00:03:54.580 --> 00:03:56.740
used in the programming
assignments,

93
00:03:56.740 --> 00:03:59.215
so similarity of v_1, v _2.

94
00:03:59.215 --> 00:04:01.510
But if you were to
read the literature,

95
00:04:01.510 --> 00:04:03.775
you might find d of v_1,

96
00:04:03.775 --> 00:04:07.070
v_2 used also, where
this d could be

97
00:04:07.070 --> 00:04:08.540
any function that calculates

98
00:04:08.540 --> 00:04:11.080
the distance between two vectors.

99
00:04:11.080 --> 00:04:13.040
A distance metric is

100
00:04:13.040 --> 00:04:16.055
the mirror image of
a similarity metric,

101
00:04:16.055 --> 00:04:17.975
and a similarity metric

102
00:04:17.975 --> 00:04:20.450
can be derived from
a distance metric.

103
00:04:20.450 --> 00:04:24.855
One example of a distance
metric is Euclidean distance.

104
00:04:24.855 --> 00:04:27.380
Selecting triplets A, P,

105
00:04:27.380 --> 00:04:31.205
and N for training
involves two steps; first,

106
00:04:31.205 --> 00:04:34.040
select a pair of questions
that are known to be

107
00:04:34.040 --> 00:04:37.360
duplicates to serve as
the anchor and positive,

108
00:04:37.360 --> 00:04:40.799
and you'll do this from
the training set; second,

109
00:04:40.799 --> 00:04:43.685
select a question
that is known to be

110
00:04:43.685 --> 00:04:46.550
difference in meaning
from the anchor,

111
00:04:46.550 --> 00:04:49.315
to form the anchor and
the negative pair.

112
00:04:49.315 --> 00:04:52.325
If you were to select
triplets at random,

113
00:04:52.325 --> 00:04:56.255
you'd be likely to select
non-duplicative pairs A and N,

114
00:04:56.255 --> 00:04:57.955
where the loss is 0.

115
00:04:57.955 --> 00:04:59.660
The loss is zero whenever

116
00:04:59.660 --> 00:05:02.690
the model correctly
predicts that A and

117
00:05:02.690 --> 00:05:04.820
P are more similar relative

118
00:05:04.820 --> 00:05:07.430
to A and N. When the loss is 0,

119
00:05:07.430 --> 00:05:08.960
the network has nothing more to

120
00:05:08.960 --> 00:05:10.765
learn from the triplets example.

121
00:05:10.765 --> 00:05:13.460
So we can train more
efficiently if we choose

122
00:05:13.460 --> 00:05:16.300
triplets that show the
model when it's incorrect,

123
00:05:16.300 --> 00:05:19.175
so that's just going to adjust
it's weight and improve.

124
00:05:19.175 --> 00:05:21.965
Instead of selecting
random triplets,

125
00:05:21.965 --> 00:05:25.684
you'll specifically select
so-called hard triplets.

126
00:05:25.684 --> 00:05:29.485
That is, triplets that are
more difficult to train on.

127
00:05:29.485 --> 00:05:32.420
Hard triplets are those
where the similarity

128
00:05:32.420 --> 00:05:36.440
between anchor and
negative is very close to,

129
00:05:36.440 --> 00:05:38.390
but still smaller than

130
00:05:38.390 --> 00:05:41.465
the similarity between
anchor and positive.

131
00:05:41.465 --> 00:05:44.410
When the model encounters
a hard triplet,

132
00:05:44.410 --> 00:05:47.780
the learning algorithm
needs to adjust its weight,

133
00:05:47.780 --> 00:05:49.820
so that's it's going
to yield similarities

134
00:05:49.820 --> 00:05:52.345
that line up with the
real-world labels.

135
00:05:52.345 --> 00:05:54.544
So by selecting hard triplets,

136
00:05:54.544 --> 00:05:56.630
focusing the training
on doing better,

137
00:05:56.630 --> 00:05:58.130
on the difficult cases,

138
00:05:58.130 --> 00:06:00.730
that it's predicting incorrectly.

139
00:06:00.730 --> 00:06:04.130
I spoke about easy
and hard triplets.

140
00:06:04.130 --> 00:06:06.155
I also spoke about a margin.

141
00:06:06.155 --> 00:06:08.990
In the next video, you'll see
how all these concepts come

142
00:06:08.990 --> 00:06:12.810
together to help us
create a cost function.