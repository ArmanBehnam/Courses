WEBVTT

1
00:00:00.000 --> 00:00:03.540
Welcome back. To train
a neural network,

2
00:00:03.540 --> 00:00:05.310
you need to compute gradients.

3
00:00:05.310 --> 00:00:08.280
You've done it by hand earlier
in this specialization,

4
00:00:08.280 --> 00:00:11.040
and you've seen that it
can be quite complex.

5
00:00:11.040 --> 00:00:12.840
But you know what?
You don't have to

6
00:00:12.840 --> 00:00:14.745
necessarily do it by yourself.

7
00:00:14.745 --> 00:00:16.440
Deep learning
frameworks can do it

8
00:00:16.440 --> 00:00:18.570
for you. Let's dive in.

9
00:00:18.570 --> 00:00:20.010
I will show you how

10
00:00:20.010 --> 00:00:21.585
the trax grad function

11
00:00:21.585 --> 00:00:23.895
allows you to easily
compute gradients,

12
00:00:23.895 --> 00:00:25.530
which will allow you to perform

13
00:00:25.530 --> 00:00:28.410
back-propagation and
train your model.

14
00:00:28.410 --> 00:00:30.450
You will see how easy it is

15
00:00:30.450 --> 00:00:32.970
compared to back
propagating by hand.

16
00:00:32.970 --> 00:00:34.740
Computing ingredients using

17
00:00:34.740 --> 00:00:36.805
Trax is pretty straightforward.

18
00:00:36.805 --> 00:00:39.245
Suppose that you have
the following equation,

19
00:00:39.245 --> 00:00:41.900
f of x, whose gradient

20
00:00:41.900 --> 00:00:45.095
with respect to x is
equal to this derivative.

21
00:00:45.095 --> 00:00:47.040
To get that derivative in Trax,

22
00:00:47.040 --> 00:00:50.295
define the function
f that takes in x,

23
00:00:50.295 --> 00:00:52.715
and then just call
the grad function

24
00:00:52.715 --> 00:00:55.384
with f as its single parameter.

25
00:00:55.384 --> 00:00:58.520
The function grad will
return a function

26
00:00:58.520 --> 00:01:02.550
that computes the gradient
of f. That's nice.

27
00:01:02.560 --> 00:01:05.120
Using the grad function to train

28
00:01:05.120 --> 00:01:07.415
a model is similarly painless.

29
00:01:07.415 --> 00:01:10.955
Suppose that you have a
neural network model y.

30
00:01:10.955 --> 00:01:12.770
To get the gradient
of your model,

31
00:01:12.770 --> 00:01:14.780
just apply the grad function with

32
00:01:14.780 --> 00:01:18.700
the forward method of your
model as a single parameter.

33
00:01:18.700 --> 00:01:22.595
Then evaluate the gradient
with your weights and inputs.

34
00:01:22.595 --> 00:01:25.115
Notice the double
sets of parentheses.

35
00:01:25.115 --> 00:01:29.585
The first one passes the
arguments for the grad function,

36
00:01:29.585 --> 00:01:31.010
and the second one,

37
00:01:31.010 --> 00:01:34.955
the arguments for the
function returned by grad.

38
00:01:34.955 --> 00:01:38.105
After you have the
gradients for your model,

39
00:01:38.105 --> 00:01:41.045
just iterate until
convergence is reached.

40
00:01:41.045 --> 00:01:44.240
That's it, forward
and back-propagation

41
00:01:44.240 --> 00:01:46.205
performed in a single line.

42
00:01:46.205 --> 00:01:49.580
This final block is
gradient descent.

43
00:01:49.580 --> 00:01:51.110
You can always change

44
00:01:51.110 --> 00:01:54.245
the optimization
algorithm if necessary.

45
00:01:54.245 --> 00:01:56.404
So let's summarize.

46
00:01:56.404 --> 00:01:58.920
Having a defined model in Trax,

47
00:01:58.920 --> 00:02:01.190
make training
significantly easier than

48
00:02:01.190 --> 00:02:03.920
computing forward and
back-propagation by

49
00:02:03.920 --> 00:02:06.875
hand because the
built-in grad function

50
00:02:06.875 --> 00:02:09.500
automatically computes
everything you need.

51
00:02:09.500 --> 00:02:12.350
In the programming
assignments from this module,

52
00:02:12.350 --> 00:02:13.910
you'll be able to define and

53
00:02:13.910 --> 00:02:16.565
train a neural
network using Trax.

54
00:02:16.565 --> 00:02:19.090
Good luck, and have fun.

55
00:02:19.090 --> 00:02:22.130
This was the last
video of the week.

56
00:02:22.130 --> 00:02:24.440
You now know how to
create neural networks

57
00:02:24.440 --> 00:02:27.035
with layers and
how to train them.

58
00:02:27.035 --> 00:02:29.870
Next, you will learn
even more complex layers

59
00:02:29.870 --> 00:02:32.200
and built networks
that perform better.

60
00:02:32.200 --> 00:02:34.900
Younes will show you how.