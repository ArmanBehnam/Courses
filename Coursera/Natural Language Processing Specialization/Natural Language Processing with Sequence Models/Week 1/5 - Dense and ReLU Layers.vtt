WEBVTT

1
00:00:00.000 --> 00:00:03.780
Now, I'll show you
dense and ReLU layers.

2
00:00:03.780 --> 00:00:08.160
First, I'll show you dense
layers and then ReLU layers.

3
00:00:08.160 --> 00:00:09.750
Suppose that you have

4
00:00:09.750 --> 00:00:12.075
a simple serial
model like this one.

5
00:00:12.075 --> 00:00:14.805
Let's focus on the first
parts of the model.

6
00:00:14.805 --> 00:00:17.550
Here, you have an input vector X

7
00:00:17.550 --> 00:00:20.475
fully connected to a
layer of activations.

8
00:00:20.475 --> 00:00:24.000
To get the value of z, let's
go into the activations.

9
00:00:24.000 --> 00:00:26.430
You will have to compute
the inner products between

10
00:00:26.430 --> 00:00:29.850
a set of trainable weights
and the input vector.

11
00:00:29.850 --> 00:00:33.780
This single computation
is called a dense layer.

12
00:00:33.780 --> 00:00:36.630
The ReLU layer is much simpler.

13
00:00:36.630 --> 00:00:39.655
Let's take the same model
you've been working with.

14
00:00:39.655 --> 00:00:42.830
The ReLU layer is
an activation layer

15
00:00:42.830 --> 00:00:46.055
that typically follows a
dense fully connected layer,

16
00:00:46.055 --> 00:00:48.530
and transforms any
negative values to

17
00:00:48.530 --> 00:00:51.910
zero before sending them
onto the next layer.

18
00:00:51.910 --> 00:00:55.640
To do this, the ReLU layer
computes the function g,

19
00:00:55.640 --> 00:00:57.800
which returns a value of zero

20
00:00:57.800 --> 00:01:00.065
for all negative values of z,

21
00:01:00.065 --> 00:01:02.645
and z for all positive ones.

22
00:01:02.645 --> 00:01:05.780
You've now seen the dense
layer and the ReLU layer.

23
00:01:05.780 --> 00:01:09.630
Next, I'll show you how
to put a model together.