WEBVTT

1
00:00:00.000 --> 00:00:02.130
Hi. My name is Lukasz,

2
00:00:02.130 --> 00:00:04.170
and I want to tell
you in this video

3
00:00:04.170 --> 00:00:07.845
why we made the machine
learning library Trax.

4
00:00:07.845 --> 00:00:11.870
This is a little bit of
a personal story for me.

5
00:00:11.870 --> 00:00:15.075
I've been at Google for
about seven years now.

6
00:00:15.075 --> 00:00:17.370
I'm a researcher in
the Google Brain Team.

7
00:00:17.370 --> 00:00:18.960
But before I was a researcher,

8
00:00:18.960 --> 00:00:20.415
I was a software engineer.

9
00:00:20.415 --> 00:00:22.020
I worked on a lot of

10
00:00:22.020 --> 00:00:25.180
machine learning
projects and frameworks.

11
00:00:25.550 --> 00:00:29.550
This journey for me ended
in the Trax library.

12
00:00:29.550 --> 00:00:33.630
I believe Trax is currently
the best library to learn

13
00:00:33.630 --> 00:00:35.100
and to productionize

14
00:00:35.100 --> 00:00:37.915
machine learning research
and machine learning models,

15
00:00:37.915 --> 00:00:40.880
especially sequenced models,
models like transformer

16
00:00:40.880 --> 00:00:44.935
and models that are used in
natural language processing.

17
00:00:44.935 --> 00:00:48.800
The reasons I believe
that come from

18
00:00:48.800 --> 00:00:52.475
a personal journey that
I took that led me here.

19
00:00:52.475 --> 00:00:54.350
I will tell you a
little bit about

20
00:00:54.350 --> 00:00:56.465
myself and how I came here,

21
00:00:56.465 --> 00:00:59.120
and then I'll tell
you why I think Trax

22
00:00:59.120 --> 00:01:02.510
is the best thing to use
currently for machine learning,

23
00:01:02.510 --> 00:01:05.430
especially in natural
language processing.

24
00:01:07.570 --> 00:01:10.520
My journey with machine learning

25
00:01:10.520 --> 00:01:12.440
and machine learning
frameworks started

26
00:01:12.440 --> 00:01:18.710
around 2014-15 when we
were making TensorFlow.

27
00:01:18.710 --> 00:01:21.380
TensorFlow is, you probably know,

28
00:01:21.380 --> 00:01:23.645
is a big machine
learning systems,

29
00:01:23.645 --> 00:01:27.320
it has about 100 million
downloads by now.

30
00:01:27.320 --> 00:01:31.085
It was released in November 2015.

31
00:01:31.085 --> 00:01:33.230
It was a very emotional moment

32
00:01:33.230 --> 00:01:36.110
for all of us when we
were releasing it.

33
00:01:36.110 --> 00:01:38.660
At that point, we were not sure

34
00:01:38.660 --> 00:01:42.705
if deep learning will
become as big as it did.

35
00:01:42.705 --> 00:01:46.220
We were not sure how many
users there will be.

36
00:01:46.220 --> 00:01:48.110
What we wanted to do was a system

37
00:01:48.110 --> 00:01:50.075
that's primarily very fast,

38
00:01:50.075 --> 00:01:53.585
that can run distributed
machine learning systems,

39
00:01:53.585 --> 00:01:57.000
large-scale fast training.

40
00:01:57.170 --> 00:01:59.670
The main focus was speed.

41
00:01:59.670 --> 00:02:02.390
A secondary focus was to make

42
00:02:02.390 --> 00:02:05.450
it easy to program the
systems that wasn't a reader,

43
00:02:05.450 --> 00:02:08.670
but it was not the
most important thing.

44
00:02:08.710 --> 00:02:11.810
After releasing
TensorFlow, I worked on

45
00:02:11.810 --> 00:02:15.424
machine translation
and especially

46
00:02:15.424 --> 00:02:18.150
on the Google's Neural
Machine Translation System.

47
00:02:18.150 --> 00:02:22.235
This was the first system
using deep sequence models

48
00:02:22.235 --> 00:02:24.140
that was used by

49
00:02:24.140 --> 00:02:25.700
the Google Translate
team that was

50
00:02:25.700 --> 00:02:27.635
actually released as a product.

51
00:02:27.635 --> 00:02:31.010
It's handling all of Google
translations these days.

52
00:02:31.010 --> 00:02:36.890
Every language that we
have has a neural model.

53
00:02:36.890 --> 00:02:40.145
It started with LSTMs
and RNN models,

54
00:02:40.145 --> 00:02:43.230
and now it's a lot
of transformers.

55
00:02:44.150 --> 00:02:46.860
We released that in

56
00:02:46.860 --> 00:02:51.495
2016 based on the
TensorFlow framework.

57
00:02:51.495 --> 00:02:53.815
These models, they're amazing.

58
00:02:53.815 --> 00:02:55.990
They're much better than

59
00:02:55.990 --> 00:02:58.765
the previous phrase-based
translation models,

60
00:02:58.765 --> 00:03:01.900
but they took a
long time to train.

61
00:03:01.900 --> 00:03:04.020
They were training for days

62
00:03:04.020 --> 00:03:07.880
on clusters of GPUs at that time.

63
00:03:07.880 --> 00:03:10.540
This was not practical for anyone

64
00:03:10.540 --> 00:03:12.580
else to do rather than Google.

65
00:03:12.580 --> 00:03:15.880
This was only because we
had this TensorFlow system,

66
00:03:15.880 --> 00:03:19.165
a large group of engineers
who would ferry very well,

67
00:03:19.165 --> 00:03:21.805
and we were training
for days and days.

68
00:03:21.805 --> 00:03:23.710
That was great. But I felt

69
00:03:23.710 --> 00:03:25.490
like this is not

70
00:03:25.490 --> 00:03:27.725
satisfactory because no
one else can do that.

71
00:03:27.725 --> 00:03:30.275
It's not possible to be
done at the university.

72
00:03:30.275 --> 00:03:32.150
You cannot launch a
startup doing that,

73
00:03:32.150 --> 00:03:35.165
because it was impossible
if you were not Google,

74
00:03:35.165 --> 00:03:36.695
or maybe from Microsoft,

75
00:03:36.695 --> 00:03:38.680
but no one else.

76
00:03:38.680 --> 00:03:41.070
I wanted to change that.

77
00:03:41.070 --> 00:03:44.985
To do that, we created the
Tensor2Tensor Library.

78
00:03:44.985 --> 00:03:49.445
The Tensor2Tensor Library,
which was released in 2017,

79
00:03:49.445 --> 00:03:51.380
started with the thought that we

80
00:03:51.380 --> 00:03:53.630
should make this deep
learning research,

81
00:03:53.630 --> 00:03:57.910
especially for sequence
models, widely accessible.

82
00:03:57.910 --> 00:04:02.120
This was not working with
these large RNN models,

83
00:04:02.120 --> 00:04:04.954
but while writing the library,

84
00:04:04.954 --> 00:04:07.235
we created this
transformer model.

85
00:04:07.235 --> 00:04:10.190
This transformer has
taken NLP by storm

86
00:04:10.190 --> 00:04:13.615
because it allows you
to train much faster.

87
00:04:13.615 --> 00:04:15.820
At that time within
a few days, now,

88
00:04:15.820 --> 00:04:17.390
it's less than a
day in a matter of

89
00:04:17.390 --> 00:04:19.640
hours on an 8 GPU system.

90
00:04:19.640 --> 00:04:21.830
You can create
translation models that

91
00:04:21.830 --> 00:04:24.480
surpass any RNN models.

92
00:04:24.480 --> 00:04:30.785
The Tensor2Tensor library has
become already widely used.

93
00:04:30.785 --> 00:04:33.425
It's used in production
Google systems.

94
00:04:33.425 --> 00:04:36.250
It's used by some
very large companies

95
00:04:36.250 --> 00:04:38.340
in the world and it
has led to a number of

96
00:04:38.340 --> 00:04:40.250
startups that they know about

97
00:04:40.250 --> 00:04:43.820
that basically exists
thanks to this library.

98
00:04:43.820 --> 00:04:47.410
You can say, well, this
is done and this is good,

99
00:04:47.410 --> 00:04:49.755
but, the problem is,

100
00:04:49.755 --> 00:04:52.530
it's become complicated and it's

101
00:04:52.530 --> 00:04:54.860
not nice to learn

102
00:04:54.860 --> 00:04:58.350
and it's become very hard
to do new researcher.

103
00:04:58.350 --> 00:05:02.705
Around 2018, we decided
it's time to improve.

104
00:05:02.705 --> 00:05:04.365
As time moves on,

105
00:05:04.365 --> 00:05:06.570
we need to do even better,

106
00:05:06.570 --> 00:05:10.055
and this is how we created Trax.

107
00:05:10.055 --> 00:05:12.560
Trax is a deep-learning library

108
00:05:12.560 --> 00:05:15.640
that's focused on
clear code and speed.

109
00:05:15.640 --> 00:05:18.825
Let me tell you why, so,

110
00:05:18.825 --> 00:05:20.900
if you think carefully

111
00:05:20.900 --> 00:05:23.235
what you want from a
deep-learning library,

112
00:05:23.235 --> 00:05:25.625
there are really two
things that matters.

113
00:05:25.625 --> 00:05:27.120
You want the the
programmers to be

114
00:05:27.120 --> 00:05:30.735
efficient and you want
the code to run fast,

115
00:05:30.735 --> 00:05:32.830
and this is because

116
00:05:33.140 --> 00:05:37.400
what costs you is the
time of the programmer,

117
00:05:37.400 --> 00:05:39.340
and the money you need to

118
00:05:39.340 --> 00:05:41.475
pay for running
your training code.

119
00:05:41.475 --> 00:05:43.820
Programmer's time
is very important.

120
00:05:43.820 --> 00:05:45.605
You need to use it efficiently,

121
00:05:45.605 --> 00:05:47.320
but in deep learning
you're training

122
00:05:47.320 --> 00:05:50.100
big models and these
costs money too.

123
00:05:50.100 --> 00:05:54.485
For example, using eight GPUs
on-demand from the Cloud,

124
00:05:54.485 --> 00:05:57.820
can cost $20 an hour almost.

125
00:05:57.820 --> 00:06:00.630
But using the
preemptible eight could

126
00:06:00.630 --> 00:06:04.145
TPU costs only $1.40.

127
00:06:04.145 --> 00:06:07.165
In Trax, you can use
one or the other

128
00:06:07.165 --> 00:06:10.870
without changing a single
character in your code.

129
00:06:11.310 --> 00:06:15.030
How does Trax make
programmers sufficient?

130
00:06:15.030 --> 00:06:17.855
Well, it was redesigned from

131
00:06:17.855 --> 00:06:21.315
the bottom-up to be easy
to debug and understand.

132
00:06:21.315 --> 00:06:23.445
You can literally read

133
00:06:23.445 --> 00:06:26.220
Trax code and understand
what's going to come.

134
00:06:26.220 --> 00:06:29.900
This is not the case in
some other libraries,

135
00:06:29.900 --> 00:06:33.120
this is unluckily of the
case anymore in TensorFlow.

136
00:06:33.120 --> 00:06:35.195
But, you can say, well
it used to be the case,

137
00:06:35.195 --> 00:06:37.130
but nowadays TensorFlow,

138
00:06:37.130 --> 00:06:39.065
even when we clean up the code,

139
00:06:39.065 --> 00:06:41.280
it needs to be
backwards compatible.

140
00:06:41.280 --> 00:06:44.990
It carries the weight of
these years of development,

141
00:06:44.990 --> 00:06:48.465
and this is crazy errors
of Machine Learning.

142
00:06:48.465 --> 00:06:50.880
There is a lot of
baggage that it just

143
00:06:50.880 --> 00:06:53.735
has to carry because it's
backward compatible.

144
00:06:53.735 --> 00:06:55.670
What we do in Trax is we

145
00:06:55.670 --> 00:06:58.110
break the backwards
compatibility.

146
00:06:58.110 --> 00:07:01.475
This means you need
to learn new things.

147
00:07:01.475 --> 00:07:03.385
This carries some price.

148
00:07:03.385 --> 00:07:05.975
But what you get for that price,

149
00:07:05.975 --> 00:07:09.525
is that it's a newly
cleanly designed library

150
00:07:09.525 --> 00:07:11.725
which has four models,

151
00:07:11.725 --> 00:07:13.475
not just primitives
to build them,

152
00:07:13.475 --> 00:07:16.255
but also four models
with dataset bindings,

153
00:07:16.255 --> 00:07:17.950
we regression test these models

154
00:07:17.950 --> 00:07:19.830
daily because we use
these libraries,

155
00:07:19.830 --> 00:07:23.200
so we know every day
these monster running.

156
00:07:23.780 --> 00:07:27.110
It's like a new
programming language.

157
00:07:27.110 --> 00:07:30.110
It costs a little bit to learn,

158
00:07:30.110 --> 00:07:32.250
this is a new thing, but it

159
00:07:32.250 --> 00:07:34.925
makes your life much
more efficient.

160
00:07:34.925 --> 00:07:37.425
To make this point point clear,

161
00:07:37.425 --> 00:07:39.820
the Adam Optimizer,
the most popular

162
00:07:39.820 --> 00:07:42.885
optimizer in machine
learning timesteps.

163
00:07:42.885 --> 00:07:44.610
On the left, you

164
00:07:44.610 --> 00:07:48.765
see a screenshot from the
paper that introduced data,

165
00:07:48.765 --> 00:07:54.000
and you see it has like
about seven lines.

166
00:07:54.000 --> 00:07:57.340
Next is just a part

167
00:07:57.340 --> 00:07:59.465
of the Adam implementation
and patronage,

168
00:07:59.465 --> 00:08:02.355
which is one of the
cleanest ones actually

169
00:08:02.355 --> 00:08:06.920
and you need to know way more,

170
00:08:06.920 --> 00:08:09.815
you need to know what
are parameter groups,

171
00:08:09.815 --> 00:08:12.820
you need to know secret keys into

172
00:08:12.820 --> 00:08:16.485
these groups that key
parameters by some means,

173
00:08:16.485 --> 00:08:21.215
you need to do seven stick
initialization and some

174
00:08:21.215 --> 00:08:26.875
conditional to introduce
either and other things.

175
00:08:26.875 --> 00:08:30.750
On the right, you see
the Adam optimizer in

176
00:08:30.750 --> 00:08:34.890
TensorFlow and Keras and as
you'll see it's even longer.

177
00:08:34.890 --> 00:08:37.690
You need to apply it to

178
00:08:37.690 --> 00:08:40.110
resource variables and two
non-research variables

179
00:08:40.110 --> 00:08:42.530
and you need to know
what these are.

180
00:08:42.530 --> 00:08:46.385
The reason they
exist is historical.

181
00:08:46.385 --> 00:08:48.450
Currently we only use
resource variables,

182
00:08:48.450 --> 00:08:50.325
but we have to support people

183
00:08:50.325 --> 00:08:53.580
who used the old
non-research variables too.

184
00:08:53.580 --> 00:08:56.000
There are a lot of things that in

185
00:08:56.000 --> 00:08:58.775
2020 you actually
don't need anymore,

186
00:08:58.775 --> 00:09:00.300
but they have to be there and

187
00:09:00.300 --> 00:09:02.755
painted and in TensorFlow code.

188
00:09:02.755 --> 00:09:07.305
While if you go to Trax code,

189
00:09:07.305 --> 00:09:09.885
this is the full code
of Adam and Trax.

190
00:09:09.885 --> 00:09:12.405
It's very similar for the paper.

191
00:09:12.405 --> 00:09:13.935
That's the whole point.

192
00:09:13.935 --> 00:09:15.480
Because if you're implementing

193
00:09:15.480 --> 00:09:19.814
a new paper or if you're
learning and you want to find,

194
00:09:19.814 --> 00:09:21.600
in the code of the framework,

195
00:09:21.600 --> 00:09:24.315
where are the equations
from the paper,

196
00:09:24.315 --> 00:09:26.830
you can really do with this here.

197
00:09:27.260 --> 00:09:31.600
So that is the benefit of Trax.

198
00:09:31.700 --> 00:09:34.050
The price of this benefit

199
00:09:34.050 --> 00:09:36.740
is that you're using a new thing.

200
00:09:36.740 --> 00:09:39.090
But there is a huge gain that

201
00:09:39.090 --> 00:09:41.760
comes to you when you're
actually debugging your code.

202
00:09:41.760 --> 00:09:43.250
When you're debugging your code,

203
00:09:43.250 --> 00:09:47.220
you will hit lines that
are in the framework.

204
00:09:47.220 --> 00:09:51.945
So you will actually need
to understand these lines,

205
00:09:51.945 --> 00:09:54.365
which means you need
to understand all of

206
00:09:54.365 --> 00:09:55.980
these PyTorch and all of

207
00:09:55.980 --> 00:09:58.560
these TensorFlow
if you use those.

208
00:09:58.560 --> 00:10:00.465
But in Trax, you only

209
00:10:00.465 --> 00:10:02.370
need to understand
these Trax lines.

210
00:10:02.370 --> 00:10:04.155
It's much easier to debug,

211
00:10:04.155 --> 00:10:06.900
which makes programmers
more efficient.

212
00:10:06.900 --> 00:10:09.365
Now this efficiency would not be

213
00:10:09.365 --> 00:10:12.195
worth that much if the
code is running slow.

214
00:10:12.195 --> 00:10:14.145
Hey, there's a lot of

215
00:10:14.145 --> 00:10:16.990
beautiful things where you

216
00:10:16.990 --> 00:10:18.450
can program things in a few line,

217
00:10:18.450 --> 00:10:21.630
but the run so slowly that
it's actually useless.

218
00:10:21.630 --> 00:10:24.990
Not so in Trax because we use

219
00:10:24.990 --> 00:10:27.590
the just-in-time compiler
technology that was

220
00:10:27.590 --> 00:10:30.360
built in the last six
years of TensorFlow.

221
00:10:30.360 --> 00:10:34.635
It's called XLA, and we
use it on top of Trax.

222
00:10:34.635 --> 00:10:37.605
These teams have put
tremendous effort

223
00:10:37.605 --> 00:10:40.815
to make this coat the
fastest code on the planet.

224
00:10:40.815 --> 00:10:44.010
There is an industry
competition called MLPerf.

225
00:10:44.010 --> 00:10:47.535
In 2020, JAX actually
won this competition,

226
00:10:47.535 --> 00:10:49.950
being the fastest transformer

227
00:10:49.950 --> 00:10:53.430
to ever be benchmarked
independently.

228
00:10:53.430 --> 00:10:58.080
So JAX transformer ran
in 0.26 of a minute,

229
00:10:58.080 --> 00:11:00.720
so in about 16 seconds, I think,

230
00:11:00.720 --> 00:11:03.120
while the fastest
TensorFlow transformer on

231
00:11:03.120 --> 00:11:05.730
the same hardware
took 0.35 minutes.

232
00:11:05.730 --> 00:11:09.000
So you see, it's almost
50 percent slower.

233
00:11:09.000 --> 00:11:14.975
The fastest PyTorch, but this
was not on TPU, took 0.62.

234
00:11:14.975 --> 00:11:19.650
So being two times faster
is significant game.

235
00:11:19.650 --> 00:11:21.900
It's not clear you'll
get the same gain in

236
00:11:21.900 --> 00:11:25.200
any model on other hardware.

237
00:11:25.200 --> 00:11:27.570
There was a lot of
work to tune it for

238
00:11:27.570 --> 00:11:29.845
this particular model hardware.

239
00:11:29.845 --> 00:11:34.410
But in general, Trax runs fast.

240
00:11:34.410 --> 00:11:37.545
This means, you'll pay less for

241
00:11:37.545 --> 00:11:40.875
the TPUs and GPUs you'll
be running on Cloud.

242
00:11:40.875 --> 00:11:44.310
It's also tested
with TPUs on Colab.

243
00:11:44.310 --> 00:11:46.560
Colabs are the IPython notebooks

244
00:11:46.560 --> 00:11:48.495
that Google gives you for free.

245
00:11:48.495 --> 00:11:50.955
You can select a
hardware accelerator,

246
00:11:50.955 --> 00:11:55.740
you can select TPU and run the
same code with no changes.

247
00:11:55.740 --> 00:11:58.520
It's GPU, TPU, or CPU,

248
00:11:58.520 --> 00:12:00.540
on this Colab, where
you're getting

249
00:12:00.540 --> 00:12:02.475
an eight-code TPU for free.

250
00:12:02.475 --> 00:12:05.460
So you can test your code
there and then run it on

251
00:12:05.460 --> 00:12:09.015
Cloud for much cheaper
than other frameworks,

252
00:12:09.015 --> 00:12:10.875
and it really runs fast.

253
00:12:10.875 --> 00:12:13.800
So these are the
reasons to use Trax,

254
00:12:13.800 --> 00:12:17.130
and for me, Trax
is also super fun.

255
00:12:17.130 --> 00:12:18.525
It's super fun to learn,

256
00:12:18.525 --> 00:12:20.535
it's super fun to use,

257
00:12:20.535 --> 00:12:23.855
because we had the liberty
to do things from scratch

258
00:12:23.855 --> 00:12:28.835
using many years
of experience now.

259
00:12:28.835 --> 00:12:32.850
You can write model
using combinators.

260
00:12:32.850 --> 00:12:36.135
This is a whole transformer
language model on the left.

261
00:12:36.135 --> 00:12:39.605
On the right, you can
see it's from a README.

262
00:12:39.605 --> 00:12:41.890
This is everything
you need to run

263
00:12:41.890 --> 00:12:45.000
a pre-trained model and
get your translations.

264
00:12:45.000 --> 00:12:47.220
So this gave us

265
00:12:47.220 --> 00:12:49.275
the opportunity to
clean up the framework,

266
00:12:49.275 --> 00:12:53.430
clean up the code, make
sure it runs really fast.

267
00:12:53.430 --> 00:12:56.400
It's a lot of fun to use.

268
00:12:56.400 --> 00:13:00.365
So I encourage you
come check it out.

269
00:13:00.365 --> 00:13:02.220
See how you can use Trax for

270
00:13:02.220 --> 00:13:06.690
your own machine learning
endeavors, both for research.

271
00:13:06.690 --> 00:13:07.980
If you want to start

272
00:13:07.980 --> 00:13:12.975
a startup or if you want to
run it for a big company,

273
00:13:12.975 --> 00:13:17.080
I think Trax will
be there for you.