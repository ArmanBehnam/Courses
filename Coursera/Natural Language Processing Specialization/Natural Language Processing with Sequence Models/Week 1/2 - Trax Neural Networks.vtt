WEBVTT

1
00:00:00.000 --> 00:00:02.565
To create a neural
network out of layers,

2
00:00:02.565 --> 00:00:04.395
you need to put them together.

3
00:00:04.395 --> 00:00:06.420
In a deep network,
you run them one

4
00:00:06.420 --> 00:00:08.370
after the other in
a sequential way.

5
00:00:08.370 --> 00:00:10.560
Let me show you how
you can do this.

6
00:00:10.560 --> 00:00:12.615
First, you will observe how

7
00:00:12.615 --> 00:00:15.555
a basic neural network
is defined in Trax.

8
00:00:15.555 --> 00:00:17.670
Then I'll show you some of

9
00:00:17.670 --> 00:00:20.745
the benefits of using a
framework like TensorFlow,

10
00:00:20.745 --> 00:00:23.430
which is the framework
that Trax is built on.

11
00:00:23.430 --> 00:00:26.430
Let's take this network
architecture as an example.

12
00:00:26.430 --> 00:00:28.950
In this model, you have
two hidden layers with

13
00:00:28.950 --> 00:00:31.320
sigmoid activation functions and

14
00:00:31.320 --> 00:00:33.960
an output layer with
softmax activation.

15
00:00:33.960 --> 00:00:35.810
In Trax, you'll need to

16
00:00:35.810 --> 00:00:38.380
specify the type of
model architecture.

17
00:00:38.380 --> 00:00:40.925
For simple architectures
like this one,

18
00:00:40.925 --> 00:00:42.950
you'll use a serial model.

19
00:00:42.950 --> 00:00:46.355
To start, list the layers
from left to right,

20
00:00:46.355 --> 00:00:49.430
or from your input variables
to the output layer.

21
00:00:49.430 --> 00:00:51.320
In this case, first you

22
00:00:51.320 --> 00:00:53.915
have a dense layer
with four units,

23
00:00:53.915 --> 00:00:55.580
and then assign

24
00:00:55.580 --> 00:00:58.510
the sigmoid activation
function to that layer.

25
00:00:58.510 --> 00:01:01.130
After that, repeat the process

26
00:01:01.130 --> 00:01:03.935
for the second hidden layer
and the output layer.

27
00:01:03.935 --> 00:01:06.200
You can specify any architecture

28
00:01:06.200 --> 00:01:07.970
you like in the simple way.

29
00:01:07.970 --> 00:01:11.390
Note that, this way to specify
your models architecture,

30
00:01:11.390 --> 00:01:12.920
follows the order in which

31
00:01:12.920 --> 00:01:16.145
the computations are made
in your neural network.

32
00:01:16.145 --> 00:01:18.110
There are several advantages

33
00:01:18.110 --> 00:01:20.450
to using libraries like Trax,

34
00:01:20.450 --> 00:01:22.820
such as they're
designed to perform

35
00:01:22.820 --> 00:01:26.720
computations efficiently
in hardware like CPUs,

36
00:01:26.720 --> 00:01:29.495
GPUs, and even TPUs.

37
00:01:29.495 --> 00:01:33.440
They allow you to easily
perform parallel computing by

38
00:01:33.440 --> 00:01:34.670
running gear models on

39
00:01:34.670 --> 00:01:38.270
multiple machines or
course simultaneously.

40
00:01:38.270 --> 00:01:41.825
They keep a record of all
the algebraic operations

41
00:01:41.825 --> 00:01:45.050
on your neural net in the
order of computation.

42
00:01:45.050 --> 00:01:47.195
So they are able to
compute the gradients

43
00:01:47.195 --> 00:01:49.490
of your model automatically.

44
00:01:49.490 --> 00:01:52.145
There are many open source
frameworks out there,

45
00:01:52.145 --> 00:01:54.235
and Trax is one of the latest.

46
00:01:54.235 --> 00:01:56.130
It's based on TensorFlow.

47
00:01:56.130 --> 00:01:57.980
You might be already
familiar with

48
00:01:57.980 --> 00:02:01.235
TensorFlow, PyTorch, and JAX.

49
00:02:01.235 --> 00:02:03.515
If you're not familiar
with those, don't worry.

50
00:02:03.515 --> 00:02:06.065
I'll show you the basics
of Trax and you'll be

51
00:02:06.065 --> 00:02:09.200
able to implement
amazing NLP models.

52
00:02:09.200 --> 00:02:12.425
So far, I showed
you how to define

53
00:02:12.425 --> 00:02:16.220
a model in Trax with the simple
sequential architecture,

54
00:02:16.220 --> 00:02:19.730
and I pointed out some of
the advantages to be had,

55
00:02:19.730 --> 00:02:23.465
like computational efficiency
and parallel computing.

56
00:02:23.465 --> 00:02:28.680
Next, I'll get into more
detail on how to use Trax.