WEBVTT

1
00:00:00.430 --> 00:00:01.680
Welcome back.

2
00:00:01.680 --> 00:00:04.160
There are many types of RNN architectures.

3
00:00:04.160 --> 00:00:07.920
Sometimes you're trying to build
the sentiment classifier for a tweet.

4
00:00:07.920 --> 00:00:10.670
In that case,
you might use one type of model.

5
00:00:10.670 --> 00:00:14.145
Another time, you might want to
generate a caption for an image, and

6
00:00:14.145 --> 00:00:16.885
then you might want to use
a different architecture.

7
00:00:16.885 --> 00:00:20.325
So there are many different types of
architectures, and in this video,

8
00:00:20.325 --> 00:00:22.405
we will show you all the different types.

9
00:00:23.765 --> 00:00:26.650
There are many different
types of tasks in AI.

10
00:00:26.650 --> 00:00:31.360
Here, I'll group them according to
the nature of their inputs and outputs.

11
00:00:31.360 --> 00:00:36.135
For instance, a one-to-one task
takes as input a set of low or

12
00:00:36.135 --> 00:00:40.950
non-correlated features x, and
returns a single output y.

13
00:00:40.950 --> 00:00:43.560
Say you had a list of scores
from your favorite team

14
00:00:43.560 --> 00:00:47.740
in the European soccer tournament,
LaLiga Santander.

15
00:00:47.740 --> 00:00:52.200
As inputs, you could have
a recurrent neural network that

16
00:00:52.200 --> 00:00:55.880
predicted the position of
your team on the leaderboard.

17
00:00:55.880 --> 00:01:00.300
However, notice that this recurrent
neural network isn't much different

18
00:01:00.300 --> 00:01:02.500
from a conventional neural network.

19
00:01:03.595 --> 00:01:10.020
It only has an additional hidden
state h superscript t subscript 0.

20
00:01:10.020 --> 00:01:14.660
So for this type of task,
RNNs aren't all that useful.

21
00:01:14.660 --> 00:01:18.350
But if you want a neural network
that takes an arbitrary image and

22
00:01:18.350 --> 00:01:22.520
generates a caption describing
the contents of that image in English,

23
00:01:22.520 --> 00:01:26.570
in this case, a brown puppy,
you can build an RNN.

24
00:01:26.570 --> 00:01:30.200
This type of task is known as one-to-many,

25
00:01:30.200 --> 00:01:35.260
because your RNN takes a single image and
generates multiple words to describe it.

26
00:01:36.400 --> 00:01:39.560
Another type of task is many-to-one.

27
00:01:39.560 --> 00:01:43.710
You may already be familiar with
one practical example of this type,

28
00:01:43.710 --> 00:01:45.900
sentiment analysis.

29
00:01:45.900 --> 00:01:49.440
In that instance,
if you have a sequence of words,

30
00:01:49.440 --> 00:01:53.810
like this tweet that says,
I'm very happy as input, and

31
00:01:53.810 --> 00:01:59.550
your model should output whether the tweet
has a negative or positive sentiment.

32
00:01:59.550 --> 00:02:05.105
Your RNN would take every word from
the sequence as inputs in different steps,

33
00:02:05.105 --> 00:02:10.112
propagate information from the beginning
to the end, and output the sentiment.

34
00:02:11.282 --> 00:02:17.020
Finally, many-to-many tasks involve
multiple inputs and multiple outputs.

35
00:02:17.020 --> 00:02:22.800
For example, in machine translation,
you have a sequence of words in French.

36
00:02:22.800 --> 00:02:27.340
You want to get its equivalent in
another language, such as English.

37
00:02:27.340 --> 00:02:29.270
An RNN would do well for

38
00:02:29.270 --> 00:02:34.520
this task, because they propagate
information from the beginning to end.

39
00:02:34.520 --> 00:02:37.890
And this is what makes them able
to capture the general meaning

40
00:02:37.890 --> 00:02:39.960
of word sequences.

41
00:02:39.960 --> 00:02:44.600
This particular architecture,
encoder-decoder, is very popular for

42
00:02:44.600 --> 00:02:46.410
machine translation.

43
00:02:46.410 --> 00:02:51.130
The first part of this neural network
doesn't return any output, y hat, and

44
00:02:51.130 --> 00:02:53.160
is called an encoder.

45
00:02:53.160 --> 00:02:57.200
Because it encodes sequences of words
in a single representation that

46
00:02:57.200 --> 00:02:59.480
captures the overall
meaning of the sentence,

47
00:03:00.650 --> 00:03:05.360
that is decoded later to a sequence
of words in the other language.

48
00:03:05.360 --> 00:03:07.350
RNNs are powerful architectures,

49
00:03:07.350 --> 00:03:10.880
this can be used to solve
a lot of interesting problems.

50
00:03:10.880 --> 00:03:14.760
In NLP, there are multiple applications
where recurrent neural networks

51
00:03:14.760 --> 00:03:19.430
are useful, such as machine
translation and caption generation.

52
00:03:19.430 --> 00:03:22.420
You can think of RNNs as versatile tools

53
00:03:22.420 --> 00:03:24.910
that can be shaped
according to the situation.

54
00:03:26.160 --> 00:03:28.230
Depending on the task
you're trying to solve,

55
00:03:28.230 --> 00:03:30.880
you can choose one of these architectures.

56
00:03:30.880 --> 00:03:34.780
In the next video, I'll talk about
a simple recurrent neural network, and

57
00:03:34.780 --> 00:03:37.180
then you can apply it to
all these architectures.