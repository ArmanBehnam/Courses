WEBVTT

1
00:00:00.370 --> 00:00:04.350
Hi again.
I will now show you how to implement RNNs.

2
00:00:04.350 --> 00:00:08.570
I'll be talking about scan functions,
which are just like abstracts RNNs,

3
00:00:08.570 --> 00:00:10.950
which allow for faster computation.

4
00:00:10.950 --> 00:00:13.700
So what's are scan functions,
and how do you implement them?

5
00:00:14.830 --> 00:00:19.740
To do this, I will show you how the
function scan is implemented in tensorFlow

6
00:00:19.740 --> 00:00:22.129
for computing the forward pass in RNNs.

7
00:00:23.232 --> 00:00:26.850
The scan function is designed
to take a function fn, and

8
00:00:26.850 --> 00:00:32.460
apply it to all of the elements from the
beginning to the end in the list elems.

9
00:00:32.460 --> 00:00:36.870
Initializer is an optional variable that
could be used in the first computation

10
00:00:36.870 --> 00:00:38.240
of fn.

11
00:00:38.240 --> 00:00:42.840
Now take this RNN where
fn is equivalent to fw.

12
00:00:42.840 --> 00:00:47.284
Elems is the list with all
the inputs x superscript t, and

13
00:00:47.284 --> 00:00:52.665
the initializer is the hidden state
as h superscript t subscript 0.

14
00:00:52.665 --> 00:00:58.012
The scan function first initializes
the hidden states as h superscript

15
00:00:58.012 --> 00:01:05.020
t subscript 0, and sets the ys which store
the prediction values as an empty list.

16
00:01:05.020 --> 00:01:10.150
Then for every x in the list of elements,
fn is called with x,

17
00:01:10.150 --> 00:01:13.595
and the value of the last
hidden state is arguments.

18
00:01:13.595 --> 00:01:18.095
So this four loop computes
every time step of the RNN, and

19
00:01:18.095 --> 00:01:21.685
stores the values of the prediction and
hidden states.

20
00:01:21.685 --> 00:01:26.625
Finally, the function returns the list of
predictions, and the last hidden state.

21
00:01:26.625 --> 00:01:28.665
You might think this
function is unnecessary,

22
00:01:28.665 --> 00:01:33.765
because it is essentially a four loop
through every time step of the RNN.

23
00:01:33.765 --> 00:01:38.450
However, Frameworks like Tensorflow
need this type of abstraction

24
00:01:38.450 --> 00:01:42.100
in order to perform parallel computations,
and run on GPUs.

25
00:01:43.340 --> 00:01:46.420
I showed you how the scan
function is defined in Tensorflow

26
00:01:46.420 --> 00:01:48.450
to mimic how RNNs work.

27
00:01:48.450 --> 00:01:53.050
It is important to know that these types
of abstractions are needed for deep

28
00:01:53.050 --> 00:01:57.700
learning Frameworks, because they allow
them to use GPUs, and compute in parallel.

29
00:01:57.700 --> 00:01:59.440
Now that you know about scan functions,

30
00:01:59.440 --> 00:02:01.970
you will use them in this
week's programming assignments.