WEBVTT

1
00:00:00.131 --> 00:00:01.340
Hi again.

2
00:00:01.340 --> 00:00:05.010
Deep recurrent neural networks are useful
because they allow you to capture

3
00:00:05.010 --> 00:00:10.480
dependencies that you could not have
otherwise captured using a shallow RNN.

4
00:00:10.480 --> 00:00:14.290
In this video, you'll understand
the equations used when implementing these

5
00:00:14.290 --> 00:00:19.320
deep RNNs, and I'll show you how that
factors in into the cost function.

6
00:00:19.320 --> 00:00:20.881
So let's dive in.

7
00:00:20.881 --> 00:00:24.262
I will show you how bidirectional
neural networks work and

8
00:00:24.262 --> 00:00:28.550
how stacking RNNs together can
produce a deep neural network.

9
00:00:28.550 --> 00:00:31.842
To illustrate the importance
of bidirectional RNNs,

10
00:00:31.842 --> 00:00:33.637
take the following example.

11
00:00:33.637 --> 00:00:36.473
I was trying really hard
to get a hold of blank.

12
00:00:36.473 --> 00:00:40.465
Louise finally answered when
I was about to give up.

13
00:00:40.465 --> 00:00:41.525
As a clever human,

14
00:00:41.525 --> 00:00:45.514
you would be able to fill in the blank
without having to think very hard.

15
00:00:45.514 --> 00:00:49.923
An RNN that propagates information
from the beginning to the end of

16
00:00:49.923 --> 00:00:53.267
sequences would be able
to make a prediction too.

17
00:00:53.267 --> 00:00:57.257
It would take the words
before the blank as input and

18
00:00:57.257 --> 00:00:59.747
do its best to predict the missing word.

19
00:00:59.747 --> 00:01:05.241
However, because Louise doesn't appear
until the beginning of the next sentence,

20
00:01:05.241 --> 00:01:08.611
it would have to guess between her,
him, and them.

21
00:01:08.611 --> 00:01:13.900
Bidirectional RNNs work in much
the same way that simple RNNs do.

22
00:01:13.900 --> 00:01:18.490
They take an input sequence x and
make the predictions y-hat.

23
00:01:18.490 --> 00:01:20.790
In the RNNs I showed you earlier,

24
00:01:20.790 --> 00:01:24.940
the information flows from the beginning
to the end of the sequence.

25
00:01:24.940 --> 00:01:27.920
However, you could have
another architecture

26
00:01:27.920 --> 00:01:31.170
where the information flowed
from the end to the beginning.

27
00:01:31.170 --> 00:01:33.950
Just imagine going from the future
to the present instead.

28
00:01:35.050 --> 00:01:39.930
When information flows in both directions,
that's a bidirectional RNN.

29
00:01:39.930 --> 00:01:43.310
It is important for you to know
that this isn't a cyclic graph,

30
00:01:43.310 --> 00:01:48.030
which means that the information flows
independently in both directions.

31
00:01:48.030 --> 00:01:50.620
So the computations from left to right

32
00:01:50.620 --> 00:01:54.850
are completely independent of
the computations from right to left.

33
00:01:54.850 --> 00:01:59.050
To get the predictions, y-hat,
in a bidirectional RNN,

34
00:01:59.050 --> 00:02:03.320
you have to start propagating
information from both directions.

35
00:02:03.320 --> 00:02:07.340
When you have computed both of
the hidden states for a time step,

36
00:02:07.340 --> 00:02:11.820
you can get the prediction y-hat for
that time using this formula,

37
00:02:11.820 --> 00:02:15.030
which is the same one used for
unidirectional or

38
00:02:15.030 --> 00:02:19.900
vanilla RNNs, but
appending both hidden states this time.

39
00:02:19.900 --> 00:02:22.150
After you compute all
the hidden states for

40
00:02:22.150 --> 00:02:26.780
both directions, you can get all
of the remaining predictions.

41
00:02:26.780 --> 00:02:30.810
Deep RNNs are similar to
regular deep neural networks.

42
00:02:30.810 --> 00:02:35.160
Deep RNNs have a layer which
takes the input sequence x and

43
00:02:35.160 --> 00:02:37.440
multiple additional hidden layers.

44
00:02:37.440 --> 00:02:42.110
As you can see,
deep RNNs are just RNNs stuck together.

45
00:02:42.110 --> 00:02:46.000
The intermediate connections pass
information through the values of

46
00:02:46.000 --> 00:02:52.050
activations, just as in conventional deep
neural networks, but for every time step.

47
00:02:52.050 --> 00:02:56.810
For vanilla deep RNNs,
you have the following two equations.

48
00:02:56.810 --> 00:02:59.730
They're the same as the ones
that you have seen before.

49
00:02:59.730 --> 00:03:03.892
But let's see how information
flows in this case.

50
00:03:03.892 --> 00:03:07.635
First, you compute the hidden states for
the current layer.

51
00:03:07.635 --> 00:03:12.605
Then you get the activations and pass
those values to the next hidden layer and

52
00:03:12.605 --> 00:03:14.415
repeat this process.

53
00:03:14.415 --> 00:03:18.905
In other words, at first,
you propagate information through time.

54
00:03:18.905 --> 00:03:21.265
Then you go deeper in the network and

55
00:03:21.265 --> 00:03:25.810
repeat the process for each layer
until you get to your predictions.

56
00:03:25.810 --> 00:03:31.422
Now you've become familiar with the two
interesting and useful varieties of RNNs.

57
00:03:31.422 --> 00:03:36.266
Bidirectional RNNs propagates information
through time from the future and

58
00:03:36.266 --> 00:03:37.241
from the past.

59
00:03:37.241 --> 00:03:42.126
Deep RNNs can help you solve more complex
tasks that aren't possible with shallow

60
00:03:42.126 --> 00:03:43.680
neural networks.

61
00:03:43.680 --> 00:03:47.150
Both architectures are relatively
simple compositions

62
00:03:47.150 --> 00:03:50.740
derived from the vanilla RNN
model that you've already seen.

63
00:03:50.740 --> 00:03:54.240
So it isn't such a big conceptual
leap to understand their math.