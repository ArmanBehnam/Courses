WEBVTT

1
00:00:00.212 --> 00:00:03.090
Take a look at this sequential model.

2
00:00:03.090 --> 00:00:07.290
Here, you have an input
vector x in magenta,

3
00:00:07.290 --> 00:00:11.880
multiple hidden units in blue,
and three output units in green.

4
00:00:11.880 --> 00:00:15.450
Suppose that this model
outputs the probability

5
00:00:15.450 --> 00:00:19.790
of whether an observation belongs
to each of three different classes.

6
00:00:19.790 --> 00:00:25.070
Looking at a single x, y pair,
you can get the loss using the cross

7
00:00:25.070 --> 00:00:29.678
entropy function,
where K is the number of categories or

8
00:00:29.678 --> 00:00:34.877
classes and every label y subscript
j is equal to either 1 or 0.

9
00:00:34.877 --> 00:00:40.105
So to get the loss for a single
observation, you have to look at every

10
00:00:40.105 --> 00:00:46.284
output probability in y-hat and compare
them with the true y values, 0 or 1.

11
00:00:47.340 --> 00:00:51.670
Vanilla RNNs have an architecture
similar to this one, where for

12
00:00:51.670 --> 00:00:56.120
every time step t,
the network outputs a vector y-hat.

13
00:00:56.120 --> 00:00:59.310
By computing these two
formulas at every step,

14
00:00:59.310 --> 00:01:03.010
the cross entropy loss in this
case is shown in this equation.

15
00:01:03.010 --> 00:01:07.535
Where the only difference from
the equation that I showed earlier is

16
00:01:07.535 --> 00:01:14.260
the summation over time t and the division
by the total number of steps, capital T.

17
00:01:14.260 --> 00:01:19.405
This is just the average of the cross
entropy loss function over time.

18
00:01:19.405 --> 00:01:23.785
To get the cost of your whole model,
as always, you would have to sum over

19
00:01:23.785 --> 00:01:27.970
every example in the training,
development, or test sets.

20
00:01:27.970 --> 00:01:32.170
Now you know that to capture
an individual loss in your model, you

21
00:01:32.170 --> 00:01:37.490
only have to take the average with respect
to time for your recurrent neural network.

22
00:01:37.490 --> 00:01:40.600
Next, I'll share with you
some implementation notes and

23
00:01:40.600 --> 00:01:42.890
the more complex RNN architectures.