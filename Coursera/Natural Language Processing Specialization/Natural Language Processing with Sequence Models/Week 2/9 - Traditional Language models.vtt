WEBVTT

1
00:00:00.190 --> 00:00:01.370
Welcome back.

2
00:00:01.370 --> 00:00:04.620
You have previously learned
about N-gram language models.

3
00:00:04.620 --> 00:00:09.040
And specifically, he used it to compute
the probability of a sequence of words.

4
00:00:09.040 --> 00:00:13.340
Now in this video, you're going to
identify the limitations of that model,

5
00:00:13.340 --> 00:00:17.350
and you'll see that it requires
a lot of space and a lot of memory.

6
00:00:17.350 --> 00:00:19.323
So with that said, let's dive in.

7
00:00:19.323 --> 00:00:23.671
Suppose you have to translate
the sentence, j'ai vu le match de foot,

8
00:00:23.671 --> 00:00:25.201
from French to English.

9
00:00:25.201 --> 00:00:30.473
If you had several similar sentence
candidates like, I saw the game of soccer.

10
00:00:30.473 --> 00:00:32.517
I saw the soccer game.

11
00:00:32.517 --> 00:00:37.654
I saw the soccer match, and
saw I the game of soccer.

12
00:00:37.654 --> 00:00:42.467
For an accurate translation, you could
compute the probabilities of each sentence

13
00:00:42.467 --> 00:00:45.042
using a language model like the N-gram,
and

14
00:00:45.042 --> 00:00:49.660
end up selecting the sequence of
words with the highest probability.

15
00:00:49.660 --> 00:00:52.570
In this case, that would be,
I saw the soccer game.

16
00:00:53.840 --> 00:00:57.770
You may have encountered the N-gram
model before, but let's go ahead and

17
00:00:57.770 --> 00:00:59.210
review it quickly.

18
00:00:59.210 --> 00:01:02.830
Recall that in order to build
an N-gram language model,

19
00:01:02.830 --> 00:01:05.980
you have to compute
conditional probabilities.

20
00:01:05.980 --> 00:01:06.790
For bigrams,

21
00:01:06.790 --> 00:01:11.850
you have to compute conditional
probabilities using one previous word.

22
00:01:11.850 --> 00:01:15.960
For trigrams, you could compute
it using two previous words.

23
00:01:15.960 --> 00:01:17.862
So for an N-gram model,

24
00:01:17.862 --> 00:01:22.490
you use conditional probabilities
using n minus one words.

25
00:01:22.490 --> 00:01:27.622
At the end, you'll get the probability
of a whole sentence by multiplying

26
00:01:27.622 --> 00:01:32.347
the probabilities of each word in
the sentence using its previous and

27
00:01:32.347 --> 00:01:33.583
minus one words.

28
00:01:33.583 --> 00:01:37.972
So in the case of a bigram, to get
the probability of a three-word sentence,

29
00:01:37.972 --> 00:01:40.944
you would multiply
the probability of the first word

30
00:01:40.944 --> 00:01:43.590
by the probability of the second word.

31
00:01:43.590 --> 00:01:45.010
Given the first word, and

32
00:01:45.010 --> 00:01:48.679
then by the probability of the third word,
given the second one.

33
00:01:48.679 --> 00:01:52.300
These models have many limitations.

34
00:01:52.300 --> 00:01:56.290
One of them is that in order to
capture dependencies between words

35
00:01:56.290 --> 00:01:59.640
far away from each other,
your model would have to account for

36
00:01:59.640 --> 00:02:04.080
conditional probabilities in
very long sequences of words.

37
00:02:04.080 --> 00:02:09.452
This could be difficult to estimate
without correspondingly large corpora.

38
00:02:09.452 --> 00:02:14.432
Even in the case of large corpora your
model would need a lots of space and

39
00:02:14.432 --> 00:02:19.082
RAM to store the probabilities of
all the possible combinations.

40
00:02:19.082 --> 00:02:22.630
You can see how quickly this
approach becomes impractical.

41
00:02:22.630 --> 00:02:27.785
Up next, I'll introduce you to recurrent
neural networks and gated recurrent units.

42
00:02:27.785 --> 00:02:32.505
Two models that are much more
efficient than N-grams for

43
00:02:32.505 --> 00:02:35.663
NLP tasks like machine translation.

44
00:02:35.663 --> 00:02:40.121
You have now seen that traditional N-gram
language models require a lot of space and

45
00:02:40.121 --> 00:02:41.013
a lot of memory.

46
00:02:41.013 --> 00:02:44.648
So if a user is downloading an app,
for example, on their phone,

47
00:02:44.648 --> 00:02:47.171
then they might not have
enough space there.

48
00:02:47.171 --> 00:02:48.014
And as a result,

49
00:02:48.014 --> 00:02:52.070
the developers might not want to use
traditional engram language models here.

50
00:02:52.070 --> 00:02:54.630
And they might want to
use a different approach,

51
00:02:54.630 --> 00:02:57.071
which is known as
recurrent neural networks.

52
00:02:57.071 --> 00:03:01.890
In the next video, I'll introduce you
to RNN or recurrent neural networks.