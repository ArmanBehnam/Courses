WEBVTT

1
00:00:00.000 --> 00:00:04.170
Welcome back. LSTMs, as
I've mentioned before,

2
00:00:04.170 --> 00:00:05.790
are the best known solution

3
00:00:05.790 --> 00:00:07.740
to the vanishing
gradient problem.

4
00:00:07.740 --> 00:00:10.000
Let me tell you about it.

5
00:00:10.000 --> 00:00:15.225
Now I'll give you a high level
view into the LSTM model,

6
00:00:15.225 --> 00:00:17.340
what's its architecture
looks like,

7
00:00:17.340 --> 00:00:19.290
along with a few examples of

8
00:00:19.290 --> 00:00:21.600
what they're used for
in the real world.

9
00:00:21.600 --> 00:00:24.225
The LSTM is a special variety

10
00:00:24.225 --> 00:00:26.910
of RNN that was
designed to handle

11
00:00:26.910 --> 00:00:29.310
entire sequences of data

12
00:00:29.310 --> 00:00:32.265
by learning when to remember
and when to forget,

13
00:00:32.265 --> 00:00:34.635
which is similar to the GRU.

14
00:00:34.635 --> 00:00:38.450
An LSTM is essentially
composed of a cell state,

15
00:00:38.450 --> 00:00:40.610
which you can think
of as its memory,

16
00:00:40.610 --> 00:00:42.080
and the hidden state,

17
00:00:42.080 --> 00:00:44.090
where computations
are performed during

18
00:00:44.090 --> 00:00:47.015
training to decide on
what changes to make.

19
00:00:47.015 --> 00:00:49.955
The hidden state typically
has three gates to pass

20
00:00:49.955 --> 00:00:53.525
through before the entire
operation is performed again.

21
00:00:53.525 --> 00:00:55.885
Any loop would
update the weights.

22
00:00:55.885 --> 00:00:57.830
These cell state travels through

23
00:00:57.830 --> 00:01:00.950
these three gates that
track inputs as it arrives.

24
00:01:00.950 --> 00:01:03.290
Each one plays a part in deciding

25
00:01:03.290 --> 00:01:04.730
how much information to pass

26
00:01:04.730 --> 00:01:07.280
along and how much
to leave behind.

27
00:01:07.280 --> 00:01:09.005
This series of gates

28
00:01:09.005 --> 00:01:10.970
allows the gradients
to flow unchanged.

29
00:01:10.970 --> 00:01:12.800
The risk of vanishing or

30
00:01:12.800 --> 00:01:15.150
exporting gradients is mitigated.

31
00:01:15.150 --> 00:01:17.780
Let's take a moment to
relate this concept

32
00:01:17.780 --> 00:01:20.300
to something you might
find a bit more familiar.

33
00:01:20.300 --> 00:01:23.915
Imagine receiving a phone
call from your best friend.

34
00:01:23.915 --> 00:01:25.790
At the time your phone rings,

35
00:01:25.790 --> 00:01:27.050
you might be thinking of

36
00:01:27.050 --> 00:01:29.675
any number of things
unrelated to your friend.

37
00:01:29.675 --> 00:01:31.670
This is similar to the cell state

38
00:01:31.670 --> 00:01:33.140
at the beginning of the loop.

39
00:01:33.140 --> 00:01:34.640
When you answer the call,

40
00:01:34.640 --> 00:01:37.340
you put aside those
unrelated thoughts while

41
00:01:37.340 --> 00:01:38.960
retaining anything you meant to

42
00:01:38.960 --> 00:01:40.910
talk to your friend about.

43
00:01:40.910 --> 00:01:44.030
This is similar to what
the forget gate does.

44
00:01:44.030 --> 00:01:46.505
As your conversation progresses,

45
00:01:46.505 --> 00:01:47.960
you'll be taking in all the

46
00:01:47.960 --> 00:01:49.775
new information from your friend

47
00:01:49.775 --> 00:01:51.620
while also thinking of what else

48
00:01:51.620 --> 00:01:53.885
might be relevant
to talk about next.

49
00:01:53.885 --> 00:01:55.430
This is similar to what

50
00:01:55.430 --> 00:01:57.695
the two layers of
the input gates do.

51
00:01:57.695 --> 00:01:59.900
When you decide what to say next,

52
00:01:59.900 --> 00:02:02.240
that's similar to
the outputs gates.

53
00:02:02.240 --> 00:02:05.750
You will then continue to
evolve the conversation in

54
00:02:05.750 --> 00:02:09.215
this way until you hang up
at the end of the call.

55
00:02:09.215 --> 00:02:11.960
You'll have a memory
or cell states that's

56
00:02:11.960 --> 00:02:13.370
been updated a few times

57
00:02:13.370 --> 00:02:15.100
since you began
your conversation.

58
00:02:15.100 --> 00:02:17.660
Let's take a look at
the basic anatomy of

59
00:02:17.660 --> 00:02:20.090
a typical LSTM units,

60
00:02:20.090 --> 00:02:21.850
and what each part does.

61
00:02:21.850 --> 00:02:24.005
First is the cell or memory,

62
00:02:24.005 --> 00:02:26.615
which is typically
followed by three gates.

63
00:02:26.615 --> 00:02:29.570
The forget gate,
typically sigmoid layer,

64
00:02:29.570 --> 00:02:31.460
which looks at all
the information from

65
00:02:31.460 --> 00:02:34.670
the previous cell state and
decides what to throw away.

66
00:02:34.670 --> 00:02:36.110
Then you need to decide

67
00:02:36.110 --> 00:02:38.165
what new information
to store in the cell,

68
00:02:38.165 --> 00:02:40.235
which brings you
to the input gate.

69
00:02:40.235 --> 00:02:41.750
This has two parts,

70
00:02:41.750 --> 00:02:45.005
a sigmoid layer to choose
which values to update,

71
00:02:45.005 --> 00:02:46.970
and the tanh layer to

72
00:02:46.970 --> 00:02:49.865
select candidates for
new values to be added.

73
00:02:49.865 --> 00:02:53.380
Then you will calculate
your new cell state.

74
00:02:53.380 --> 00:02:55.730
Finally, the output gate

75
00:02:55.730 --> 00:02:57.830
takes the previous
hidden state along with

76
00:02:57.830 --> 00:02:59.900
the current inputs and processes

77
00:02:59.900 --> 00:03:03.110
them through another
sigmoid and tanh layer,

78
00:03:03.110 --> 00:03:05.240
the products of which decides how

79
00:03:05.240 --> 00:03:07.535
much to write to the
next hidden state.

80
00:03:07.535 --> 00:03:09.800
Before diving deeper
into the math,

81
00:03:09.800 --> 00:03:13.505
let's discuss some of the
many applications of LSTMs.

82
00:03:13.505 --> 00:03:15.050
As you might have guessed,

83
00:03:15.050 --> 00:03:17.630
it's very handy for
building language models,

84
00:03:17.630 --> 00:03:19.040
spanning use cases from

85
00:03:19.040 --> 00:03:21.260
predicting the next
character of your e-mail,

86
00:03:21.260 --> 00:03:22.850
to building chatbots capable

87
00:03:22.850 --> 00:03:25.415
of remembering longer
conversations.

88
00:03:25.415 --> 00:03:28.535
Music composition can
be done with LSTMs.

89
00:03:28.535 --> 00:03:30.980
This approach makes
a lot of sense

90
00:03:30.980 --> 00:03:32.840
considering that music is built

91
00:03:32.840 --> 00:03:34.730
using long sequences of notes,

92
00:03:34.730 --> 00:03:37.840
much like text uses long
sequences of words.

93
00:03:37.840 --> 00:03:40.280
Other cool applications
are automatic

94
00:03:40.280 --> 00:03:43.279
image captioning and
speech recognition.

95
00:03:43.279 --> 00:03:46.655
In overcoming the problems
of a traditional RNN,

96
00:03:46.655 --> 00:03:48.455
the LSTM has become

97
00:03:48.455 --> 00:03:52.430
incredibly popular tool
with a wide range of uses,

98
00:03:52.430 --> 00:03:55.640
and has helped evolve NLP
in some exciting ways

99
00:03:55.640 --> 00:03:57.500
that I'll discuss in more detail

100
00:03:57.500 --> 00:03:59.405
towards the end of this week.

101
00:03:59.405 --> 00:04:02.000
In summary, LSTMs are

102
00:04:02.000 --> 00:04:04.190
a solution to the vanishing
gradients problem.

103
00:04:04.190 --> 00:04:09.335
That typical LSTMs have a cell
or memory and three gates,

104
00:04:09.335 --> 00:04:10.790
which are the forget gates,

105
00:04:10.790 --> 00:04:13.150
inputs gates, and outputs gates.

106
00:04:13.150 --> 00:04:16.490
Now you understand LSTMs
on the high level,

107
00:04:16.490 --> 00:04:20.240
but what exactly is the math
behind the LSTM computation?

108
00:04:20.240 --> 00:04:23.250
I will show you in
the next video.