WEBVTT

1
00:00:00.520 --> 00:00:01.980
Welcome back.

2
00:00:01.980 --> 00:00:05.270
It is often the case that you'll
end up having vectors in very,

3
00:00:05.270 --> 00:00:07.250
very high dimensions.

4
00:00:07.250 --> 00:00:10.850
You want to find a way to reduce
the dimension of these vectors

5
00:00:10.850 --> 00:00:15.060
to two dimensions so
you can plot it on an XY axis.

6
00:00:15.060 --> 00:00:17.870
You will now learn about
principal components analysis,

7
00:00:17.870 --> 00:00:19.930
which will allow you to do so.

8
00:00:19.930 --> 00:00:23.083
You are going to be using
principal component analysis to

9
00:00:23.083 --> 00:00:27.092
visualize vector representations
with higher dimensions than the ones

10
00:00:27.092 --> 00:00:29.010
that you've seen plotted so far.

11
00:00:29.010 --> 00:00:32.639
To get started, I'll give you some
intuition on the motivation for

12
00:00:32.639 --> 00:00:35.067
visualizing vector presentation of words.

13
00:00:35.067 --> 00:00:38.962
And you'll see for yourself what
principal components analysis does and

14
00:00:38.962 --> 00:00:41.426
how it is used for
dimensionality reduction.

15
00:00:41.426 --> 00:00:44.119
Imagine you have the following
representation for

16
00:00:44.119 --> 00:00:45.689
your words in a vector space.

17
00:00:45.689 --> 00:00:49.979
In this scenario, your vector
space dimension is higher than tw.

18
00:00:49.979 --> 00:00:54.489
You know that the words oil and gas,
and city, and town are related.

19
00:00:54.489 --> 00:00:58.610
And you want to see if that relationship
is captured by the representation

20
00:00:58.610 --> 00:00:59.505
of your words.

21
00:00:59.505 --> 00:01:03.800
So how could you visualize your
words in order to see this and

22
00:01:03.800 --> 00:01:06.120
other possible relationships?

23
00:01:06.120 --> 00:01:09.950
Dimensionality reduction is
a perfect choice for this task.

24
00:01:09.950 --> 00:01:14.400
When you have a representation of your
words in a high dimensional space.

25
00:01:14.400 --> 00:01:16.830
You could use an algorithm like PCA

26
00:01:16.830 --> 00:01:21.380
to get a representation on a vector
space with fewer dimensions.

27
00:01:21.380 --> 00:01:23.420
If you want to visualize your data,

28
00:01:23.420 --> 00:01:27.430
you can get a reduced representation
with three or fewer features.

29
00:01:28.750 --> 00:01:32.410
If you perform principal components
analysis on your data and

30
00:01:32.410 --> 00:01:38.090
get a two-dimensional representation,
you can then plot a visual of your words.

31
00:01:38.090 --> 00:01:41.890
In this case, you might find that
your initial representation captured

32
00:01:41.890 --> 00:01:47.410
the relationship between the words oil and
gas, and city, and town.

33
00:01:47.410 --> 00:01:52.710
Because in your two-dimensional space they
appear to be clustered with related words.

34
00:01:52.710 --> 00:01:57.510
You can even find other relationships
among your words that you didn't expect,

35
00:01:57.510 --> 00:01:59.565
which is a fun and useful possibility.

36
00:02:00.785 --> 00:02:03.565
Now that you know what
PCA can help you achieve,

37
00:02:03.565 --> 00:02:06.055
let's go into detail on how it works.

38
00:02:06.055 --> 00:02:10.565
For the sake of simplicity, I'll begin
with a two dimensional vector space.

39
00:02:10.565 --> 00:02:14.825
Say that you want your data to be
represented by one feature instead.

40
00:02:14.825 --> 00:02:19.845
Using PCA, first you'll find
a set of uncorrelated features.

41
00:02:19.845 --> 00:02:23.560
And then projects your data
to a one dimensional space,

42
00:02:23.560 --> 00:02:26.890
trying to retain as much
information as possible.

43
00:02:26.890 --> 00:02:30.770
As you can see,
this process is quite straightforward.

44
00:02:30.770 --> 00:02:35.540
Coming up, you'll see for yourself
the details of how this algorithm works.

45
00:02:35.540 --> 00:02:38.340
Along with how to get
uncorrelated features,

46
00:02:38.340 --> 00:02:41.500
you'll also project your data for
your representation

47
00:02:41.500 --> 00:02:45.900
in a lower dimension while retaining
as much information as possible.

48
00:02:46.990 --> 00:02:49.039
Let's do a quick review before you go.

49
00:02:49.039 --> 00:02:50.793
PCA is an algorithm used for

50
00:02:50.793 --> 00:02:56.660
dimensionality reduction that can find
uncorrelated features for your data.

51
00:02:56.660 --> 00:02:59.670
It's very helpful for
visualizing your data

52
00:02:59.670 --> 00:03:03.940
to check if your representation is
capturing relationships among words.