WEBVTT

1
00:00:00.000 --> 00:00:01.740
Hello. In this video,

2
00:00:01.740 --> 00:00:04.350
you're going to learn to
manipulate vectors and

3
00:00:04.350 --> 00:00:07.995
specifically by performing
some simple vector arithmetic,

4
00:00:07.995 --> 00:00:10.890
meaning by adding vectors
and subtracting vectors,

5
00:00:10.890 --> 00:00:12.750
you will be able to predict

6
00:00:12.750 --> 00:00:15.060
the countries of
certain capitals.

7
00:00:15.060 --> 00:00:17.625
Let's dive in. In this portion,

8
00:00:17.625 --> 00:00:20.580
I'll show you how to manipulate
vector representations

9
00:00:20.580 --> 00:00:23.835
in order to infer unknown
relations among words.

10
00:00:23.835 --> 00:00:26.280
Suppose that you
have a vector space

11
00:00:26.280 --> 00:00:28.845
with countries and
their capital cities.

12
00:00:28.845 --> 00:00:31.650
You know that the capital
of the United States is

13
00:00:31.650 --> 00:00:35.445
Washington DC and you don't
know the capital of Russia,

14
00:00:35.445 --> 00:00:38.010
but you'd like to use the
known relationship between

15
00:00:38.010 --> 00:00:41.270
Washington DC and the
USA to figure it out.

16
00:00:41.270 --> 00:00:44.780
For that, you'll just use
some simple vector algebra.

17
00:00:44.780 --> 00:00:47.300
For this example, you are in

18
00:00:47.300 --> 00:00:50.840
a hypothetical two-dimensional
vector space that has

19
00:00:50.840 --> 00:00:52.310
different representations for

20
00:00:52.310 --> 00:00:55.280
different countries
and capitals cities.

21
00:00:55.280 --> 00:00:58.550
First, you will have to find
the relationship between

22
00:00:58.550 --> 00:01:02.425
the Washington DC and USA
vector representations.

23
00:01:02.425 --> 00:01:05.600
In other words, which
vector connects them?

24
00:01:05.600 --> 00:01:08.675
To do that, get the difference
between the two vectors.

25
00:01:08.675 --> 00:01:10.820
The values from
that will tell you

26
00:01:10.820 --> 00:01:13.175
how many units on each dimension

27
00:01:13.175 --> 00:01:15.005
you should move in order to find

28
00:01:15.005 --> 00:01:17.960
a country's capital
in that vector space.

29
00:01:17.960 --> 00:01:20.615
So to find the capital
city of Russia,

30
00:01:20.615 --> 00:01:23.270
you will have to sum
it's vector presentation

31
00:01:23.270 --> 00:01:26.590
with the vector that you
also got in the last step.

32
00:01:26.590 --> 00:01:29.540
At the end, you should
deduce that the capital of

33
00:01:29.540 --> 00:01:33.400
Russia has a vector
representation of 10, 4.

34
00:01:33.400 --> 00:01:37.105
However, there are no cities
with that representation,

35
00:01:37.105 --> 00:01:39.020
so you'll have to take the one

36
00:01:39.020 --> 00:01:40.940
that is the most
similar to its by

37
00:01:40.940 --> 00:01:43.190
comparing each vector with

38
00:01:43.190 --> 00:01:46.390
the Euclidean distances
or cosine similarities.

39
00:01:46.390 --> 00:01:47.900
In this case, the vector

40
00:01:47.900 --> 00:01:50.330
representation that
is closest to the 10,

41
00:01:50.330 --> 00:01:52.625
4 is the one for Moscow.

42
00:01:52.625 --> 00:01:54.455
Using this simple process,

43
00:01:54.455 --> 00:01:56.210
you could have predicted
the capital of

44
00:01:56.210 --> 00:01:59.090
Russia if you knew the
capital of the USA.

45
00:01:59.090 --> 00:02:02.620
The only catch here is that
you need a vector space where

46
00:02:02.620 --> 00:02:03.860
the representations

47
00:02:03.860 --> 00:02:06.695
capture the relative
meaning of words.

48
00:02:06.695 --> 00:02:08.780
Now you have a simple process

49
00:02:08.780 --> 00:02:10.310
to get unknown relationships

50
00:02:10.310 --> 00:02:12.050
between words by the use of

51
00:02:12.050 --> 00:02:13.970
known relationships
between others.

52
00:02:13.970 --> 00:02:15.680
You now know the importance of

53
00:02:15.680 --> 00:02:17.450
having vectors spaces where

54
00:02:17.450 --> 00:02:19.355
the representations of words

55
00:02:19.355 --> 00:02:22.490
capture the relative meaning
in natural language.

56
00:02:22.490 --> 00:02:24.950
In upcoming parts of
this specialization,

57
00:02:24.950 --> 00:02:27.290
you will perform more
sophisticated tasks

58
00:02:27.290 --> 00:02:30.470
taking advantage of this
type of representation.

59
00:02:30.470 --> 00:02:32.405
You have now seen a clustering of

60
00:02:32.405 --> 00:02:35.030
all vectors when
plotted on two axes.

61
00:02:35.030 --> 00:02:36.800
You have also seen
that the vectors

62
00:02:36.800 --> 00:02:38.000
of the words that occur in

63
00:02:38.000 --> 00:02:39.740
similar places in the sentence

64
00:02:39.740 --> 00:02:42.035
will be encoded in a similar way.

65
00:02:42.035 --> 00:02:44.210
You can take advantage
of this type of

66
00:02:44.210 --> 00:02:46.850
consistency encoding
to identify patterns.

67
00:02:46.850 --> 00:02:48.590
For example, if you had

68
00:02:48.590 --> 00:02:50.540
the word doctor and
you were to find

69
00:02:50.540 --> 00:02:52.400
the closest words
that are closest to

70
00:02:52.400 --> 00:02:54.665
it by computing
cosine similarity,

71
00:02:54.665 --> 00:02:56.540
you might get the word doctors,

72
00:02:56.540 --> 00:03:00.255
nurse, cardiologist,
surgeon, etc.

73
00:03:00.255 --> 00:03:01.820
In the next video,

74
00:03:01.820 --> 00:03:03.080
you will learn how to plot

75
00:03:03.080 --> 00:03:06.780
these d-dimensional
vectors on a 2D plane.