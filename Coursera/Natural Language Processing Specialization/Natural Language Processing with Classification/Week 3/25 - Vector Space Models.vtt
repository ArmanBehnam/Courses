WEBVTT

1
00:00:00.440 --> 00:00:03.180
Last week you learned about
a classification algorithm

2
00:00:03.180 --> 00:00:04.690
known as naive Bayes.

3
00:00:04.690 --> 00:00:07.660
This week you're going to
learn about vector spaces.

4
00:00:07.660 --> 00:00:12.330
And specifically, you will learn what type
of information these vectors could encode.

5
00:00:12.330 --> 00:00:15.330
You'll all see different types
of applications that you can use

6
00:00:15.330 --> 00:00:17.090
with these vector spaces, and

7
00:00:17.090 --> 00:00:20.360
you'll see different types of
algorithms you'll be implementing.

8
00:00:20.360 --> 00:00:22.420
Let's take a look at an example.

9
00:00:22.420 --> 00:00:23.320
In this video,

10
00:00:23.320 --> 00:00:28.160
I'm going to introduce you to the general
idea behind vector space models.

11
00:00:28.160 --> 00:00:30.460
You're going to see their advantages,

12
00:00:30.460 --> 00:00:34.420
along with some of their applications
in natural language processing.

13
00:00:34.420 --> 00:00:36.620
So suppose you have two questions.

14
00:00:36.620 --> 00:00:39.293
The first one is, where are you heading?

15
00:00:39.293 --> 00:00:41.310
And the second one is, where are you from?

16
00:00:42.400 --> 00:00:46.130
These sentences have identical words,
except for the last ones.

17
00:00:47.190 --> 00:00:50.630
However, they both have
a different meaning.

18
00:00:50.630 --> 00:00:54.220
On the other hand,
say you have two more questions

19
00:00:54.220 --> 00:00:59.580
whose words are completely different but
both sentences mean the same thing.

20
00:00:59.580 --> 00:01:04.480
Vector space models will help you identify
whether the first pair of questions or

21
00:01:04.480 --> 00:01:09.010
the second pair are similar in meaning
even if they do not share the same words.

22
00:01:09.010 --> 00:01:12.050
They can be used to
identify similarity for

23
00:01:12.050 --> 00:01:15.455
a question answering,
paraphrasing, and summarization.

24
00:01:17.050 --> 00:01:21.881
Vector space models will also allow you
to capture dependencies between words.

25
00:01:21.881 --> 00:01:23.444
Consider this sentence.

26
00:01:23.444 --> 00:01:25.730
You eat cereal from a bowl.

27
00:01:25.730 --> 00:01:30.670
Here, you can see that the word cereal and
the word bowl are related.

28
00:01:30.670 --> 00:01:32.210
Now let's look at this other sentence.

29
00:01:33.470 --> 00:01:36.040
You buy something and
someone else sells it.

30
00:01:36.040 --> 00:01:41.630
So what it's saying is that someone sells
something because someone else buys it.

31
00:01:41.630 --> 00:01:45.743
The second half of the sentence
is dependent on the first half.

32
00:01:45.743 --> 00:01:49.147
With vector space models,
you will be able to capture this and

33
00:01:49.147 --> 00:01:53.440
many other types of relationships
among different sets of words.

34
00:01:53.440 --> 00:01:58.235
Vector space models are used in
information extraction to answer

35
00:01:58.235 --> 00:02:02.329
questions in the style of who,
what, where, how, and

36
00:02:02.329 --> 00:02:06.969
etc., in machine translation and
in chatbots programming.

37
00:02:06.969 --> 00:02:10.157
They're also used in many,
many other applications.

38
00:02:10.157 --> 00:02:14.492
As a final thoughts, I'd like to share
with you this quote from John Firth,

39
00:02:14.492 --> 00:02:16.960
a famous English linguist.

40
00:02:16.960 --> 00:02:19.792
You shall know a word by
the company it keeps.

41
00:02:19.792 --> 00:02:23.128
This is one of the most
fundamental concepts in NLP.

42
00:02:23.128 --> 00:02:27.753
When using vector space models, the way
that representations are made is by

43
00:02:27.753 --> 00:02:31.294
identifying the context around
each word in the text, and

44
00:02:31.294 --> 00:02:33.645
this captures the relative meaning.

45
00:02:33.645 --> 00:02:40.280
To recap, vector space models allow you to
represent words and documents as vectors.

46
00:02:40.280 --> 00:02:41.790
This captures the relative meaning.

47
00:02:42.910 --> 00:02:46.240
You learned about vector space models,
and you've seen different types of

48
00:02:46.240 --> 00:02:49.580
applications where these
vector space models are used.

49
00:02:49.580 --> 00:02:52.180
In the next video,
you will build them from scratch.

50
00:02:52.180 --> 00:02:56.460
And specifically, you will see how they
are built using co-occurrence matrices.