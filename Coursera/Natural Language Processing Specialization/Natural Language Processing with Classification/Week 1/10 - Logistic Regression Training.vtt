WEBVTT

1
00:00:00.000 --> 00:00:01.665
In the previous video,

2
00:00:01.665 --> 00:00:03.840
you learned how to classify
whether a tweet has

3
00:00:03.840 --> 00:00:06.630
a positive sentiment
or negative sentiment,

4
00:00:06.630 --> 00:00:08.910
using a theta that
I have given you.

5
00:00:08.910 --> 00:00:10.500
In this video, you will

6
00:00:10.500 --> 00:00:12.240
learn your own
theta from scratch,

7
00:00:12.240 --> 00:00:14.430
and specifically,
I'll walk you through

8
00:00:14.430 --> 00:00:17.835
an algorithm that allows you
to get your theta variable.

9
00:00:17.835 --> 00:00:19.785
Let's see how you can do this.

10
00:00:19.785 --> 00:00:22.635
To train your logistic
regression classifier,

11
00:00:22.635 --> 00:00:26.250
iterate until you find the
set of parameters theta,

12
00:00:26.250 --> 00:00:28.260
that minimizes your
cost function.

13
00:00:28.260 --> 00:00:30.870
Let us suppose that your
loss only depends on

14
00:00:30.870 --> 00:00:33.509
the parameters theta1 and theta2,

15
00:00:33.509 --> 00:00:35.580
you would have a
cost function that

16
00:00:35.580 --> 00:00:38.025
looks like this contour
plots on the left.

17
00:00:38.025 --> 00:00:39.770
On the right, you can see

18
00:00:39.770 --> 00:00:42.815
the evolution of the cost
function as you iterate.

19
00:00:42.815 --> 00:00:44.540
First, you would have to

20
00:00:44.540 --> 00:00:46.750
initialize your parameters theta.

21
00:00:46.750 --> 00:00:49.580
Then you will update
your theta in

22
00:00:49.580 --> 00:00:52.610
the direction of the gradient
of your cost function.

23
00:00:52.610 --> 00:00:54.455
After a 100 iterations,

24
00:00:54.455 --> 00:00:56.120
you would be at this point,

25
00:00:56.120 --> 00:00:59.940
after 200 here, and so on.

26
00:00:59.940 --> 00:01:03.590
After many iterations, you
derive to a point near

27
00:01:03.590 --> 00:01:07.585
your optimum costs and you'd
end your training here.

28
00:01:07.585 --> 00:01:10.320
Let's look at this
process in more detail.

29
00:01:10.320 --> 00:01:12.080
First, you'd have to

30
00:01:12.080 --> 00:01:14.525
initialize your
parameters vector theta.

31
00:01:14.525 --> 00:01:17.285
Then you'd use the
logistic function

32
00:01:17.285 --> 00:01:20.225
to get values for each
of your observations.

33
00:01:20.225 --> 00:01:23.299
After that, you'd be able
to calculate the gradients

34
00:01:23.299 --> 00:01:26.545
of your cost function and
update your parameters.

35
00:01:26.545 --> 00:01:28.520
Finally, you'd be able to compute

36
00:01:28.520 --> 00:01:31.580
your cost J and determine if

37
00:01:31.580 --> 00:01:33.590
more iterations are
needed according to

38
00:01:33.590 --> 00:01:37.100
a stop-parameter or maximum
number of iterations.

39
00:01:37.100 --> 00:01:39.410
As you might have seen
in the other courses,

40
00:01:39.410 --> 00:01:43.150
this algorithm is known
as gradient descent.

41
00:01:43.150 --> 00:01:45.560
Now, that you have
your theta variable,

42
00:01:45.560 --> 00:01:47.390
you want to evaluate your theta,

43
00:01:47.390 --> 00:01:49.640
meaning you want to
evaluate your classifier.

44
00:01:49.640 --> 00:01:52.430
Once you put in your theta
into your sigmoid function,

45
00:01:52.430 --> 00:01:56.434
do get a good classifier or
do you get a bad classifier?

46
00:01:56.434 --> 00:02:00.270
In the next video, we will
show you how you can do this.