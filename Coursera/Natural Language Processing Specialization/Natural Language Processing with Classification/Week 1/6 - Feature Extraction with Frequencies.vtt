WEBVTT

1
00:00:00.000 --> 00:00:02.580
Welcome back. You
previously learned to

2
00:00:02.580 --> 00:00:04.860
encode a tweet as a vector of

3
00:00:04.860 --> 00:00:08.610
dimension V. You will now
learn to encode a tweet or

4
00:00:08.610 --> 00:00:12.915
specifically represented as
a vector of dimension 3.

5
00:00:12.915 --> 00:00:14.745
In doing so, you'll have

6
00:00:14.745 --> 00:00:16.350
a much faster speed for

7
00:00:16.350 --> 00:00:18.224
your logistic
regression classifier,

8
00:00:18.224 --> 00:00:21.150
because instead of
learning V features,

9
00:00:21.150 --> 00:00:23.700
you only have to
learn three features.

10
00:00:23.700 --> 00:00:26.415
Let's take a look at
how you can do this.

11
00:00:26.415 --> 00:00:29.120
You just saw that the
frequency of a word in

12
00:00:29.120 --> 00:00:31.310
a class is simply
the number of times

13
00:00:31.310 --> 00:00:33.410
that the word appears
on the set of tweets

14
00:00:33.410 --> 00:00:35.600
belonging to that class and that

15
00:00:35.600 --> 00:00:37.085
this table is basically

16
00:00:37.085 --> 00:00:40.090
a dictionary mapping
from word class pairs,

17
00:00:40.090 --> 00:00:42.770
to frequencies, or
it just tells us how

18
00:00:42.770 --> 00:00:44.120
many times each word showed

19
00:00:44.120 --> 00:00:46.205
up in its corresponding class.

20
00:00:46.205 --> 00:00:49.010
Now that you've built your
frequencies dictionary,

21
00:00:49.010 --> 00:00:50.480
you can use it to extract

22
00:00:50.480 --> 00:00:53.605
useful features for
sentiment analysis.

23
00:00:53.605 --> 00:00:55.845
What does a feature look like?

24
00:00:55.845 --> 00:00:58.485
Let's look at the
arbitrary tweet m.

25
00:00:58.485 --> 00:01:02.705
The first feature would be
a bias unit equal to 1.

26
00:01:02.705 --> 00:01:05.150
The second is the sum of

27
00:01:05.150 --> 00:01:08.435
the positive frequencies for
every unique word on tweet

28
00:01:08.435 --> 00:01:11.480
m. The third is the sum of

29
00:01:11.480 --> 00:01:15.265
negative frequencies for every
unique word on the tweet.

30
00:01:15.265 --> 00:01:18.575
So to extract the features
for this representation,

31
00:01:18.575 --> 00:01:21.650
you'd only have to sum
frequencies of words.

32
00:01:21.650 --> 00:01:26.165
Easy. For instance, take
the following tweets.

33
00:01:26.165 --> 00:01:28.430
Now let's look at
the frequencies for

34
00:01:28.430 --> 00:01:31.250
the positive class
from the last lecture.

35
00:01:31.250 --> 00:01:34.580
The only words from the
vocabulary that don't appear on

36
00:01:34.580 --> 00:01:37.905
these tweets are
happy and because.

37
00:01:37.905 --> 00:01:40.490
Now let's take a look at
the second feature from

38
00:01:40.490 --> 00:01:43.160
the representation that
you saw on the last slide.

39
00:01:43.160 --> 00:01:44.825
To get this value,

40
00:01:44.825 --> 00:01:47.150
you need to sum the
frequencies of the words

41
00:01:47.150 --> 00:01:50.080
from the vocabulary that
appear on the tweet.

42
00:01:50.080 --> 00:01:55.530
At the end, you get a
value equal to eight.

43
00:01:56.300 --> 00:01:59.570
Now let's get the value
of the third feature.

44
00:01:59.570 --> 00:02:02.450
It is the sum of
negative frequencies of

45
00:02:02.450 --> 00:02:05.440
the words from the vocabulary
that appear on the tweet.

46
00:02:05.440 --> 00:02:08.195
For this example,
you should get 11

47
00:02:08.195 --> 00:02:11.090
after summing up the
underlined frequencies.

48
00:02:11.090 --> 00:02:12.410
So far, this tweets,

49
00:02:12.410 --> 00:02:14.240
this representation
would be equal to

50
00:02:14.240 --> 00:02:17.935
the vector 1, 8, 11.

51
00:02:17.935 --> 00:02:19.880
You now know how to represent

52
00:02:19.880 --> 00:02:22.745
a tweet as a vector
of dimension 3.

53
00:02:22.745 --> 00:02:24.530
In the next video
you will learn to

54
00:02:24.530 --> 00:02:27.080
pre-process your tweets
and as a result,

55
00:02:27.080 --> 00:02:29.105
you will use those
pre-processed words

56
00:02:29.105 --> 00:02:32.340
as words of your vocabulary.