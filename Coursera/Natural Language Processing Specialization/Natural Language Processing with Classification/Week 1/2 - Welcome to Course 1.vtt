WEBVTT

1
00:00:00.000 --> 00:00:03.480
Welcome to this first
course of NLP in

2
00:00:03.480 --> 00:00:05.505
which you'll learn
about classification

3
00:00:05.505 --> 00:00:07.020
and vector spaces.

4
00:00:07.020 --> 00:00:10.890
You'll also learn about the
application of these ideas to

5
00:00:10.890 --> 00:00:15.300
problems like sentiment
analysis and word translation.

6
00:00:15.300 --> 00:00:19.545
For example, let's say you
have 1,000 product reviews,

7
00:00:19.545 --> 00:00:21.875
so pieces of texts
written by users.

8
00:00:21.875 --> 00:00:24.180
Can you build a system
to automatically go

9
00:00:24.180 --> 00:00:26.370
through all of these
product reviews to

10
00:00:26.370 --> 00:00:28.410
figure out what
fraction of them are

11
00:00:28.410 --> 00:00:31.380
positive reviews versus
negative reviews?

12
00:00:31.380 --> 00:00:33.210
This course will teach you how.

13
00:00:33.210 --> 00:00:36.950
Lucas, can you say a few words
about this first course?

14
00:00:36.950 --> 00:00:39.380
Sure. In the first week,

15
00:00:39.380 --> 00:00:41.240
you'll learn how to
represent text as

16
00:00:41.240 --> 00:00:43.445
a vector and build a classifier

17
00:00:43.445 --> 00:00:45.440
that will classify
whether a sample text

18
00:00:45.440 --> 00:00:48.245
is a positive sentiment
or a negative one.

19
00:00:48.245 --> 00:00:51.270
You will use logistic
regression for that.

20
00:00:51.270 --> 00:00:52.730
In the second week,

21
00:00:52.730 --> 00:00:54.200
you'll use the Naive Bayes

22
00:00:54.200 --> 00:00:55.915
classifier on the same problem.

23
00:00:55.915 --> 00:00:57.560
In the third week, you'll learn

24
00:00:57.560 --> 00:00:59.390
about vector space models.

25
00:00:59.390 --> 00:01:03.889
You'll learn how to represent
text documents like tweets,

26
00:01:03.889 --> 00:01:06.110
articles, queries,

27
00:01:06.110 --> 00:01:09.905
or any object that contains
of text as a vector.

28
00:01:09.905 --> 00:01:12.765
This is important in
information retrieval,

29
00:01:12.765 --> 00:01:15.365
in indexing, in
relevancy ranking,

30
00:01:15.365 --> 00:01:17.605
and also in
information filtering.

31
00:01:17.605 --> 00:01:20.405
For example, when you
look up a query online,

32
00:01:20.405 --> 00:01:22.160
the algorithm relies on all of

33
00:01:22.160 --> 00:01:24.365
these concepts to give
you back your results.

34
00:01:24.365 --> 00:01:25.880
Finally, in week 4,

35
00:01:25.880 --> 00:01:27.920
you will build your
first simple machine

36
00:01:27.920 --> 00:01:30.500
translation system and
you'll make use of

37
00:01:30.500 --> 00:01:32.720
locality sensitive hashing to

38
00:01:32.720 --> 00:01:35.780
improve the performance of
nearest neighbor search.

39
00:01:35.780 --> 00:01:37.730
Thanks, Eunice and Lucas.

40
00:01:37.730 --> 00:01:42.210
With that, let's get started
and go on to the next video.