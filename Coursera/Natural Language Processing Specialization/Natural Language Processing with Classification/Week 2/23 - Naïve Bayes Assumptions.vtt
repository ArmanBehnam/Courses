WEBVTT

1
00:00:00.330 --> 00:00:04.340
This week, I'll go into the assumptions
underlying the Naive Bayes method.

2
00:00:04.340 --> 00:00:07.400
The main one is independence
of words in a sentence and

3
00:00:07.400 --> 00:00:10.290
I'll tell you why this can be a big
problem when the method is applied.

4
00:00:11.340 --> 00:00:15.260
>> Naive Bayes is a very simple model
because it doesn't require setting

5
00:00:15.260 --> 00:00:17.220
any custom parameters.

6
00:00:17.220 --> 00:00:21.115
This method is referred to as naive
because of the assumptions it makes

7
00:00:21.115 --> 00:00:22.045
about the data.

8
00:00:22.045 --> 00:00:26.618
The first assumption is independence
between the predictors or features

9
00:00:26.618 --> 00:00:31.498
associated with each class and the second
has to do with your validation sets.

10
00:00:31.498 --> 00:00:36.490
Let's explore each of these assumptions
and how they could affect your results.

11
00:00:36.490 --> 00:00:39.660
To illustrate towards independence
between features looks like,

12
00:00:39.660 --> 00:00:42.010
let's look at the following sentence.

13
00:00:42.010 --> 00:00:44.400
It is sunny and hot in the Sahara Desert.

14
00:00:45.490 --> 00:00:50.120
Naive Bayes assumes that the words
in a piece of text are independent

15
00:00:50.120 --> 00:00:54.670
of one another, but as you can see,
this typically isn't the case.

16
00:00:54.670 --> 00:01:00.270
The word sunny and hot often appear
together as they do in this example.

17
00:01:00.270 --> 00:01:03.986
Taken together, they might also
be related to the thing they're

18
00:01:03.986 --> 00:01:06.086
describing like a beach or a desert.

19
00:01:06.086 --> 00:01:11.471
So the words in a sentence are not always
necessarily independent of one another,

20
00:01:11.471 --> 00:01:14.056
but Naive Bayes assumes that they are.

21
00:01:14.056 --> 00:01:15.792
This could lead you to under or

22
00:01:15.792 --> 00:01:19.899
over estimates the conditional
probabilities of individual words.

23
00:01:19.899 --> 00:01:25.419
When using Naive Bayes, for example,
if your task was to complete the sentence,

24
00:01:25.419 --> 00:01:30.059
it's always cold and snowy in blank,
Naive Bayes might assign equal

25
00:01:30.059 --> 00:01:35.019
probability to the words spring,
summer, fall, and winter even though

26
00:01:35.019 --> 00:01:40.255
from the context you can see that winter
should be the most likely candidate.

27
00:01:40.255 --> 00:01:44.050
In the next courses of
this specialization,

28
00:01:44.050 --> 00:01:48.710
you will be introduced to some more
sophisticated methods that deal with this.

29
00:01:48.710 --> 00:01:53.543
Another issue with Naive Bayes is that it
relies on the distribution of the training

30
00:01:53.543 --> 00:01:54.243
data sets.

31
00:01:54.243 --> 00:01:58.340
A good data set will contain
the same proportion of positive and

32
00:01:58.340 --> 00:02:01.133
negative tweets as a random sample would.

33
00:02:01.133 --> 00:02:05.558
However, most of the available
annotated corpora are artificially

34
00:02:05.558 --> 00:02:09.684
balanced just like the data set
you'll use for the assignment.

35
00:02:09.684 --> 00:02:11.186
In the real tweet stream,

36
00:02:11.186 --> 00:02:15.706
positive tweet is sent to occur more
often than their negative counterparts.

37
00:02:15.706 --> 00:02:16.675
One reason for

38
00:02:16.675 --> 00:02:22.330
this is that negative tweets might contain
content that is banned by the platform or

39
00:02:22.330 --> 00:02:27.530
muted by the user such as inappropriate or
offensive vocabulary.

40
00:02:27.530 --> 00:02:31.430
Assuming that reality behaves
as your training corpus,

41
00:02:31.430 --> 00:02:35.300
this could result in a very optimistic or
very pessimistic model.

42
00:02:35.300 --> 00:02:38.930
There's a lot more on this in
the last video of this module,

43
00:02:38.930 --> 00:02:42.470
which analyzes the sources
of errors in Naive Bayes.

44
00:02:42.470 --> 00:02:45.850
Let's do a quick recap of
all this new information.

45
00:02:45.850 --> 00:02:50.470
The assumption of independence in Naive
Bayes is very difficult to guarantee, but

46
00:02:50.470 --> 00:02:54.580
despite that, the model works
pretty well in certain situations.

47
00:02:54.580 --> 00:02:58.350
And for the assignments in this module,
the relative frequency of positive and

48
00:02:58.350 --> 00:03:02.150
negative tweets in your training
data sets needs to be balanced in

49
00:03:02.150 --> 00:03:04.160
order to deliver an accurate results.

50
00:03:05.160 --> 00:03:08.870
Now you understand the assumptions
that underlie the Naive Bayes method.

51
00:03:08.870 --> 00:03:11.480
What if it fails to perform well for
some sentence?

52
00:03:11.480 --> 00:03:14.240
In the next video,
I'll show you what to do in such cases.