WEBVTT

1
00:00:00.000 --> 00:00:02.610
Welcome back.
Earlier in the week,

2
00:00:02.610 --> 00:00:05.295
you used a Naive Bayes
method to classify tweets.

3
00:00:05.295 --> 00:00:07.140
But that can be used
to do a number of

4
00:00:07.140 --> 00:00:10.500
other things like identify
who's an author of a text.

5
00:00:10.500 --> 00:00:14.220
I will give you a few ideas
of what those things may be.

6
00:00:14.220 --> 00:00:16.350
When you use Naive Bayes to

7
00:00:16.350 --> 00:00:18.300
predict the sentiments
of a tweet,

8
00:00:18.300 --> 00:00:19.770
what you're actually doing is

9
00:00:19.770 --> 00:00:21.450
estimating the probability for

10
00:00:21.450 --> 00:00:22.920
each class by using

11
00:00:22.920 --> 00:00:25.845
the joint probability of
the words in classes.

12
00:00:25.845 --> 00:00:27.630
The Naive Bayes formula is

13
00:00:27.630 --> 00:00:30.090
just the ratio between
these two probabilities,

14
00:00:30.090 --> 00:00:33.465
the products of the priors
and the likelihoods.

15
00:00:33.465 --> 00:00:35.160
You can use this ratio between

16
00:00:35.160 --> 00:00:37.290
conditional
probabilities for much

17
00:00:37.290 --> 00:00:39.195
more than sentiment analysis.

18
00:00:39.195 --> 00:00:42.390
For one, you could do
author identification.

19
00:00:42.390 --> 00:00:44.635
If you had two large corporal,

20
00:00:44.635 --> 00:00:46.775
each written by
different authors,

21
00:00:46.775 --> 00:00:49.100
you could train the model
to recognize whether

22
00:00:49.100 --> 00:00:53.150
a new document was written
by one or the other.

23
00:00:53.150 --> 00:00:55.130
Or if you had some works by

24
00:00:55.130 --> 00:00:57.860
Shakespeare and some
works by Hemingway,

25
00:00:57.860 --> 00:00:59.525
you could calculate
the Lambda for

26
00:00:59.525 --> 00:01:01.730
each word to predict how likely

27
00:01:01.730 --> 00:01:03.680
a new word is to be used by

28
00:01:03.680 --> 00:01:07.010
Shakespeare or
alternatively by Hemingway.

29
00:01:07.010 --> 00:01:11.765
This method also allows you
to determine author identity.

30
00:01:11.765 --> 00:01:14.465
Another common use
is spam filtering.

31
00:01:14.465 --> 00:01:17.030
Using information
taken from the sender,

32
00:01:17.030 --> 00:01:19.250
subject and content, you could

33
00:01:19.250 --> 00:01:21.845
decide whether an
email is spam or not.

34
00:01:21.845 --> 00:01:23.720
One of the earliest uses of

35
00:01:23.720 --> 00:01:25.490
Naive Bayes was filtering between

36
00:01:25.490 --> 00:01:29.045
relevant and irrelevant
documents in a database.

37
00:01:29.045 --> 00:01:32.970
Given the sets of keywords
in a query, in this case,

38
00:01:32.970 --> 00:01:34.610
you only needed to calculate

39
00:01:34.610 --> 00:01:37.370
the likelihood of the
documents given the query.

40
00:01:37.370 --> 00:01:39.155
You can't know beforehand what's

41
00:01:39.155 --> 00:01:41.930
irrelevant or a relevant
document looks like.

42
00:01:41.930 --> 00:01:45.440
So you can compute the
likelihood for each document in

43
00:01:45.440 --> 00:01:47.180
your dataset and then store

44
00:01:47.180 --> 00:01:49.670
the documents based
on its likelihoods.

45
00:01:49.670 --> 00:01:52.610
You can choose to keep
the first M results or

46
00:01:52.610 --> 00:01:54.320
the ones that have a likelihood

47
00:01:54.320 --> 00:01:56.150
larger than a certain threshold.

48
00:01:56.150 --> 00:01:58.250
Finally, you can also use

49
00:01:58.250 --> 00:02:00.545
Naive Bayes for word
disambiguation,

50
00:02:00.545 --> 00:02:02.780
which is to say, breaking

51
00:02:02.780 --> 00:02:05.070
down words for
contextual clarity.

52
00:02:05.070 --> 00:02:08.570
Consider that you have only
two possible interpretations

53
00:02:08.570 --> 00:02:11.000
of a given word within a text.

54
00:02:11.000 --> 00:02:12.380
Let's say you don't know if

55
00:02:12.380 --> 00:02:14.750
the word bank in your reading is

56
00:02:14.750 --> 00:02:15.920
referring to the bank of

57
00:02:15.920 --> 00:02:18.710
a river or to a
financial institution.

58
00:02:18.710 --> 00:02:20.810
To disambiguate your word,

59
00:02:20.810 --> 00:02:22.580
calculate the score
of the document,

60
00:02:22.580 --> 00:02:24.095
given that it refers to

61
00:02:24.095 --> 00:02:26.165
each one of the
possible meanings.

62
00:02:26.165 --> 00:02:29.420
In this case, if the text
refers to the concept

63
00:02:29.420 --> 00:02:32.870
of river instead of
the concept of money,

64
00:02:32.870 --> 00:02:36.400
then the score will be bigger
than one. That's cool.

65
00:02:36.400 --> 00:02:40.715
In summary, Bayes Rule and
it's naive approximation

66
00:02:40.715 --> 00:02:45.005
has a wide range of applications
in sentiment analysis,

67
00:02:45.005 --> 00:02:46.955
author identification,

68
00:02:46.955 --> 00:02:50.540
information retrieval
and word disambiguation.

69
00:02:50.540 --> 00:02:53.075
It's a popular method since it's

70
00:02:53.075 --> 00:02:56.455
relatively simple to
train, use and interpret.

71
00:02:56.455 --> 00:02:58.200
You'll be using
the Bayes rule and

72
00:02:58.200 --> 00:03:00.380
Naive Bayes again
in the weeks ahead.

73
00:03:00.380 --> 00:03:03.040
Now you are fully equipped.

74
00:03:03.040 --> 00:03:04.965
As you've seen in this video,

75
00:03:04.965 --> 00:03:08.120
Naive Bayes can be used for
many classification tasks.

76
00:03:08.120 --> 00:03:09.590
Next, I will show you

77
00:03:09.590 --> 00:03:12.780
the assumptions that underlie
the Naive Bayes method.