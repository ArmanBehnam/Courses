WEBVTT

1
00:00:00.850 --> 00:00:04.670
To start, we are going to first review
what's probabilities and conditional

2
00:00:04.670 --> 00:00:10.570
probabilities are, how they operate, and
how they can be expressed mathematically.

3
00:00:10.570 --> 00:00:14.850
Then I'll go over how to derive Bayes'
rule from the definition of conditional

4
00:00:14.850 --> 00:00:16.460
probabilities.

5
00:00:16.460 --> 00:00:20.580
Bayes' rule is applied in many different
fields ranging from medicine to education

6
00:00:20.580 --> 00:00:23.530
and is used extensively in NLP.

7
00:00:23.530 --> 00:00:26.810
Once you understand the theory behind
Bayes' rule, you can use it to

8
00:00:26.810 --> 00:00:31.250
perform sentiment analysis on tweets,
which will be this week's assignment.

9
00:00:31.250 --> 00:00:32.660
Then in the next course,

10
00:00:32.660 --> 00:00:36.660
you will Implement autocorrect using
its basic formulation as well.

11
00:00:36.660 --> 00:00:40.340
Imagine you have an extensive corpus
of tweets that can be categorized

12
00:00:40.340 --> 00:00:44.600
as either positive or
negative sentiment, but not both.

13
00:00:44.600 --> 00:00:49.450
Within that corpus, the word happy is
sometimes being labeled positive and

14
00:00:49.450 --> 00:00:50.890
sometimes negative.

15
00:00:50.890 --> 00:00:54.043
Let's explore why this
situation is occurring,

16
00:00:54.043 --> 00:00:59.232
one way to think about probabilities is
by counting how frequently events occur.

17
00:00:59.232 --> 00:01:04.175
Suppose you defined events A as
a tweet being labeled positive,

18
00:01:04.175 --> 00:01:08.201
then the probability of events
A shown as P of A here is

19
00:01:08.201 --> 00:01:13.144
calculated as the ratio between
the count of positive tweets and

20
00:01:13.144 --> 00:01:18.700
the corpus divided by the total
number of tweets in the corpus.

21
00:01:18.700 --> 00:01:25.537
In this example, that number
comes out to 13 over 20 or 0.65.

22
00:01:25.537 --> 00:01:31.203
You could also express that value
as a percentage, 65% positive.

23
00:01:31.203 --> 00:01:35.366
It's worth noting that
the complementary probability here,

24
00:01:35.366 --> 00:01:40.078
which is the probability of the tweets
expressing a negative sentiment

25
00:01:40.078 --> 00:01:45.425
is just equal to one minus
the probability of a positive sentiment.

26
00:01:45.425 --> 00:01:47.640
Notes that for this to be true,

27
00:01:47.640 --> 00:01:52.920
all tweets must be categorized as either
positive or negative but not both.

28
00:01:52.920 --> 00:01:58.560
Let's define events B in a similar way by
counting tweets containing the word happy.

29
00:01:58.560 --> 00:02:02.860
In this case, the total number of
tweets containing the word happy

30
00:02:02.860 --> 00:02:06.300
shown here as N happy is 4.

31
00:02:06.300 --> 00:02:11.160
Here's another way of looking at it,
take a look at the section of the diagram

32
00:02:11.160 --> 00:02:15.430
where tweets are labeled positive and
also contain the word happy.

33
00:02:15.430 --> 00:02:19.650
In the context of this diagram,
the probability that a tweet is labeled

34
00:02:19.650 --> 00:02:24.600
positive and also contains the word
happy is just the ratio of the area

35
00:02:24.600 --> 00:02:29.640
of the intersection divided by
the area of the entire corpus.

36
00:02:29.640 --> 00:02:34.130
In other words,
if there were 20 tweets in the corpus and

37
00:02:34.130 --> 00:02:38.540
three of them are labeled positive and
also contain the word happy,

38
00:02:38.540 --> 00:02:43.576
then the associated
probability is 3 / 20 or

39
00:02:43.576 --> 00:02:46.269
or 0.15.