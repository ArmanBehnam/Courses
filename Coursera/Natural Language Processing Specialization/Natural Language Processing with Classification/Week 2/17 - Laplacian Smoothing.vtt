WEBVTT

1
00:00:00.000 --> 00:00:03.330
Let's now dive into
Laplacian smoothing,

2
00:00:03.330 --> 00:00:04.800
a technique you can use to

3
00:00:04.800 --> 00:00:07.275
avoid your probabilities
being zero.

4
00:00:07.275 --> 00:00:09.360
The expression used to calculate

5
00:00:09.360 --> 00:00:12.825
the conditional probability
of a word, given the class,

6
00:00:12.825 --> 00:00:14.760
is the frequency of the word in

7
00:00:14.760 --> 00:00:18.630
the corpus shown here
as freq of word i,

8
00:00:18.630 --> 00:00:20.850
class divided by the number of

9
00:00:20.850 --> 00:00:23.730
words in the corpus or N class.

10
00:00:23.730 --> 00:00:26.550
Smoothing the probability
function means that you will

11
00:00:26.550 --> 00:00:29.445
use a slightly different
formula from the original.

12
00:00:29.445 --> 00:00:32.730
Note that I've added a
one in the numerator.

13
00:00:32.730 --> 00:00:34.860
This little transformation avoids

14
00:00:34.860 --> 00:00:36.635
the probability being zero.

15
00:00:36.635 --> 00:00:39.410
However, it adds a new term to

16
00:00:39.410 --> 00:00:41.270
all the frequencies that is

17
00:00:41.270 --> 00:00:43.730
not correctly
normalized by N class.

18
00:00:43.730 --> 00:00:45.260
To account for this,

19
00:00:45.260 --> 00:00:49.400
you will add a new term in
the denominator V class.

20
00:00:49.400 --> 00:00:53.120
That is the number of unique
words in your vocabulary to

21
00:00:53.120 --> 00:00:56.840
account for that extra term
added in the numerator.

22
00:00:56.840 --> 00:00:58.700
So now all the probabilities

23
00:00:58.700 --> 00:01:00.650
in each column will sum to one.

24
00:01:00.650 --> 00:01:03.955
This process is called
Laplacian smoothing.

25
00:01:03.955 --> 00:01:06.215
Going back to this example,

26
00:01:06.215 --> 00:01:08.290
let's use the formula on it.

27
00:01:08.290 --> 00:01:10.580
The first thing you
need to calculate is

28
00:01:10.580 --> 00:01:13.205
the number of unique
words in your vocabulary.

29
00:01:13.205 --> 00:01:16.100
In this example, you
have eight unique words.

30
00:01:16.100 --> 00:01:17.990
So now let's calculate

31
00:01:17.990 --> 00:01:21.460
the probability for each
word in the positive class.

32
00:01:21.460 --> 00:01:24.260
For the word I, the
positive class,

33
00:01:24.260 --> 00:01:31.795
you get 3 plus 1 divided by
13 plus 8 which is 0.19.

34
00:01:31.795 --> 00:01:33.660
For the negative class,

35
00:01:33.660 --> 00:01:39.930
you have 3 plus 1 divided
by 12 plus 8 which is 0.2,

36
00:01:39.930 --> 00:01:42.600
and then so on for the
rest of the table.

37
00:01:42.600 --> 00:01:45.435
The numbers shown here
have been rounded,

38
00:01:45.435 --> 00:01:47.150
but using this method the sum of

39
00:01:47.150 --> 00:01:50.365
probabilities in your
table will still be one.

40
00:01:50.365 --> 00:01:52.610
Note that the word because no

41
00:01:52.610 --> 00:01:54.905
longer has a probability of zero.

42
00:01:54.905 --> 00:01:57.685
Great, that's
Laplacian smoothing.

43
00:01:57.685 --> 00:02:00.980
Now you know why you have to
use Laplacian smoothing so

44
00:02:00.980 --> 00:02:02.990
your probabilities don't end up

45
00:02:02.990 --> 00:02:06.030
being zero. That's very cool.