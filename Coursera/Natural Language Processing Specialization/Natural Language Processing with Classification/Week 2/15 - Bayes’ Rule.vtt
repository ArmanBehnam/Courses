WEBVTT

1
00:00:00.550 --> 00:00:02.480
In order to derive Bayes' rule,

2
00:00:02.480 --> 00:00:06.280
let's first take a look at
the conditional probabilities.

3
00:00:06.280 --> 00:00:09.899
Now think about what happens if
instead of the entire Corpus,

4
00:00:09.899 --> 00:00:13.108
you only consider tweets
that contain the word happy.

5
00:00:13.108 --> 00:00:18.359
This is the same as saying given that
a tweet contains the word happy with that,

6
00:00:18.359 --> 00:00:23.295
you would be considering only the tweets
inside the blue circle where many

7
00:00:23.295 --> 00:00:26.098
of the positive tweets are now excluded.

8
00:00:26.098 --> 00:00:31.019
In this case, the probability that
a tweet is positive given that

9
00:00:31.019 --> 00:00:35.671
it contains the word happy simply
becomes the number of tweets

10
00:00:35.671 --> 00:00:40.290
that are positive and
also contain the word happy.

11
00:00:40.290 --> 00:00:43.040
And we divide that by the number
that contain the word happy.

12
00:00:43.040 --> 00:00:47.325
As you can see by this calculation,
your tweet has a 75%

13
00:00:47.325 --> 00:00:52.060
likelihood of being positive if
it's contains the word happy.

14
00:00:52.060 --> 00:00:55.348
You could make the same case for
positive tweets.

15
00:00:55.348 --> 00:00:59.986
The purple area denotes
the probability that a positive tweets

16
00:00:59.986 --> 00:01:02.800
contains the word happy.

17
00:01:02.800 --> 00:01:08.553
In this case, the probability
is 3 over 13, which is 0.231.

18
00:01:08.553 --> 00:01:12.861
With all of this discussion of the
probability of missing certain conditions,

19
00:01:12.861 --> 00:01:15.638
we are talking about
conditional probabilities.

20
00:01:15.638 --> 00:01:20.667
Conditional probabilities could
be interpreted as the probability

21
00:01:20.667 --> 00:01:24.664
of an outcome B knowing that
event A already happened.

22
00:01:24.664 --> 00:01:28.288
Or given that I'm looking
at an element from set A,

23
00:01:28.288 --> 00:01:31.590
the probability that it
also belongs to set B.

24
00:01:31.590 --> 00:01:36.392
There's another way of looking at this
with the Venn diagram you saw before.

25
00:01:36.392 --> 00:01:41.367
So using the previous example,
the probability of a tweets being

26
00:01:41.367 --> 00:01:46.523
positive given that it has the word
happy is equal to the probability

27
00:01:46.523 --> 00:01:51.136
of the intersection between
the tweets that are positive and

28
00:01:51.136 --> 00:01:56.021
the tweets that have the word happy
divided by the probability of

29
00:01:56.021 --> 00:02:00.120
a tweets given from the Corpus
having the word happy.

30
00:02:00.120 --> 00:02:04.023
Let's take a closer look at
the equation from the previous slide.

31
00:02:04.023 --> 00:02:08.759
You could write a similar equation by
simply swapping the position of the two

32
00:02:08.759 --> 00:02:09.653
conditions.

33
00:02:09.653 --> 00:02:12.666
Now, you have the conditional probability

34
00:02:12.666 --> 00:02:18.150
of a tweets containing the word happy
given that it is a positive tweets.

35
00:02:18.150 --> 00:02:22.470
Armed with both of these equations,
you're now ready to derive Bayes' rule.

36
00:02:22.470 --> 00:02:27.530
To combine these equations, note that the
intersection represents the same quantity

37
00:02:27.530 --> 00:02:32.810
no matter which way it's written, knowing
that you can remove it from the equation.

38
00:02:32.810 --> 00:02:37.138
With a little algebraic manipulation,
you are able to arrive at this equation.

39
00:02:37.138 --> 00:02:41.882
This is now an expression of Bayes' rule
in the context of the previous sentiment

40
00:02:41.882 --> 00:02:43.082
analysis problem.

41
00:02:43.082 --> 00:02:48.895
More generally, Bayes' rule
states that the probability of X

42
00:02:48.895 --> 00:02:54.703
given Y is equal to the probability
of Y given X times the ratio of

43
00:02:54.703 --> 00:03:01.160
the probability of X over
the probability of Y and that's it.

44
00:03:01.160 --> 00:03:06.000
You just arrived at the basic
formulation of Bayes' rule, nicely done.

45
00:03:06.000 --> 00:03:07.260
So to wrap up,

46
00:03:07.260 --> 00:03:11.600
you just derived Bayes' rule from
expressions of conditional probability.

47
00:03:11.600 --> 00:03:14.620
Throughout the rest of this course,
you'll be using Bayes' rule for

48
00:03:14.620 --> 00:03:16.990
various applications in NLP.

49
00:03:16.990 --> 00:03:19.630
The main takeaway for now is that

50
00:03:19.630 --> 00:03:24.730
Bayes' rule is based on the mathematical
formulation of conditional probabilities.

51
00:03:24.730 --> 00:03:29.920
And that with Bayes' rule, you can
calculate the probability of X given Y

52
00:03:29.920 --> 00:03:33.220
if you already know
the probability of Y given X.

53
00:03:33.220 --> 00:03:36.810
And the ratio of
the probabilities of X and Y.

54
00:03:36.810 --> 00:03:38.140
So that's great work.

55
00:03:38.140 --> 00:03:38.830
I'll see you later.