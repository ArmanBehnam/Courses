WEBVTT

1
00:00:00.260 --> 00:00:02.960
Congratulations on finishing
the first course, and

2
00:00:02.960 --> 00:00:06.320
welcome to this second course on NLP.

3
00:00:06.320 --> 00:00:09.830
In this course you learn about
probabilistic models and

4
00:00:09.830 --> 00:00:13.180
how to use them to predict word sequences.

5
00:00:13.180 --> 00:00:17.890
This is a technology that powers
auto-correction as well as web

6
00:00:17.890 --> 00:00:22.850
search suggestions where you may go to
websites and go to the search bar and

7
00:00:22.850 --> 00:00:23.820
type in a few words.

8
00:00:23.820 --> 00:00:26.810
And it will automatically suggest
the next word will suggest

9
00:00:26.810 --> 00:00:29.295
what you may be searching for.

10
00:00:29.295 --> 00:00:35.275
In this course you also learn about Markov
models and diva turbie algorithm, both

11
00:00:35.275 --> 00:00:41.595
of which are very important fundamental
building blocks of many NLP systems.

12
00:00:41.595 --> 00:00:44.845
Eunice do you want to say
a few words about this course?

13
00:00:44.845 --> 00:00:47.945
>> And the first week you will learn
to build an auto-correct system

14
00:00:47.945 --> 00:00:51.680
by using probabilities of
sequences of characters.

15
00:00:51.680 --> 00:00:54.880
In the second week, you'll learn
about hidden Markov models, and

16
00:00:54.880 --> 00:00:58.900
you'll use them to implement
a parts of speech tagging system.

17
00:00:58.900 --> 00:01:03.090
For example, if you looked up
the query book of flights,

18
00:01:03.090 --> 00:01:05.590
then knowing that the word book is a verb,

19
00:01:05.590 --> 00:01:09.230
not a noun, will help the model
better understand your query.

20
00:01:09.230 --> 00:01:10.153
>> By the third week,

21
00:01:10.153 --> 00:01:14.259
you will build an autocomplete systems
using probabilities of sequences of words.

22
00:01:15.360 --> 00:01:19.060
In week four,
we'll go back to word vectors.

23
00:01:19.060 --> 00:01:21.790
You've already learned to
use them in course one, and

24
00:01:21.790 --> 00:01:25.280
we will show you how to generate
them using neural networks.

25
00:01:25.280 --> 00:01:27.040
>> Thanks, Eunice and Lucas.

26
00:01:27.040 --> 00:01:30.250
I'm excited about all of you
learning in this course these

27
00:01:30.250 --> 00:01:32.990
important building blocks for NLP.

28
00:01:32.990 --> 00:01:33.540
Let's jump in