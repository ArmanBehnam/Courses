WEBVTT

1
00:00:00.390 --> 00:00:01.730
Welcome back.

2
00:00:01.730 --> 00:00:05.740
In this video, I'm going to show you
how you can use the probability matrix,

3
00:00:05.740 --> 00:00:08.370
the one that you learned
in the previous video, and

4
00:00:08.370 --> 00:00:11.340
I'll show you how you can
use it to create a path, so

5
00:00:11.340 --> 00:00:15.240
that you can assign a parts
of speech tag to every word.

6
00:00:15.240 --> 00:00:16.889
Let me show you how you can do this.

7
00:00:18.070 --> 00:00:22.620
The backward pass is the last of
three steps of the Viterbi algorithm,

8
00:00:22.620 --> 00:00:26.550
where you'll retrieve the most likely
sequence of parts of speech tags for

9
00:00:26.550 --> 00:00:28.580
your given sequence of words.

10
00:00:28.580 --> 00:00:32.205
By now you've populated the matrices C and
D.

11
00:00:32.205 --> 00:00:36.055
Now you just have to extract the path
through your graph from the matrix D,

12
00:00:36.055 --> 00:00:40.275
which represents the sequence of hidden
states that most likely generated

13
00:00:40.275 --> 00:00:44.385
our sequence,
word one all the way to word K.

14
00:00:44.385 --> 00:00:48.495
First, calculate the index
of the entry Ci,K

15
00:00:48.495 --> 00:00:51.605
with the highest probability
in the last column of C.

16
00:00:51.605 --> 00:00:56.370
The probability at this index is
the probability of the most likely

17
00:00:56.370 --> 00:01:01.470
sequence of hidden states,
generating the given sequence of words.

18
00:01:01.470 --> 00:01:05.880
You use this index s to traverse
backwards through the matrix D,

19
00:01:05.880 --> 00:01:09.360
to reconstruct the sequence
of parts of speech tags.

20
00:01:09.360 --> 00:01:12.960
First, calculate the index
of the entry Ci,K

21
00:01:12.960 --> 00:01:16.030
with the highest probability
in the last column of C.

22
00:01:16.030 --> 00:01:19.585
The probability at this index
is the probability of the most

23
00:01:19.585 --> 00:01:24.700
likely sequence of hidden states,
generating the given sequence of words.

24
00:01:24.700 --> 00:01:28.665
You use this index s to traverse
backwards through the matrix D,

25
00:01:28.665 --> 00:01:32.430
to reconstruct the sequence
of parts of speech tags.

26
00:01:32.430 --> 00:01:37.130
Let's look at a simple example for a
matrix D, for a model with four states and

27
00:01:37.130 --> 00:01:39.560
a given word sequence of length five.

28
00:01:39.560 --> 00:01:40.770
The matrix D,

29
00:01:40.770 --> 00:01:45.000
stores all the labels of the hidden states
you've traversed in the forward path.

30
00:01:45.000 --> 00:01:47.020
If you're going back through the states,

31
00:01:47.020 --> 00:01:51.050
starting with the path that has the
highest probability, you effectively got

32
00:01:51.050 --> 00:01:55.620
the most likely sequence of hidden states,
or parts of speech sites.

33
00:01:55.620 --> 00:01:59.190
You start by looking up the entry
with the highest probability in

34
00:01:59.190 --> 00:02:04.580
the last row of the matrix C, and
extract the index s of that entry.

35
00:02:04.580 --> 00:02:08.460
Let's look at the matrix C with some
probabilities you might have computed in

36
00:02:08.460 --> 00:02:10.320
the forward pass.

37
00:02:10.320 --> 00:02:14.722
You now want to calculate the index
s of the entry in the last row of C,

38
00:02:14.722 --> 00:02:17.910
which has the highest probability.

39
00:02:17.910 --> 00:02:22.725
The entry with the highest probability,
is the first one with 0.01.

40
00:02:22.725 --> 00:02:27.215
Written as a formula,
s is the argmax Ci,K,

41
00:02:27.215 --> 00:02:30.545
which in this case is equal to 1.

42
00:02:30.545 --> 00:02:34.230
That index represents the last
hidden state you traversed

43
00:02:34.230 --> 00:02:35.650
when you observe the word w5.

44
00:02:35.650 --> 00:02:41.900
So the most likely states of word
w5 is the t1 parts of speech tag.

45
00:02:41.900 --> 00:02:44.595
So you add t1 to the end
of the sequence and

46
00:02:44.595 --> 00:02:48.775
look up the next index in D,
which tells you where you came from.

47
00:02:48.775 --> 00:02:50.985
This next index is 3.

48
00:02:50.985 --> 00:02:53.675
Now move on to the fourth
word in the sequence,

49
00:02:53.675 --> 00:02:57.535
which means you're now looking at
the fourth column in the matrix D.

50
00:02:57.535 --> 00:03:00.065
To decide which row of
the matrix to look at,

51
00:03:00.065 --> 00:03:04.870
recall that in matrix D, column five,
the top row holds the value 3, or

52
00:03:04.870 --> 00:03:09.220
3 represents the previous state
with the highest probability.

53
00:03:09.220 --> 00:03:12.440
So t3 is the most likely state for
word number 4.

54
00:03:12.440 --> 00:03:15.580
So you would associate word number 4,

55
00:03:15.580 --> 00:03:19.950
with the state number 3, which is
the parts of speech label number 3.

56
00:03:19.950 --> 00:03:23.420
You can keep walking left to
each column of the matrix D.

57
00:03:23.420 --> 00:03:27.070
Since the value stored in column 4,
row 3, is the number 1,

58
00:03:27.070 --> 00:03:32.790
you can assign the parts of speech sag
t1 to the previous word, word number 3.

59
00:03:32.790 --> 00:03:36.933
Also, when you go left to column 3,
you will look for row 1,

60
00:03:36.933 --> 00:03:41.107
representing state 1,
which you see highlighted in green.

61
00:03:41.107 --> 00:03:45.282
The value stored in column 3,
row 1 is 3, this means that

62
00:03:45.282 --> 00:03:50.308
the previous word has parts of speech
tag number 3 associated with it.

63
00:03:50.308 --> 00:03:55.420
So associated word number 2
with parts of speech tag 3.

64
00:03:55.420 --> 00:03:58.599
Now walk left one column
in the matrix to column 2.

65
00:03:58.599 --> 00:04:04.863
Since the value stored in the green cell
in column 3 is 3, go to row 3 of column 2.

66
00:04:04.863 --> 00:04:07.685
In column 2 the value
stored in row 3 is 2.

67
00:04:07.685 --> 00:04:09.539
This means that for word one,

68
00:04:09.539 --> 00:04:12.886
the most likely state is
part of speech tag number 2.

69
00:04:12.886 --> 00:04:16.724
And the algorithm stops as you
have arrived at the start token,

70
00:04:16.724 --> 00:04:20.960
with the values stored in the second
row of the first column being 0.

71
00:04:20.960 --> 00:04:24.460
The sequence of ti that you have
recovered in the backward pass,

72
00:04:24.460 --> 00:04:28.390
is the sequence of parts of speech
tags with the highest probability.

73
00:04:28.390 --> 00:04:29.580
Before you go,

74
00:04:29.580 --> 00:04:33.560
there are two implementation
specific issues to be aware of.

75
00:04:33.560 --> 00:04:37.994
When you implement the Viterbi algorithm
in the programming assignment,

76
00:04:37.994 --> 00:04:39.752
be careful with the indices,

77
00:04:39.752 --> 00:04:44.450
as lists of matrix indices in
Python start with 0 instead of 1.

78
00:04:44.450 --> 00:04:49.280
Another implementation specific issue, is
when you multiply many very small numbers

79
00:04:49.280 --> 00:04:53.560
like probabilities, this will lead
to numerical issues, so you should

80
00:04:53.560 --> 00:04:58.580
use log probabilities instead, where
numbers are summed instead of multiplied.

81
00:04:58.580 --> 00:05:02.600
Don't worry, I'll be revisiting
this concept later in the course.

82
00:05:02.600 --> 00:05:04.900
Congratulations on finishing this week.

83
00:05:04.900 --> 00:05:08.900
You now know about the Viterbi algorithm,
and specifically you've used this for

84
00:05:08.900 --> 00:05:10.540
parts of speech tagging.

85
00:05:10.540 --> 00:05:13.420
You can use part of
speech tagging in search,

86
00:05:13.420 --> 00:05:18.130
in machine translation, in speech
recognition, and also in parsing.

87
00:05:18.130 --> 00:05:21.370
Next week you're going to learn
about a different type of task,

88
00:05:21.370 --> 00:05:23.952
which is known as N-gram language models.