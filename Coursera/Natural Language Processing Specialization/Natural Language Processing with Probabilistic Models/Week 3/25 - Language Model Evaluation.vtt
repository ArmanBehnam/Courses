WEBVTT

1
00:00:00.062 --> 00:00:01.953
Welcome back.

2
00:00:01.953 --> 00:00:05.603
In this video I'll show you how
to evaluate a language model.

3
00:00:05.603 --> 00:00:09.933
The metric for this is called perplexity
and I will explain what this is.

4
00:00:09.933 --> 00:00:14.561
>> First, you'll divide the text corpus
into train validation and test data,

5
00:00:14.561 --> 00:00:18.762
then you will dive into the concepts
of perplexity an important metric

6
00:00:18.762 --> 00:00:20.918
used to evaluate language models.

7
00:00:20.918 --> 00:00:25.695
So, how can you tell how well your
language model is performing?

8
00:00:25.695 --> 00:00:30.364
Recall from the previous videos that
a language model assigns a probability to

9
00:00:30.364 --> 00:00:31.363
each sentence.

10
00:00:31.363 --> 00:00:33.870
The model was trained on the corpus.

11
00:00:33.870 --> 00:00:37.961
So for the training sentences,
it may assign very high probabilities.

12
00:00:37.961 --> 00:00:42.033
You should therefore first split
the corpus to have some testing and

13
00:00:42.033 --> 00:00:45.183
validation data that are not used for
the training.

14
00:00:45.183 --> 00:00:48.761
As you may have done in the other
machine learning projects,

15
00:00:48.761 --> 00:00:53.262
you'll create the following splits of
training validation and test sets.

16
00:00:53.262 --> 00:00:56.132
The training set is used
to train your model.

17
00:00:56.132 --> 00:01:00.780
The validation set is used for
things like tuning hyper-parameters,

18
00:01:00.780 --> 00:01:03.342
and the test set is held out for the end.

19
00:01:03.342 --> 00:01:05.070
Where you test it once and

20
00:01:05.070 --> 00:01:10.683
get an accuracy score that reflects how
well your model performs on unseen data.

21
00:01:10.683 --> 00:01:15.912
A commonly used splits for
smaller data sets is 80, 10, 10 or

22
00:01:15.912 --> 00:01:20.890
80% to training 10% to validation and
10% to testing.

23
00:01:20.890 --> 00:01:25.563
In a very large data sets such as
text analysis testing could make

24
00:01:25.563 --> 00:01:28.690
up as little as 1% of your training sets.

25
00:01:29.890 --> 00:01:33.022
In NLP, there are two main methods for
splitting.

26
00:01:33.022 --> 00:01:38.100
You can split the corpus by choosing
longer continuous segments,

27
00:01:38.100 --> 00:01:42.453
like Wikipedia articles or
you can randomly choose short

28
00:01:42.453 --> 00:01:46.364
sequences of words such as
those in the sentences.

29
00:01:46.364 --> 00:01:50.430
Now that you've split the data sets
you can evaluate the test sets using

30
00:01:50.430 --> 00:01:52.710
the perplexity metric.

31
00:01:52.710 --> 00:01:56.942
Perplexity is a commonly used
metric in language modeling.

32
00:01:56.942 --> 00:01:58.701
But what does it mean?

33
00:01:58.701 --> 00:02:02.654
If you're familiar with the word
perplexed, you know that a person is

34
00:02:02.654 --> 00:02:06.530
perplexed when they are confused
by something very complex.

35
00:02:06.530 --> 00:02:11.493
You can think of perplexity as
a measure of the complexity in a sample

36
00:02:11.493 --> 00:02:14.376
of texts like how complex that text is.

37
00:02:14.376 --> 00:02:19.624
Perplexity is used to tell us whether
a set of sentences look like they

38
00:02:19.624 --> 00:02:25.787
were written by humans rather than by
a simple program choosing words at random.

39
00:02:25.787 --> 00:02:32.541
A text that is written by humans is more
likely to have a lower perplexity score.

40
00:02:32.541 --> 00:02:36.129
On the other hand a text
generated by random word choice

41
00:02:36.129 --> 00:02:39.060
would have a higher perplexity.

42
00:02:39.060 --> 00:02:42.292
Let me show you how to calculate
the perplexity of the model.

43
00:02:42.292 --> 00:02:47.360
You'll start by computing the probability
of all sentences in your test sets.

44
00:02:47.360 --> 00:02:52.021
And then raise the probability
to the power of -1/m.

45
00:02:52.021 --> 00:02:56.793
Perplexity is basically the inverse
probability of the test sets normalized by

46
00:02:56.793 --> 00:02:59.870
the number of words in the test set.

47
00:02:59.870 --> 00:03:04.804
So the higher the language model estimates
the probability of your test set the lower

48
00:03:04.804 --> 00:03:06.622
the perplexity is going to be.

49
00:03:06.622 --> 00:03:09.086
As a side note worth mentioning,

50
00:03:09.086 --> 00:03:14.462
perplexity is closely related to
entropy which measures uncertainty.

51
00:03:14.462 --> 00:03:18.774
Let's look at an example of two language
models that are going to return different

52
00:03:18.774 --> 00:03:20.720
probabilities for your test sets W.

53
00:03:20.720 --> 00:03:23.997
There are 100 words in the test sets.

54
00:03:23.997 --> 00:03:26.679
So m is equal to 100.

55
00:03:26.679 --> 00:03:33.921
The first model returns a 0.9 probability
of the test set, which is very high.

56
00:03:33.921 --> 00:03:37.990
This means that the first model
predicts your test sets very well.

57
00:03:37.990 --> 00:03:40.501
So the model is highly effective.

58
00:03:40.501 --> 00:03:45.313
As you can see the perplexity for
that model and test set is about 1,

59
00:03:45.313 --> 00:03:46.672
which is very low.

60
00:03:46.672 --> 00:03:50.718
The second model returns
a very low probability for

61
00:03:50.718 --> 00:03:54.302
your test sets 10 to the power of- 250.

62
00:03:54.302 --> 00:03:59.091
For this model and test set the perplexity
is equal to about 316 which is

63
00:03:59.091 --> 00:04:01.340
much higher than the first model.

64
00:04:02.350 --> 00:04:07.012
So one thing to remember is that
the smaller the perplexity score

65
00:04:07.012 --> 00:04:11.602
the more likely the sentence is
to sound natural to human ears.

66
00:04:11.602 --> 00:04:15.719
For context, good language
models have perplexity scores

67
00:04:15.719 --> 00:04:19.980
between 60 to 20 sometimes even lower for
English.

68
00:04:19.980 --> 00:04:24.966
Perplexities for character level language
models where you track characters

69
00:04:24.966 --> 00:04:27.013
instead of words will be lower.

70
00:04:27.013 --> 00:04:30.685
Now, we get ready to calculate
perplexity for bigram models.

71
00:04:30.685 --> 00:04:34.628
In a bigram model you calculate
the products of bigram

72
00:04:34.628 --> 00:04:39.541
probabilities of all sentences,
then take the power of- 1/m.

73
00:04:39.541 --> 00:04:44.210
Recall that the power of-
1/m of the probability is

74
00:04:44.210 --> 00:04:48.792
the same as the mth order
route of 1/ probability.

75
00:04:48.792 --> 00:04:53.817
One thing to notice here is that in the
case of the same probability for different

76
00:04:53.817 --> 00:04:59.350
test sets, the bigger the sets m is the
lower the final perplexity is going to be.

77
00:04:59.350 --> 00:05:02.873
If all sentences in the test
sets are concatenated,

78
00:05:02.873 --> 00:05:07.278
the formula can be simplified to
the products of probabilities of

79
00:05:07.278 --> 00:05:09.211
bigrams in the entire sets.

80
00:05:09.211 --> 00:05:12.101
One other thing to note
is that some papers

81
00:05:12.101 --> 00:05:15.163
use log perplexity instead of perplexity.

82
00:05:15.163 --> 00:05:21.568
So the perplexity formulae changes from
the mth order root of 1 over probability

83
00:05:21.568 --> 00:05:27.421
to 1 over m times the sum of the
logarithms of the probabilities of words.

84
00:05:27.421 --> 00:05:28.752
This is easier to compute.

85
00:05:28.752 --> 00:05:33.216
So it's not uncommon to find researchers
reporting the log perplexity of

86
00:05:33.216 --> 00:05:34.395
language models.

87
00:05:34.395 --> 00:05:38.273
Note that the logarithm to
the base 2 is typically used.

88
00:05:38.273 --> 00:05:42.597
In a good model with
perplexity between 20 and 60,

89
00:05:42.597 --> 00:05:46.750
log perplexity would be between 4.3 and
5.9.

90
00:05:46.750 --> 00:05:51.203
Now how does the improved perplexity
translates in a production

91
00:05:51.203 --> 00:05:53.021
quality language model?

92
00:05:53.021 --> 00:05:56.142
Here is an example of
a Wall Street Journal Corpus.

93
00:05:56.142 --> 00:06:03.486
If you take a unigram language model,
the perplexity is very high 962.

94
00:06:03.486 --> 00:06:06.904
This just generates words
by their probability.

95
00:06:06.904 --> 00:06:11.873
With a bigram language model, the text
starts to make a little more sense.

96
00:06:11.873 --> 00:06:17.280
Using a trigram, you can see the language
it produces is pretty close to reasonable.

97
00:06:17.280 --> 00:06:20.753
The perplexity is now equal
to 109 much closer to

98
00:06:20.753 --> 00:06:24.969
the target perplexity of 22:16,
I mentioned earlier.

99
00:06:24.969 --> 00:06:26.833
Later in the specialization,

100
00:06:26.833 --> 00:06:32.077
you'll encounter deep learning language
models with even lower perplexity scores.

101
00:06:32.077 --> 00:06:36.915
>> You now understand what perplexity
is and how to evaluate language models.

102
00:06:36.915 --> 00:06:41.118
To use them for real tasks we'll need to
be able to handle words that did not occur

103
00:06:41.118 --> 00:06:42.258
in the training set.

104
00:06:42.258 --> 00:06:44.295
I'll show you how to do
that in the next video.