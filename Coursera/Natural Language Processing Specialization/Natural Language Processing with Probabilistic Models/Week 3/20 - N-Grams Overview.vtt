WEBVTT

1
00:00:00.000 --> 00:00:01.590
This week you'll create

2
00:00:01.590 --> 00:00:04.650
an n-gram language model
from a text corpus.

3
00:00:04.650 --> 00:00:07.215
Recall that a text
corpus is typically

4
00:00:07.215 --> 00:00:09.810
a large database
of text documents,

5
00:00:09.810 --> 00:00:12.735
such as all the pages
on the Wikipedia,

6
00:00:12.735 --> 00:00:14.475
all books from one author,

7
00:00:14.475 --> 00:00:16.710
or all tweets from one account.

8
00:00:16.710 --> 00:00:18.810
A language model is a tool that's

9
00:00:18.810 --> 00:00:21.495
calculates the
probabilities of sentences.

10
00:00:21.495 --> 00:00:24.720
You can think of a sentence
as a sequence of words.

11
00:00:24.720 --> 00:00:27.450
Language models can also estimate

12
00:00:27.450 --> 00:00:29.430
the probability of
an upcoming word

13
00:00:29.430 --> 00:00:31.815
given a history of
previous words.

14
00:00:31.815 --> 00:00:36.555
For example, if you begin
an e-mail, Hello, how are,

15
00:00:36.555 --> 00:00:38.850
your e-mail application may guess

16
00:00:38.850 --> 00:00:41.850
that the next word you
want to write is the word,

17
00:00:41.850 --> 00:00:44.415
you, as in, Hello, how are you?

18
00:00:44.415 --> 00:00:46.350
In the assignment, you'll build

19
00:00:46.350 --> 00:00:48.740
your own n-gram
language model and

20
00:00:48.740 --> 00:00:51.730
apply it to autocomplete
a given sentence.

21
00:00:51.730 --> 00:00:53.405
First, it will process

22
00:00:53.405 --> 00:00:55.790
a text corpus into
a language model.

23
00:00:55.790 --> 00:00:58.220
Users of your
autocomplete system will

24
00:00:58.220 --> 00:01:00.790
provide the starting
words of a sentence.

25
00:01:00.790 --> 00:01:02.510
The model that you will build

26
00:01:02.510 --> 00:01:04.160
will then allow you to query

27
00:01:04.160 --> 00:01:05.645
for the most likely words

28
00:01:05.645 --> 00:01:07.850
following the starts
of that sentence.

29
00:01:07.850 --> 00:01:09.560
Then it outputs that as

30
00:01:09.560 --> 00:01:12.620
a suggestion to
complete the sentence.

31
00:01:12.620 --> 00:01:14.900
While you are going to focus on

32
00:01:14.900 --> 00:01:17.030
the autocomplete
application this week,

33
00:01:17.030 --> 00:01:20.585
language models are widely
used in other areas as well.

34
00:01:20.585 --> 00:01:22.220
Language models are used in

35
00:01:22.220 --> 00:01:23.630
speech recognition to convert

36
00:01:23.630 --> 00:01:25.550
the outputs into real words.

37
00:01:25.550 --> 00:01:28.790
For example, if an acoustic
model converts speech

38
00:01:28.790 --> 00:01:32.085
to text and adheres, I saw a van.

39
00:01:32.085 --> 00:01:34.775
Then the speech to
text system can use

40
00:01:34.775 --> 00:01:36.950
a language model
to know that it's

41
00:01:36.950 --> 00:01:39.215
more likely the sentence was,

42
00:01:39.215 --> 00:01:43.075
I saw a van and not
eyes awe of an.

43
00:01:43.075 --> 00:01:47.210
Language models are also used
in spelling correction to

44
00:01:47.210 --> 00:01:49.190
identify words that should be

45
00:01:49.190 --> 00:01:51.685
replaced based on the
sentence there in.

46
00:01:51.685 --> 00:01:53.900
Probabilities are also important

47
00:01:53.900 --> 00:01:56.540
for augmentative
communication systems.

48
00:01:56.540 --> 00:01:59.630
These are systems that
can take a series of

49
00:01:59.630 --> 00:02:01.460
hand gestures from a user

50
00:02:01.460 --> 00:02:03.965
to help them form
words and sentences.

51
00:02:03.965 --> 00:02:05.705
People like Stephen Hawking,

52
00:02:05.705 --> 00:02:08.495
who are unable to
physically talk or sign,

53
00:02:08.495 --> 00:02:10.610
can instead use simple movements

54
00:02:10.610 --> 00:02:12.725
to select words from a menu.

55
00:02:12.725 --> 00:02:15.500
Then the system can
speak for them.

56
00:02:15.500 --> 00:02:18.350
Word prediction using
language models can

57
00:02:18.350 --> 00:02:21.560
be used to suggest likely
words for the menu.

58
00:02:21.560 --> 00:02:23.870
Here's what you'll
be doing this week.

59
00:02:23.870 --> 00:02:27.110
First, you'll transform
your raw text corpus

60
00:02:27.110 --> 00:02:28.730
into a language model

61
00:02:28.730 --> 00:02:30.410
which returns the probability of

62
00:02:30.410 --> 00:02:34.010
the next word by using the
previous words of a sentence.

63
00:02:34.010 --> 00:02:36.710
Next, you'll adapt
your language model to

64
00:02:36.710 --> 00:02:39.830
deal with words the model
hasn't seen during training.

65
00:02:39.830 --> 00:02:43.555
These words are called
out of vocabulary words.

66
00:02:43.555 --> 00:02:46.220
Smoothing is another
technique that you can

67
00:02:46.220 --> 00:02:49.310
use to deal with
previously unseen input.

68
00:02:49.310 --> 00:02:51.590
Probability of unseen words and

69
00:02:51.590 --> 00:02:55.435
sentences can still be
successfully estimated this way.

70
00:02:55.435 --> 00:02:58.190
Smoothing essentially
pretends that each word and

71
00:02:58.190 --> 00:03:01.460
phrase appears in the training
corpus at least once.

72
00:03:01.460 --> 00:03:02.810
This helps to calculate

73
00:03:02.810 --> 00:03:06.685
the probability even for
unusual words and sequences.

74
00:03:06.685 --> 00:03:09.050
Finally, I'll show
you how to choose

75
00:03:09.050 --> 00:03:12.305
the best language model
with the perplexity metric,

76
00:03:12.305 --> 00:03:14.710
a new tool for your toolkits.

77
00:03:14.710 --> 00:03:16.340
When you combine these skills,

78
00:03:16.340 --> 00:03:18.559
you'll be able to
successfully implement

79
00:03:18.559 --> 00:03:20.705
a sentence autocompletion model

80
00:03:20.705 --> 00:03:23.129
in this week's assignments.