WEBVTT

1
00:00:00.000 --> 00:00:04.335
First, you will process the
corpus into accounts matrix.

2
00:00:04.335 --> 00:00:05.790
This captures the number of

3
00:00:05.790 --> 00:00:08.190
occurrences of relative n-grams.

4
00:00:08.190 --> 00:00:11.670
Next, you will transform
the count matrix into

5
00:00:11.670 --> 00:00:14.190
a probability matrix
that contains

6
00:00:14.190 --> 00:00:15.390
information about

7
00:00:15.390 --> 00:00:18.150
the conditional probability
of the n-grams.

8
00:00:18.150 --> 00:00:19.740
Then you will relate

9
00:00:19.740 --> 00:00:22.390
the probability matrix
to the language model.

10
00:00:22.390 --> 00:00:24.840
I will also show you how to deal

11
00:00:24.840 --> 00:00:27.210
with technical issues
that arise from

12
00:00:27.210 --> 00:00:28.860
multiplying a loss of

13
00:00:28.860 --> 00:00:32.430
small numbers when calculating
the sentence probability.

14
00:00:32.430 --> 00:00:35.550
The last section briefly
describes how to use

15
00:00:35.550 --> 00:00:37.320
the n-gram language model to

16
00:00:37.320 --> 00:00:39.665
generate new sentences
from scratch.

17
00:00:39.665 --> 00:00:42.070
So let's start with count matrix.

18
00:00:42.070 --> 00:00:44.660
Here's a reminder
of the formula for

19
00:00:44.660 --> 00:00:47.600
testing the conditional
probability of n-gram.

20
00:00:47.600 --> 00:00:52.330
Lowercase n denotes where the
n-gram ends in a sentence.

21
00:00:52.330 --> 00:00:54.500
The count matrix captures

22
00:00:54.500 --> 00:00:58.220
the numerator for all n-grams
appearing in the corpus.

23
00:00:58.220 --> 00:01:01.700
All unique corpus, N minus-1
grams make up the rows,

24
00:01:01.700 --> 00:01:05.155
and all unique words of the
corpus make up the columns.

25
00:01:05.155 --> 00:01:08.070
Now look at the count
matrix of a bigram model.

26
00:01:08.070 --> 00:01:10.875
For the corpus I study I learn,

27
00:01:10.875 --> 00:01:13.830
the rows represent
the first word of

28
00:01:13.830 --> 00:01:15.260
the bigram and the columns

29
00:01:15.260 --> 00:01:17.450
represent the second
word of the bigram.

30
00:01:17.450 --> 00:01:19.490
For bigram study I,

31
00:01:19.490 --> 00:01:22.205
you need to find a row
with the word study,

32
00:01:22.205 --> 00:01:24.145
any column with the word I.

33
00:01:24.145 --> 00:01:27.120
This bigram appeared
just once in the corpus.

34
00:01:27.120 --> 00:01:28.820
The whole count matrix can be

35
00:01:28.820 --> 00:01:31.700
treated in a single pass
through the corpus.

36
00:01:31.700 --> 00:01:34.670
You can do that by reading
through the corpus with

37
00:01:34.670 --> 00:01:36.620
a sliding window composed

38
00:01:36.620 --> 00:01:39.100
of two words to
represent your bigram.

39
00:01:39.100 --> 00:01:41.060
For each bigram you find,

40
00:01:41.060 --> 00:01:44.135
you increase the value in
the count matrix by one.

41
00:01:44.135 --> 00:01:46.820
Let's move on to the
probability matrix.

42
00:01:46.820 --> 00:01:49.610
Now that you've used the
count matrix to provide

43
00:01:49.610 --> 00:01:53.275
your numerator for the
n-gram probability formula,

44
00:01:53.275 --> 00:01:55.455
it's time to get the denominator.

45
00:01:55.455 --> 00:01:57.440
First, update the count matrix

46
00:01:57.440 --> 00:01:59.780
by calculating the
sum for each row,

47
00:01:59.780 --> 00:02:02.070
then normalize each cell.

48
00:02:02.070 --> 00:02:03.905
You can do that by dividing

49
00:02:03.905 --> 00:02:06.700
each cell by the
corresponding row sum.

50
00:02:06.700 --> 00:02:10.040
The row sum is equivalent to
your counts of the n minus

51
00:02:10.040 --> 00:02:13.850
1 gram prefixes from the
formula's denominator.

52
00:02:13.850 --> 00:02:16.430
This will always be true since

53
00:02:16.430 --> 00:02:18.425
the n minus 1 gram prefix

54
00:02:18.425 --> 00:02:20.990
is always followed by some word.

55
00:02:20.990 --> 00:02:23.855
If the prefix was at the
end of the sentence,

56
00:02:23.855 --> 00:02:26.950
it is now followed by the
end of the sentence token.

57
00:02:26.950 --> 00:02:30.800
Let's create a probability
matrix from an example.

58
00:02:30.800 --> 00:02:34.790
First, calculate the sum of
bigram count for each row,

59
00:02:34.790 --> 00:02:37.765
then divide each
cell by the row sum.

60
00:02:37.765 --> 00:02:39.435
Let's look at one example.

61
00:02:39.435 --> 00:02:42.780
The prefix I is followed
by study or by learn.

62
00:02:42.780 --> 00:02:44.945
As you can see in
the count matrix,

63
00:02:44.945 --> 00:02:46.849
if you add up those
two instances,

64
00:02:46.849 --> 00:02:49.565
you'll see that the
word I appears twice.

65
00:02:49.565 --> 00:02:52.205
Now let's go back to
the probability matrix.

66
00:02:52.205 --> 00:02:55.460
You can see that the
probability of I study is

67
00:02:55.460 --> 00:02:59.590
1.5 and the probability
of I learn is also 1.5.

68
00:02:59.590 --> 00:03:03.200
The next step is to connect
the probability matrix with

69
00:03:03.200 --> 00:03:05.060
your definition of
the language model

70
00:03:05.060 --> 00:03:06.490
from this week's overview.

71
00:03:06.490 --> 00:03:10.370
The language model can now be
a simple scripts that uses

72
00:03:10.370 --> 00:03:12.260
the probability matrix to

73
00:03:12.260 --> 00:03:15.170
estimate the probability
of a given sentence.

74
00:03:15.170 --> 00:03:17.300
It estimates the probability

75
00:03:17.300 --> 00:03:19.610
by splitting the sentence into

76
00:03:19.610 --> 00:03:22.070
a series of n-grams and then

77
00:03:22.070 --> 00:03:25.655
finding their probability
in the probability matrix.

78
00:03:25.655 --> 00:03:28.760
Alternatively, the language model

79
00:03:28.760 --> 00:03:30.770
can predict the next elements of

80
00:03:30.770 --> 00:03:32.945
a sequence by extracting

81
00:03:32.945 --> 00:03:36.340
the last n minus 1 gram
from the end of a sequence.

82
00:03:36.340 --> 00:03:38.450
After that, the language model

83
00:03:38.450 --> 00:03:40.310
finds the corresponding row in

84
00:03:40.310 --> 00:03:42.200
the probability matrix and

85
00:03:42.200 --> 00:03:44.455
returns the word with
the highest probability.

86
00:03:44.455 --> 00:03:47.465
Using the probability matrix
from the previous slide,

87
00:03:47.465 --> 00:03:50.195
find the probability of
the sentence I learn.

88
00:03:50.195 --> 00:03:54.835
You take 1 times 0.5 times
1, which equals 0.5.

89
00:03:54.835 --> 00:03:56.990
You have a 50 percent
chance of seeing

90
00:03:56.990 --> 00:03:59.705
the sentence I learn
next in your corpus.

91
00:03:59.705 --> 00:04:01.880
That's cool. There are

92
00:04:01.880 --> 00:04:04.655
a few loose ends in the
language model implementation.

93
00:04:04.655 --> 00:04:08.090
Let's discuss them. The sentence
probability calculation

94
00:04:08.090 --> 00:04:11.735
requires multiplication of
a lots of small numbers.

95
00:04:11.735 --> 00:04:14.240
In fact, all of the probabilities

96
00:04:14.240 --> 00:04:17.210
fall in the interval 0-1.

97
00:04:17.210 --> 00:04:19.790
Multiplying many
probabilities brings

98
00:04:19.790 --> 00:04:21.875
the risk of numerical underflow.

99
00:04:21.875 --> 00:04:24.430
You may remember this
from previous modules.

100
00:04:24.430 --> 00:04:27.050
All you need to know
is that computers have

101
00:04:27.050 --> 00:04:29.830
difficulty storing very
small decimal numbers.

102
00:04:29.830 --> 00:04:31.810
This can end up causing errors.

103
00:04:31.810 --> 00:04:33.465
If you have an
opportunity to store

104
00:04:33.465 --> 00:04:35.700
a larger number
instead, you should.

105
00:04:35.700 --> 00:04:37.760
You may recall the
mathematical trick

106
00:04:37.760 --> 00:04:39.410
for solving this where

107
00:04:39.410 --> 00:04:43.205
you wrote the product of terms
as the sum of other terms,

108
00:04:43.205 --> 00:04:45.535
the logarithm of a product.

109
00:04:45.535 --> 00:04:48.800
One interesting application
of language models is

110
00:04:48.800 --> 00:04:52.430
text generation from scratch
or using a small hint.

111
00:04:52.430 --> 00:04:54.140
For example, the algorithm

112
00:04:54.140 --> 00:04:56.740
chooses brackets S Lynn to start.

113
00:04:56.740 --> 00:05:00.435
Next, it chooses a
bigram with Lynn,

114
00:05:00.435 --> 00:05:02.850
in this case, Lynn drinks,

115
00:05:02.850 --> 00:05:05.775
then it chooses drinks tea.

116
00:05:05.775 --> 00:05:09.430
Finally, it chooses to
end the sentence there.

117
00:05:09.430 --> 00:05:11.270
This happens for all bigram

118
00:05:11.270 --> 00:05:13.265
starting with tea in your corpus.

119
00:05:13.265 --> 00:05:16.055
This is how the algorithm
accomplishes this.

120
00:05:16.055 --> 00:05:18.515
First, it randomly chooses

121
00:05:18.515 --> 00:05:20.990
among all bigrams starting with

122
00:05:20.990 --> 00:05:23.450
the starts of sentence
symbol brackets

123
00:05:23.450 --> 00:05:26.940
S based on the
bigram probability.

124
00:05:26.940 --> 00:05:30.635
That means the bigrams
with higher values

125
00:05:30.635 --> 00:05:34.330
in the probability matrix are
more likely to be chosen.

126
00:05:34.330 --> 00:05:38.164
Next, the algorithm
chooses a new bigram

127
00:05:38.164 --> 00:05:40.040
at random from the bigrams

128
00:05:40.040 --> 00:05:42.500
beginning with a
previously chosen word,

129
00:05:42.500 --> 00:05:46.220
then this bigram is
added to your sentence.

130
00:05:46.220 --> 00:05:49.310
The algorithm continues
on like this until

131
00:05:49.310 --> 00:05:53.275
the end sentence token brackets
backslash S is chosen.

132
00:05:53.275 --> 00:05:55.040
As you might have guessed,

133
00:05:55.040 --> 00:05:58.580
this is done by randomly
choosing a bigram that starts

134
00:05:58.580 --> 00:06:03.175
with the previous word and
ends with the backslash S.

135
00:06:03.175 --> 00:06:06.589
Now you know how to calculate
n-gram probabilities

136
00:06:06.589 --> 00:06:09.680
from a corpus so we can build
your own language model.

137
00:06:09.680 --> 00:06:12.930
Next, I'll show you
how to evaluate it.