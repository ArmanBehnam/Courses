WEBVTT

1
00:00:00.310 --> 00:00:01.580
Welcome to this week.

2
00:00:01.580 --> 00:00:04.060
You are now going to learn
about word vectors and

3
00:00:04.060 --> 00:00:07.785
specifically you're going to see how
you can train them from scratch.

4
00:00:07.785 --> 00:00:10.680
Previously, you just
downloaded word vectors and

5
00:00:10.680 --> 00:00:14.485
used them but in this week,
you're going to be able to train them.

6
00:00:14.485 --> 00:00:15.274
In this video,

7
00:00:15.274 --> 00:00:19.119
you're going to learn about the different
applications of word vectors.

8
00:00:19.119 --> 00:00:22.869
And then you're going to learn about the
different ways of training word vectors.

9
00:00:22.869 --> 00:00:28.431
Word vectors also known as word embeddings
are the cornerstone of most consumer and

10
00:00:28.431 --> 00:00:30.105
enterprise uses of NLP.

11
00:00:30.105 --> 00:00:33.526
If you completed course one
of the NLP specialization,

12
00:00:33.526 --> 00:00:37.822
you may remember that you used
pre-generated word embeddings to find

13
00:00:37.822 --> 00:00:42.578
semantic analogies between words and
to calculate the similarity of words.

14
00:00:42.578 --> 00:00:47.511
You could for instance combine word
embeddings with a classifier to perform

15
00:00:47.511 --> 00:00:51.058
sentiment analysis or
to classify customer reviews or

16
00:00:51.058 --> 00:00:53.616
comments from user feedback surveys.

17
00:00:53.616 --> 00:00:58.046
More advanced use cases for word
embeddings include machine translation

18
00:00:58.046 --> 00:01:01.772
systems, information extraction and
question answering.

19
00:01:01.772 --> 00:01:04.520
These are your learning objectives for
this week.

20
00:01:04.520 --> 00:01:07.750
Identify the key concepts
of word representations.

21
00:01:07.750 --> 00:01:09.730
You will represent words numerically so

22
00:01:09.730 --> 00:01:12.410
that they can be used
with mathematical models.

23
00:01:12.410 --> 00:01:15.070
I'll also go over the benefits
of word embeddings and

24
00:01:15.070 --> 00:01:17.360
representing words with numbers.

25
00:01:17.360 --> 00:01:19.150
Generate word embeddings.

26
00:01:19.150 --> 00:01:23.580
I'll show you the general way in which a
model can learn word embeddings from data,

27
00:01:23.580 --> 00:01:27.600
illustrated with some examples
of commonly used methods.

28
00:01:27.600 --> 00:01:32.048
Prepare text for machine learning,
you will transform a corpus of text into

29
00:01:32.048 --> 00:01:34.771
a training sets for
a machine learning model.

30
00:01:34.771 --> 00:01:37.590
I'll give practical advice
that you can apply to

31
00:01:37.590 --> 00:01:40.488
real life corpora as diverse as books and
tweets.

32
00:01:40.488 --> 00:01:43.976
You will implement the continuous
bag-of-words model,

33
00:01:43.976 --> 00:01:48.510
which is one of the many ways in
which word embeddings can be created.

34
00:01:48.510 --> 00:01:49.720
It's a simple and

35
00:01:49.720 --> 00:01:54.450
efficient approach that initially
popularized the use of word embeddings.

36
00:01:54.450 --> 00:01:58.230
There are other techniques like GloVe,
Word2Vec and others can be used to train

37
00:01:58.230 --> 00:02:03.620
them, but for this week, we're going to
look at the continuous bag-of-words model.

38
00:02:03.620 --> 00:02:05.660
If you aren't familiar
with neural networks,

39
00:02:05.660 --> 00:02:08.479
then I strongly recommend that
you go through the first course

40
00:02:08.479 --> 00:02:12.229
of the deep learning
specialization by deeplearning.ai.

41
00:02:12.229 --> 00:02:14.469
If you are familiar with neural networks,
but

42
00:02:14.469 --> 00:02:17.449
haven't used them in a while,
then don't worry.

43
00:02:17.449 --> 00:02:18.899
I'll review them in this week's lecture.