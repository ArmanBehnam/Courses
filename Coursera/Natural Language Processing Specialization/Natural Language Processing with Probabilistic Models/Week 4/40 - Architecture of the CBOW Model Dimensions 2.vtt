WEBVTT

1
00:00:00.000 --> 00:00:02.820
By now, you are familiar
with the dimensions

2
00:00:02.820 --> 00:00:05.280
of the vectors and
matrices used in

3
00:00:05.280 --> 00:00:07.440
the neural network for
the continuous bag of

4
00:00:07.440 --> 00:00:10.470
words model when feeding
in a single example.

5
00:00:10.470 --> 00:00:12.390
Instead of passing inputs vectors

6
00:00:12.390 --> 00:00:14.640
representing individual examples,

7
00:00:14.640 --> 00:00:17.745
you may want to pass in
several examples at a time.

8
00:00:17.745 --> 00:00:20.040
This usually makes
the learning process

9
00:00:20.040 --> 00:00:21.735
quicker for a few reasons.

10
00:00:21.735 --> 00:00:24.750
For brevity, I won't get
into them right now.

11
00:00:24.750 --> 00:00:27.420
Feeding several examples
into the neural nets at

12
00:00:27.420 --> 00:00:30.120
the same time is known
as a batch processing.

13
00:00:30.120 --> 00:00:31.470
You're going to be doing it

14
00:00:31.470 --> 00:00:33.360
yourself in this
week's assignments.

15
00:00:33.360 --> 00:00:36.150
Say that you want to
feed m inputs context

16
00:00:36.150 --> 00:00:37.320
word vectors into

17
00:00:37.320 --> 00:00:39.810
the neural network
during each iteration.

18
00:00:39.810 --> 00:00:42.185
M is called the batch size.

19
00:00:42.185 --> 00:00:43.700
It's a hyperparameter for

20
00:00:43.700 --> 00:00:45.995
the model that you
define at training time,

21
00:00:45.995 --> 00:00:48.980
you can join these m column
vectors side-by-side to

22
00:00:48.980 --> 00:00:51.965
form a matrix with v
rows and m columns,

23
00:00:51.965 --> 00:00:54.110
which I'll note as capital X.

24
00:00:54.110 --> 00:00:56.825
You can then pass this
matrix through the network,

25
00:00:56.825 --> 00:00:59.330
and you will get capital H,

26
00:00:59.330 --> 00:01:01.700
the matrix representing
the m values

27
00:01:01.700 --> 00:01:03.920
of the hidden layer equals ReLU,

28
00:01:03.920 --> 00:01:06.065
the weights matrix Z_1,

29
00:01:06.065 --> 00:01:10.715
where Z_1 is W_1
times X plus B_1.

30
00:01:10.715 --> 00:01:13.900
It's an n rows by
m columns matrix.

31
00:01:13.900 --> 00:01:16.730
Note that you need to
replace the original B_1 by

32
00:01:16.730 --> 00:01:20.120
its vector with a
bias matrix, B_1,

33
00:01:20.120 --> 00:01:22.490
where you duplicates
the column vector m

34
00:01:22.490 --> 00:01:25.505
times to have an m by n matrix,

35
00:01:25.505 --> 00:01:27.170
so that the bias terms are

36
00:01:27.170 --> 00:01:29.435
added to each of
the weighted sums,

37
00:01:29.435 --> 00:01:33.270
just has a tip;when you're
working on the assignment,

38
00:01:33.270 --> 00:01:36.485
this is known as broadcasting
the vector to a matrix,

39
00:01:36.485 --> 00:01:39.410
and it's performed automatically
by NumPy when adding

40
00:01:39.410 --> 00:01:43.060
a matrix to a column vector
with the same number of rows.

41
00:01:43.060 --> 00:01:45.535
Next, capital Y hat,

42
00:01:45.535 --> 00:01:48.320
the matrix containing
the m outputs.

43
00:01:48.320 --> 00:01:52.710
It is equal to the softmax
of w times h plus b_2.

44
00:01:52.710 --> 00:01:55.040
Again, you're replicating
the bias vector

45
00:01:55.040 --> 00:01:59.840
lowercase b_2 m times to get
the bias matrix capital b_2.

46
00:01:59.840 --> 00:02:02.680
Y hat has v rows and m columns.

47
00:02:02.680 --> 00:02:05.915
You can break y-hat down
into m column vectors,

48
00:02:05.915 --> 00:02:09.230
each one corresponding to
one of the input contexts,

49
00:02:09.230 --> 00:02:11.675
word vectors from
the inputs matrix.

50
00:02:11.675 --> 00:02:13.640
The vector from the
first column of

51
00:02:13.640 --> 00:02:15.890
the inputs matrix
is transformed into

52
00:02:15.890 --> 00:02:17.810
the vector corresponding to

53
00:02:17.810 --> 00:02:20.330
the first column of
the outputs matrix,

54
00:02:20.330 --> 00:02:24.470
and similarly for the
other m minus one vectors.

55
00:02:24.470 --> 00:02:27.980
Next, I'll introduce the
activation functions

56
00:02:27.980 --> 00:02:30.365
used by the continuous
bag of words model.

57
00:02:30.365 --> 00:02:31.550
You're getting closer to

58
00:02:31.550 --> 00:02:34.650
a working model.
I'll see you soon.