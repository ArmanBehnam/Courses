WEBVTT

1
00:00:00.000 --> 00:00:02.714
This week was all
about word embeddings,

2
00:00:02.714 --> 00:00:04.560
which capture the
meaning of words in

3
00:00:04.560 --> 00:00:07.140
vectors that you can
use for both simple,

4
00:00:07.140 --> 00:00:09.435
and advanced NLP applications.

5
00:00:09.435 --> 00:00:11.895
You've covered a lot
of ground this week.

6
00:00:11.895 --> 00:00:14.720
Let's reinforce your new
skills with some review,

7
00:00:14.720 --> 00:00:16.650
and get excited for
the assignment.

8
00:00:16.650 --> 00:00:18.270
In the final assignment,

9
00:00:18.270 --> 00:00:20.250
you'll get to practice
all your new skills

10
00:00:20.250 --> 00:00:23.410
this week, including
preparing data,

11
00:00:23.410 --> 00:00:26.970
you'll read and tokenize a
corpus to build a vocabulary,

12
00:00:26.970 --> 00:00:29.130
creating word representations,

13
00:00:29.130 --> 00:00:31.290
mapping words to
indices and back,

14
00:00:31.290 --> 00:00:34.335
and converting these
indices to one-hot vectors.

15
00:00:34.335 --> 00:00:36.435
Learning word
embeddings by creating

16
00:00:36.435 --> 00:00:38.420
a continuous bag of words model.

17
00:00:38.420 --> 00:00:42.020
You will build a neural network
for the model, train it,

18
00:00:42.020 --> 00:00:44.890
and extracts the resulting
word embedding vectors,

19
00:00:44.890 --> 00:00:47.225
visualizing the word
embedding vectors

20
00:00:47.225 --> 00:00:49.985
as a form of
intrinsic evaluation.

21
00:00:49.985 --> 00:00:51.800
After completing the assignments,

22
00:00:51.800 --> 00:00:53.600
you will then be
well-equipped to move on to

23
00:00:53.600 --> 00:00:55.385
more advanced language modeling

24
00:00:55.385 --> 00:00:57.160
and word embedding approaches.

25
00:00:57.160 --> 00:00:59.330
These sophisticated
approaches supports

26
00:00:59.330 --> 00:01:00.830
out of vocabulary words,

27
00:01:00.830 --> 00:01:02.330
and are better at capturing

28
00:01:02.330 --> 00:01:04.685
the multiple possible
meanings of words.

29
00:01:04.685 --> 00:01:06.680
These capabilities are essential

30
00:01:06.680 --> 00:01:09.850
for real-world
applications of NLP.

31
00:01:09.850 --> 00:01:12.500
Finally, you should know
that even though you're

32
00:01:12.500 --> 00:01:15.634
implementing everything from
scratch for this assignment,

33
00:01:15.634 --> 00:01:18.320
in the real-world, you
will often use NLP

34
00:01:18.320 --> 00:01:19.670
and machine learning libraries

35
00:01:19.670 --> 00:01:21.320
to do all the heavy lifting.

36
00:01:21.320 --> 00:01:23.645
Take some time to
explore these libraries.

37
00:01:23.645 --> 00:01:25.220
It's time well-spent.

38
00:01:25.220 --> 00:01:27.365
Specifically, for word embedding,

39
00:01:27.365 --> 00:01:30.170
the keras and PyTorch
libraries enable you to add

40
00:01:30.170 --> 00:01:31.520
an embedding layer in

41
00:01:31.520 --> 00:01:34.120
the neural network using
a single line of code.

42
00:01:34.120 --> 00:01:37.830
Other libraries like Trax
also allow you to do so.

43
00:01:37.830 --> 00:01:41.550
Well, congratulations on
reaching the end of this course.

44
00:01:41.550 --> 00:01:44.165
This was a lot of
complex material,

45
00:01:44.165 --> 00:01:46.895
and you should feel pride
in your accomplishments.

46
00:01:46.895 --> 00:01:48.844
Best of luck in the assignments,

47
00:01:48.844 --> 00:01:51.040
and most importantly, have fun.

48
00:01:51.040 --> 00:01:52.430
I'll see you in the next course

49
00:01:52.430 --> 00:01:54.335
for more natural
language processing.

50
00:01:54.335 --> 00:01:56.675
Congratulations on
finishing this week.

51
00:01:56.675 --> 00:01:59.315
You now not only know how
to use words vectors,

52
00:01:59.315 --> 00:02:01.745
but you also know how to
train them from scratch.

53
00:02:01.745 --> 00:02:04.560
This is a useful skill to have.