WEBVTT

1
00:00:00.470 --> 00:00:01.830
Good to see you again.

2
00:00:01.830 --> 00:00:06.165
The two types of evaluation metrics
are intrinsic evaluation and

3
00:00:06.165 --> 00:00:07.829
extrinsic evaluation.

4
00:00:07.829 --> 00:00:12.540
Depending on the task you're trying to
optimize for, you can choose one of them.

5
00:00:12.540 --> 00:00:14.723
So, let's dive in.

6
00:00:14.723 --> 00:00:18.257
Allow me to introduce
intrinsic evaluation.

7
00:00:18.257 --> 00:00:23.494
Intrinsic evaluation methods assess how
well the word embeddings inherently

8
00:00:23.494 --> 00:00:28.182
capture the semantic or
syntactic relationships between the words.

9
00:00:28.182 --> 00:00:34.440
Semantics refers to the meaning of words,
whereas syntax refers to the grammar.

10
00:00:34.440 --> 00:00:38.700
You could do this by testing
the word embeddings on analogies.

11
00:00:38.700 --> 00:00:41.571
You could test semantic analogies,

12
00:00:41.571 --> 00:00:47.138
as in finding the missing word in France
is to Paris as Italy is to blank.

13
00:00:47.138 --> 00:00:51.542
You could also evaluate
the embeddings on syntactic analogies,

14
00:00:51.542 --> 00:00:54.598
such as plurals, tenses and comparatives.

15
00:00:54.598 --> 00:00:59.891
As in, seen is to saw as being is to?

16
00:00:59.891 --> 00:01:03.469
One consideration you should
be aware of for this approach,

17
00:01:03.469 --> 00:01:06.917
is that there could be several
possible correct answers.

18
00:01:06.917 --> 00:01:12.817
As in, wolf is to pack as bee is to blank,
where the collective noun and

19
00:01:12.817 --> 00:01:18.420
missing word could, for
instance, be swarm or colony.

20
00:01:18.420 --> 00:01:22.672
Here's an example from the original
Word2vec research publication,

21
00:01:22.672 --> 00:01:26.165
with the word embeddings created
from a huge training set.

22
00:01:26.165 --> 00:01:30.515
I should mention that the word
embeddings were created by a continuous

23
00:01:30.515 --> 00:01:34.289
skip-gram model, and
not a continuous bag of words model.

24
00:01:34.289 --> 00:01:36.886
But the evaluation principle is the same.

25
00:01:36.886 --> 00:01:40.430
Note that the analogies are not perfect.

26
00:01:40.430 --> 00:01:44.980
For example, the word embeddings failed
to completely capture the relationship

27
00:01:44.980 --> 00:01:47.494
between chemical elements and
their symbols.

28
00:01:47.494 --> 00:01:52.048
The symbol for copper is Cu,
the symbol for zinc is Zn,

29
00:01:52.048 --> 00:01:56.899
the symbol for gold is Au,
the symbol for Uranium is U, but

30
00:01:56.899 --> 00:02:02.450
the Word2vec embedding outputs
the word plutonium instead of U.

31
00:02:03.520 --> 00:02:07.370
You could also perform intrinsic
evaluation by using a clustering

32
00:02:07.370 --> 00:02:10.860
algorithm to group similar
word embedding vectors, and

33
00:02:10.860 --> 00:02:13.980
determining of the cluster's
capture related words.

34
00:02:13.980 --> 00:02:19.310
This could be automated using a human
made reference, such as a thesaurus.

35
00:02:19.310 --> 00:02:22.870
Here is a visualization of
clusters taken from a paper

36
00:02:22.870 --> 00:02:24.795
on the evaluation of word embeddings.

37
00:02:24.795 --> 00:02:29.625
If you look at the word cluster,
you can see that the words fight and

38
00:02:29.625 --> 00:02:32.640
agitate are close to each other.

39
00:02:32.640 --> 00:02:38.520
Similarly, the word hair care and
hairdressing are also close to each other.

40
00:02:38.520 --> 00:02:40.807
Interesting, right?

41
00:02:40.807 --> 00:02:45.607
In this week's assignment you will
visualize the word embedding vectors,

42
00:02:45.607 --> 00:02:50.182
which qualifies as a basic intrinsic
evaluation of the vectors that rely

43
00:02:50.182 --> 00:02:53.934
on human judgment to assess
the quality of the embeddings.

44
00:02:53.934 --> 00:02:56.441
But first, I'll show you one more approach

45
00:02:56.441 --> 00:03:00.450
to evaluate the quality of word
embeddings, extrinsic evaluation.