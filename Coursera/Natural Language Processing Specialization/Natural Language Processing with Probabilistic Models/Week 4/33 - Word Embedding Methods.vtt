WEBVTT

1
00:00:00.000 --> 00:00:02.280
Here are a few well
established methods that

2
00:00:02.280 --> 00:00:04.665
you can use to generate
word embeddings.

3
00:00:04.665 --> 00:00:06.975
Word2vec by Google, which

4
00:00:06.975 --> 00:00:09.780
initially popularized the
use of machine learning,

5
00:00:09.780 --> 00:00:11.505
to generate word embeddings.

6
00:00:11.505 --> 00:00:14.610
Word2vec, uses a
shallow neural network

7
00:00:14.610 --> 00:00:16.035
to learn word embeddings.

8
00:00:16.035 --> 00:00:18.690
It proposes two
model architectures,

9
00:00:18.690 --> 00:00:20.505
continuous bag of words,

10
00:00:20.505 --> 00:00:22.650
which is a simple but
efficient approach

11
00:00:22.650 --> 00:00:24.330
that you will
implement this week.

12
00:00:24.330 --> 00:00:26.730
The objective of the
model is to learn to

13
00:00:26.730 --> 00:00:29.805
predict a missing word given
the surrounding words.

14
00:00:29.805 --> 00:00:32.340
Continuous skip-gram, also known

15
00:00:32.340 --> 00:00:34.545
as the skip-gram with
negative sampling,

16
00:00:34.545 --> 00:00:35.880
which does the reverse of

17
00:00:35.880 --> 00:00:37.920
the continuous bag
of words method.

18
00:00:37.920 --> 00:00:39.810
The model learns to
predict the word

19
00:00:39.810 --> 00:00:42.255
surrounding a given input word.

20
00:00:42.255 --> 00:00:45.770
Global vectors or
glove by Stanford,

21
00:00:45.770 --> 00:00:48.154
which involves
factorizing the logarithm

22
00:00:48.154 --> 00:00:50.855
of the corpuses word
co-occurrence matrix,

23
00:00:50.855 --> 00:00:52.100
which is similar to

24
00:00:52.100 --> 00:00:54.140
the counter matrix
you've used before.

25
00:00:54.140 --> 00:00:56.450
fastText by Facebook,

26
00:00:56.450 --> 00:00:59.270
which is based on the
skip-gram model and takes into

27
00:00:59.270 --> 00:01:01.280
account the structure of words by

28
00:01:01.280 --> 00:01:04.555
representing words as an
n-gram of characters.

29
00:01:04.555 --> 00:01:06.110
This enables the model to

30
00:01:06.110 --> 00:01:08.195
support previously unseen words,

31
00:01:08.195 --> 00:01:10.490
known as outer vocabulary words,

32
00:01:10.490 --> 00:01:12.020
by inferring their embedding from

33
00:01:12.020 --> 00:01:13.820
the sequence of
characters they are made

34
00:01:13.820 --> 00:01:15.890
of and the
corresponding sequences

35
00:01:15.890 --> 00:01:17.750
that it was initially trained on.

36
00:01:17.750 --> 00:01:19.890
For example, it would create

37
00:01:19.890 --> 00:01:22.525
similar embeddings
for kitty and kitten,

38
00:01:22.525 --> 00:01:25.565
even if it had never seen
the word kitty before.

39
00:01:25.565 --> 00:01:28.100
As kitty and kitten are

40
00:01:28.100 --> 00:01:30.910
made of similar
sequences of characters.

41
00:01:30.910 --> 00:01:33.210
Another benefit of fastText,

42
00:01:33.210 --> 00:01:35.120
is that word embedding vectors

43
00:01:35.120 --> 00:01:36.710
can be averaged together to

44
00:01:36.710 --> 00:01:40.555
make vector representations
of phrases and sentences.

45
00:01:40.555 --> 00:01:44.105
Next up, some more sophisticated
modeling approaches.

46
00:01:44.105 --> 00:01:45.620
These methods use advanced

47
00:01:45.620 --> 00:01:47.630
deep neural network
architectures,

48
00:01:47.630 --> 00:01:49.550
to refine the representation of

49
00:01:49.550 --> 00:01:52.174
the words meaning according
to their contexts.

50
00:01:52.174 --> 00:01:53.690
In the previous models

51
00:01:53.690 --> 00:01:56.090
a given word always has
the same embedding.

52
00:01:56.090 --> 00:01:58.520
In these more advanced models,

53
00:01:58.520 --> 00:02:00.350
the words have
different embeddings

54
00:02:00.350 --> 00:02:02.125
depending on their context.

55
00:02:02.125 --> 00:02:03.965
This adds supports for

56
00:02:03.965 --> 00:02:07.430
polysemy or words with
similar meanings.

57
00:02:07.430 --> 00:02:09.035
Such as the word plants,

58
00:02:09.035 --> 00:02:11.855
which can refer to an
organism like a flower,

59
00:02:11.855 --> 00:02:14.695
a factory or which
can be an adverb,

60
00:02:14.695 --> 00:02:16.655
with yet more different meanings.

61
00:02:16.655 --> 00:02:17.780
A few examples of

62
00:02:17.780 --> 00:02:20.540
advanced models that generate
word embeddings are,

63
00:02:20.540 --> 00:02:23.000
BERT or bidirectional encoder

64
00:02:23.000 --> 00:02:26.290
representations from
transformers by Google,

65
00:02:26.290 --> 00:02:29.620
ELMO for embeddings
from language models,

66
00:02:29.620 --> 00:02:32.360
by the Allen Institute for AI or

67
00:02:32.360 --> 00:02:37.010
GPT-2 or generative
pre-training 2 by Open AI.

68
00:02:37.010 --> 00:02:39.065
If you want to use
these advanced methods,

69
00:02:39.065 --> 00:02:40.580
you can find off the shelf

70
00:02:40.580 --> 00:02:42.770
pre-trained models
on the Internet.

71
00:02:42.770 --> 00:02:44.780
You can fine tune
these models using

72
00:02:44.780 --> 00:02:47.420
your own corpus to
generate high-quality,

73
00:02:47.420 --> 00:02:49.805
domain, specific word embeddings.

74
00:02:49.805 --> 00:02:52.310
To recap, you now
have some tools in

75
00:02:52.310 --> 00:02:54.650
your toolbox for creating
word embeddings,

76
00:02:54.650 --> 00:02:57.565
including some pretty
sophisticated new models.

77
00:02:57.565 --> 00:02:59.705
That's nice. Up next,

78
00:02:59.705 --> 00:03:02.930
I will introduce the
continuous bag of words model,

79
00:03:02.930 --> 00:03:06.269
that you'll be using for
this week's assignment.