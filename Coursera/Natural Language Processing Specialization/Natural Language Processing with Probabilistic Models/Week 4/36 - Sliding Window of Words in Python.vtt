WEBVTT

1
00:00:00.000 --> 00:00:04.050
Previously, you clean then
tokenize the corpus and

2
00:00:04.050 --> 00:00:05.790
you now have this clean corpus as

3
00:00:05.790 --> 00:00:07.785
an array of words or tokens.

4
00:00:07.785 --> 00:00:09.960
Now, I will show you how to

5
00:00:09.960 --> 00:00:12.690
extract center words and
their context words,

6
00:00:12.690 --> 00:00:14.880
which will serve as
examples to train

7
00:00:14.880 --> 00:00:17.055
the continuous bag
of words model.

8
00:00:17.055 --> 00:00:19.425
Here's the code to
do this in Python.

9
00:00:19.425 --> 00:00:23.415
The get_windows function
takes two arguments: words,

10
00:00:23.415 --> 00:00:26.190
which is an array
of words or tokens,

11
00:00:26.190 --> 00:00:28.065
but I'll stick with
the term words here.

12
00:00:28.065 --> 00:00:31.755
The context have size
stored in the variable C,

13
00:00:31.755 --> 00:00:33.570
which is the number
of words to be

14
00:00:33.570 --> 00:00:35.865
taken on each side
of the center word.

15
00:00:35.865 --> 00:00:38.155
This was true in
the previous video.

16
00:00:38.155 --> 00:00:40.780
For a total window size of five.

17
00:00:40.780 --> 00:00:43.160
The function initializes
a counter with

18
00:00:43.160 --> 00:00:45.290
the index of the first word

19
00:00:45.290 --> 00:00:47.240
that has enough words before it.

20
00:00:47.240 --> 00:00:49.025
So in the working example,

21
00:00:49.025 --> 00:00:50.780
I'm happy because I'm learning,

22
00:00:50.780 --> 00:00:52.250
the tokenized array would

23
00:00:52.250 --> 00:00:54.835
be I'm happy because
I'm learning,

24
00:00:54.835 --> 00:01:00.635
where I has the index 0
and index 1, and so on.

25
00:01:00.635 --> 00:01:02.825
For a context have size of two,

26
00:01:02.825 --> 00:01:04.310
or the context words are

27
00:01:04.310 --> 00:01:06.895
the two words before and
after the center word.

28
00:01:06.895 --> 00:01:10.235
The first center word that
can be used is happy.

29
00:01:10.235 --> 00:01:11.945
Its index is two,

30
00:01:11.945 --> 00:01:14.005
which is the size of the context.

31
00:01:14.005 --> 00:01:16.445
I then start a loop
in this index,

32
00:01:16.445 --> 00:01:17.930
which will run until it

33
00:01:17.930 --> 00:01:20.225
reaches the last
possible center word,

34
00:01:20.225 --> 00:01:23.075
which is the last word that
has two words after it,

35
00:01:23.075 --> 00:01:24.500
stopping just before it

36
00:01:24.500 --> 00:01:26.360
reaches the index
corresponding to

37
00:01:26.360 --> 00:01:28.010
the number of words in the array

38
00:01:28.010 --> 00:01:30.184
minus the size of the context.

39
00:01:30.184 --> 00:01:32.450
In each iteration of the loop,

40
00:01:32.450 --> 00:01:34.445
I extract the center word,

41
00:01:34.445 --> 00:01:36.785
which is the word that
the current index.

42
00:01:36.785 --> 00:01:39.860
Next, I create an
array with the C words

43
00:01:39.860 --> 00:01:43.080
before the center word
and the C words after it.

44
00:01:43.080 --> 00:01:46.735
I can now return the context
word and center words.

45
00:01:46.735 --> 00:01:50.010
I'm using a special way of
returning value in Python.

46
00:01:50.010 --> 00:01:51.290
As you can see, I'm using

47
00:01:51.290 --> 00:01:53.375
the yield keyword
instead of return,

48
00:01:53.375 --> 00:01:55.639
without going into
too much detail

49
00:01:55.639 --> 00:01:58.490
where return word immediately
exit the function,

50
00:01:58.490 --> 00:02:00.560
yield returns the
values and simply

51
00:02:00.560 --> 00:02:02.690
pauses the main
get_window function,

52
00:02:02.690 --> 00:02:04.340
and the function known as

53
00:02:04.340 --> 00:02:06.260
a generator function
to the use of

54
00:02:06.260 --> 00:02:08.180
yield will then continue

55
00:02:08.180 --> 00:02:10.490
running if more
values are needed.

56
00:02:10.490 --> 00:02:12.920
Simply put, with a yield,

57
00:02:12.920 --> 00:02:16.130
I can return values from
a function several times,

58
00:02:16.130 --> 00:02:17.540
which is what I'm doing at

59
00:02:17.540 --> 00:02:19.525
each iteration of the word loop.

60
00:02:19.525 --> 00:02:22.220
Finally, I increase my
index by one to move

61
00:02:22.220 --> 00:02:24.950
my sliding window one
word to the right.

62
00:02:24.950 --> 00:02:26.420
So just to recap,

63
00:02:26.420 --> 00:02:28.070
the get _windows function takes

64
00:02:28.070 --> 00:02:30.500
any corpus and the
context size and

65
00:02:30.500 --> 00:02:32.150
returns the context words and

66
00:02:32.150 --> 00:02:34.715
center words for each
successive window.

67
00:02:34.715 --> 00:02:36.485
Here's how to use this function.

68
00:02:36.485 --> 00:02:39.140
I'm using a loop to get
the successive tuples of

69
00:02:39.140 --> 00:02:42.190
context words and center
words into x and y,

70
00:02:42.190 --> 00:02:44.600
and then displaying these words,

71
00:02:44.600 --> 00:02:46.580
you will notice that I'm using

72
00:02:46.580 --> 00:02:48.680
the usual Machine
Learning notation

73
00:02:48.680 --> 00:02:50.135
for features and targets,

74
00:02:50.135 --> 00:02:52.040
as this is what the
context words and

75
00:02:52.040 --> 00:02:55.295
center words are for the
continuous bag of words model.

76
00:02:55.295 --> 00:02:57.485
So if I run the
code on the array,

77
00:02:57.485 --> 00:02:59.540
I am happy because I'm learning,

78
00:02:59.540 --> 00:03:01.775
which I got by
tokenizing the sentence,

79
00:03:01.775 --> 00:03:03.650
I'm happy because I'm learning,

80
00:03:03.650 --> 00:03:05.765
and the contexts
have size of two,

81
00:03:05.765 --> 00:03:07.090
this is the output.

82
00:03:07.090 --> 00:03:10.520
Up next, you'll convert this
sets of words into a set of

83
00:03:10.520 --> 00:03:12.230
vectors as can be consumed by

84
00:03:12.230 --> 00:03:15.090
the continuous bag
of words model.